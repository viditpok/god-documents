{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573d191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, io, zipfile, shutil, random, warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Set\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR   = Path(\"data\")\n",
    "TMP_DIR    = DATA_DIR / \"_extract_tmp\"\n",
    "OUTDIR     = Path(\"outputs\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOT_DIR   = OUTDIR / \"plots\"; PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PERQ_CSV   = OUTDIR / \"per_security_quarter_means.csv\"\n",
    "STATS_ALL  = OUTDIR / \"stats_overall_byType_byQuarter.csv\"\n",
    "COHORT_S   = OUTDIR / \"cohort_stats_stocks_baseQuarter.csv\"\n",
    "COHORT_E   = OUTDIR / \"cohort_stats_etfs_baseQuarter.csv\"\n",
    "COHORT_T_S = OUTDIR / \"cohort_tickers_stocks_baseQuarter.txt\"\n",
    "COHORT_T_E = OUTDIR / \"cohort_tickers_etfs_baseQuarter.txt\"\n",
    "\n",
    "RNG_SEED = 23\n",
    "random.seed(RNG_SEED); np.random.seed(RNG_SEED)\n",
    "\n",
    "TIME_COL   = \"Quarter\"\n",
    "TICKER_COL = \"Ticker\"\n",
    "TYPE_COL   = \"Type\"\n",
    "\n",
    "RAW_VARS = [\n",
    "    \"Cancels\",\"Trades\",\"LitTrades\",\"OddLots\",\"Hidden\",\"TradesForHidden\",\n",
    "    \"OrderVol\",\"TradeVol\",\"LitVol\",\"OddLotVol\",\"HiddenVol\",\"TradeVolForHidden\",\n",
    "    \"TradesForOddLots\",\"TradeVolForOddLots\"\n",
    "]\n",
    "\n",
    "METRICS = {\n",
    "    \"Cancel_to_Trade\": (\"Cancels\",\"Trades\"),\n",
    "    \"Trade_to_Order_Volume_pct\": (\"TradeVol\",\"OrderVol\"),\n",
    "    \"Hidden_Rate_pct\": (\"TradesForHidden\",\"Trades\"),\n",
    "    \"Hidden_Volume_pct\": (\"TradeVolForHidden\",\"TradeVol\"),\n",
    "    \"Oddlot_Rate_pct\": (\"OddLots\",\"Trades\"),\n",
    "    \"Oddlot_Volume_pct\": (\"OddLotVol\",\"TradeVol\"),\n",
    "}\n",
    "\n",
    "KEEP_COLS_MIN = [\"Date\",\"Security\",TICKER_COL,TIME_COL,TYPE_COL]\n",
    "KEEP_COLS_ALL = KEEP_COLS_MIN + RAW_VARS + list(METRICS.keys())\n",
    "\n",
    "_ETF_RE = re.compile(r\"(ETF|ETP|EXCHANGE\\s*TRADED|TRUST)\", re.IGNORECASE)\n",
    "pat_qy1 = re.compile(r\"q([1-4])[_-]?(\\d{4})_all\\.csv$\", re.IGNORECASE)\n",
    "pat_qy2 = re.compile(r\"individual_security[_-]?(\\d{4})[_-]?q([1-4])\\.csv$\", re.IGNORECASE)\n",
    "pat_any_q = re.compile(r\"(?i)(?:^|[_-])(q([1-4]))[_-]?((?:20)?\\d{2})|((?:20)?\\d{2})[_-]?(q([1-4]))\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081ed91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_path_robust(path: Path, usecols=None, chunksize=None):\n",
    "    try:\n",
    "        return pd.read_csv(path, engine=\"c\", usecols=usecols, chunksize=chunksize, low_memory=False)\n",
    "    except Exception:\n",
    "        return pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\", usecols=usecols, chunksize=chunksize, low_memory=False)\n",
    "\n",
    "def unzip_iter_csvs_and_loose_csvs(data_dir: Path, tmp_root: Path):\n",
    "    if tmp_root.exists(): shutil.rmtree(tmp_root)\n",
    "    tmp_root.mkdir(parents=True, exist_ok=True)\n",
    "    for zp in sorted(data_dir.glob(\"*.zip\")):\n",
    "        try:\n",
    "            with zipfile.ZipFile(zp) as zf:\n",
    "                zf.extractall(tmp_root / zp.stem)\n",
    "                subdir = tmp_root / zp.stem\n",
    "                cands = [p for p in subdir.rglob(\"*.csv\")]\n",
    "                if not cands:\n",
    "                    continue\n",
    "                yield max(cands, key=lambda p: p.stat().st_size)\n",
    "        except Exception:\n",
    "            continue\n",
    "    for p in sorted(data_dir.glob(\"*.csv\")):\n",
    "        yield p\n",
    "\n",
    "def extract_quarter_from_path(path: Path) -> str | None:\n",
    "    b = path.name\n",
    "    m = pat_qy1.search(b)\n",
    "    if m: return f\"{m.group(2)}Q{m.group(1)}\"\n",
    "    m = pat_qy2.search(b)\n",
    "    if m: return f\"{m.group(1)}Q{m.group(2)}\"\n",
    "    for part in [path.stem, path.parent.name, path.parent.parent.name]:\n",
    "        if not part: continue\n",
    "        mm = pat_any_q.search(part)\n",
    "        if mm:\n",
    "            q1, y2, y3, q2 = mm.group(1), mm.group(3), mm.group(4), mm.group(6)\n",
    "            if q1 and y2:\n",
    "                y = int(y2); y = (y+2000) if y < 100 else y\n",
    "                return f\"{y}Q{int(q1[-1])}\"\n",
    "            if y3 and q2:\n",
    "                y = int(y3); y = (y+2000) if y < 100 else y\n",
    "                return f\"{y}Q{int(q2[-1])}\"\n",
    "    return None\n",
    "\n",
    "def ensure_header(path: Path):\n",
    "    return (not path.exists()) or (path.stat().st_size == 0)\n",
    "\n",
    "def to_quarter_series(s: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    out = pd.Series(pd.NA, index=s.index, dtype=\"string\")\n",
    "    m = dt.notna() & (dt.dt.year >= 2010) & (dt.dt.year <= 2030)\n",
    "    if m.any():\n",
    "        out.loc[m] = (dt.dt.year[m].astype(\"Int64\").astype(\"string\")\n",
    "                      + \"Q\" +\n",
    "                      dt.dt.quarter[m].astype(\"Int64\").astype(\"string\"))\n",
    "    return out\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.replace(r\"\\('000\\)$\", \"\", regex=True)\n",
    "    if \"Ticker\" in df.columns:\n",
    "        df[\"Ticker\"] = df[\"Ticker\"].astype(str).str.strip().str.upper()\n",
    "    def to_num(cols: List[str]):\n",
    "        if cols:\n",
    "            df[cols] = df[cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    if \"Trades\" not in df.columns:\n",
    "        parts = [c for c in (\"LitTrades\",\"TradesForHidden\",\"TradesForOddLots\") if c in df.columns]\n",
    "        to_num(parts); df[\"Trades\"] = df[parts].sum(axis=1, min_count=1) if parts else pd.NA\n",
    "    if \"TradeVol\" not in df.columns:\n",
    "        if \"TradeVolForOddLots\" in df.columns:\n",
    "            df[\"TradeVol\"] = pd.to_numeric(df[\"TradeVolForOddLots\"], errors=\"coerce\")\n",
    "        else:\n",
    "            parts = [c for c in (\"LitVol\",\"HiddenVol\",\"TradeVolForOddLots\") if c in df.columns]\n",
    "            to_num(parts); df[\"TradeVol\"] = df[parts].sum(axis=1, min_count=1) if parts else pd.NA\n",
    "    return df\n",
    "\n",
    "def classify_type(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"Security\" in df.columns:\n",
    "        sec = df[\"Security\"].astype(str).str.strip()\n",
    "        sec_u = sec.str.upper()\n",
    "        uniq = set(sec_u.unique())\n",
    "        if uniq.issubset({\"STOCK\",\"ETF\"}):\n",
    "            df[TYPE_COL] = sec_u.map({\"STOCK\":\"Stock\",\"ETF\":\"ETF\"}).astype(\"category\")\n",
    "        else:\n",
    "            df[TYPE_COL] = np.where(sec.str.contains(_ETF_RE, na=False), \"ETF\", \"Stock\").astype(\"category\")\n",
    "    else:\n",
    "        df[TYPE_COL] = pd.Series(pd.Categorical([pd.NA]*len(df), categories=[\"Stock\",\"ETF\"]), index=df.index)\n",
    "    return df\n",
    "\n",
    "def normalize_type_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"Type\" in df.columns:\n",
    "        s = df[\"Type\"].astype(str).str.strip().str.lower().replace({\"etp\":\"etf\"})\n",
    "        s = s.map(lambda x: \"ETF\" if x == \"etf\" else (\"Stock\" if x == \"stock\" else np.nan))\n",
    "        df[\"Type\"] = pd.Categorical(s, categories=[\"Stock\",\"ETF\"])\n",
    "    return df\n",
    "\n",
    "def add_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for new_col,(num_col,den_col) in METRICS.items():\n",
    "        num = pd.to_numeric(df.get(num_col), errors=\"coerce\")\n",
    "        den = pd.to_numeric(df.get(den_col), errors=\"coerce\")\n",
    "        ratio = num.div(den).where(den.ne(0))\n",
    "        if new_col.endswith(\"_pct\"): ratio = ratio * 100.0\n",
    "        df[new_col] = ratio\n",
    "    return df\n",
    "\n",
    "def quarter_from_name_or_date(df: pd.DataFrame, q_hint: str | None) -> pd.Series:\n",
    "    q_from_date = None\n",
    "    if \"Date\" in df.columns:\n",
    "        q_from_date = to_quarter_series(df[\"Date\"])\n",
    "    if q_hint is not None:\n",
    "        if q_from_date is None:\n",
    "            return pd.Series(q_hint, index=df.index, dtype=\"string\")\n",
    "        out = q_from_date.copy()\n",
    "        out = out.fillna(q_hint)\n",
    "        return out\n",
    "    else:\n",
    "        if q_from_date is not None:\n",
    "            return q_from_date\n",
    "        return pd.Series(pd.NA, index=df.index, dtype=\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9407bb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PERQ_CSV.exists():\n",
    "    PERQ_CSV.unlink()\n",
    "\n",
    "for csv_path in unzip_iter_csvs_and_loose_csvs(DATA_DIR, TMP_DIR):\n",
    "    q_hint = extract_quarter_from_path(csv_path)\n",
    "    reader = read_csv_path_robust(csv_path, usecols=None, chunksize=200_000)\n",
    "    for chunk in reader:\n",
    "        chunk = normalize_columns(chunk)\n",
    "        chunk[TIME_COL] = quarter_from_name_or_date(chunk, q_hint)\n",
    "        chunk = classify_type(chunk)\n",
    "        chunk = normalize_type_values(chunk)\n",
    "        chunk = add_metrics(chunk)\n",
    "        have = [c for c in KEEP_COLS_ALL if c in chunk.columns]\n",
    "        if not have:\n",
    "            continue\n",
    "        chunk = chunk[have]\n",
    "        chunk = chunk[chunk[TIME_COL].notna() & chunk[TICKER_COL].notna() & chunk[TYPE_COL].notna()]\n",
    "        vals = [c for c in (RAW_VARS + list(METRICS.keys())) if c in chunk.columns]\n",
    "        if not vals:\n",
    "            continue\n",
    "        if not chunk[vals].notna().any(axis=1).any():\n",
    "            continue\n",
    "        chunk = chunk[chunk[vals].notna().any(axis=1)]\n",
    "        agg = chunk.groupby([TYPE_COL, TIME_COL, TICKER_COL], as_index=False)[vals].mean()\n",
    "        agg.to_csv(PERQ_CSV, mode=\"a\", index=False, header=ensure_header(PERQ_CSV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10028209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_descriptive_stats(df: pd.DataFrame, group_cols: List[str], value_cols: List[str]) -> pd.DataFrame:\n",
    "    def _agg(g: pd.DataFrame):\n",
    "        out = {}\n",
    "        for col in value_cols:\n",
    "            x = pd.to_numeric(g[col], errors='coerce')\n",
    "            out[(col,'mean')] = np.nanmean(x)\n",
    "            out[(col,'p25')]  = np.nanpercentile(x,25)\n",
    "            out[(col,'p50')]  = np.nanpercentile(x,50)\n",
    "            out[(col,'p75')]  = np.nanpercentile(x,75)\n",
    "            out[(col,'std')]  = np.nanstd(x, ddof=1) if np.sum(~np.isnan(x))>1 else np.nan\n",
    "        return pd.Series(out)\n",
    "    stats = df.groupby(group_cols, dropna=False, sort=True).apply(_agg).reset_index()\n",
    "    stats.columns = ['_'.join([c for c in col if c]) if isinstance(col,tuple) else col for col in stats.columns]\n",
    "    return stats\n",
    "\n",
    "usecols_perq = [TYPE_COL, TIME_COL, TICKER_COL] + RAW_VARS + list(METRICS.keys())\n",
    "perq = pd.read_csv(PERQ_CSV, usecols=lambda c: c in set(usecols_perq), low_memory=False)\n",
    "\n",
    "perq[TIME_COL]   = perq[TIME_COL].astype(str).str.strip()\n",
    "perq[TICKER_COL] = perq[TICKER_COL].astype(str).str.strip()\n",
    "perq[TYPE_COL]   = perq[TYPE_COL].astype(str).str.strip().str.capitalize()\n",
    "\n",
    "target_cols_all = [c for c in RAW_VARS + list(METRICS.keys()) if c in perq.columns]\n",
    "\n",
    "if target_cols_all:\n",
    "    stats_overall = grouped_descriptive_stats(perq, [TYPE_COL, TIME_COL], target_cols_all)\n",
    "    stats_overall.to_csv(STATS_ALL, index=False)\n",
    "\n",
    "def quarter_key(qs: str):\n",
    "    m = re.match(r\"^(\\d{4})Q([1-4])$\", str(qs))\n",
    "    return (int(m.group(1)), int(m.group(2))) if m else (9999, 9)\n",
    "\n",
    "def tickers_in_quarter(df, q, t):\n",
    "    sub = df[(df[TIME_COL]==q) & (df[TYPE_COL]==t)]\n",
    "    return set(sub[TICKER_COL].dropna().astype(str).unique())\n",
    "\n",
    "desired_base = \"2012Q1\"\n",
    "available_quarters = sorted(perq[TIME_COL].unique(), key=quarter_key)\n",
    "s_des = tickers_in_quarter(perq, desired_base, \"Stock\")\n",
    "e_des = tickers_in_quarter(perq, desired_base, \"ETF\") | tickers_in_quarter(perq, desired_base, \"Etf\")\n",
    "\n",
    "if (desired_base not in set(perq[TIME_COL])) or (len(s_des) < 100) or (len(e_des) < 100):\n",
    "    fallback = None\n",
    "    for q in available_quarters:\n",
    "        s = tickers_in_quarter(perq, q, \"Stock\")\n",
    "        e = tickers_in_quarter(perq, q, \"ETF\") | tickers_in_quarter(perq, q, \"Etf\")\n",
    "        if len(s) >= 100 and len(e) >= 100:\n",
    "            fallback = q; break\n",
    "    base_quarter = fallback if fallback else (available_quarters[0] if available_quarters else \"NA\")\n",
    "else:\n",
    "    base_quarter = desired_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa721178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_cohort(df: pd.DataFrame, cohort_quarter: str, type_value: str, n=100, seed=RNG_SEED) -> Set[str]:\n",
    "    base = df[(df[TIME_COL]==cohort_quarter) & (df[TYPE_COL]==type_value)]\n",
    "    tickers = base[TICKER_COL].dropna().astype(str).unique()\n",
    "    k = min(n, len(tickers))\n",
    "    if k == 0: return set()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return set(rng.choice(tickers, size=k, replace=False).tolist())\n",
    "\n",
    "cohort_stocks = sample_cohort(perq[[TYPE_COL,TIME_COL,TICKER_COL]], base_quarter, \"Stock\", 100, RNG_SEED)\n",
    "cohort_etfs   = sample_cohort(perq[[TYPE_COL,TIME_COL,TICKER_COL]], base_quarter, \"ETF\",   100, RNG_SEED)\n",
    "if len(cohort_etfs) < 100:\n",
    "    cohort_etfs = sample_cohort(perq[[TYPE_COL,TIME_COL,TICKER_COL]], base_quarter, \"Etf\", 100, RNG_SEED) or cohort_etfs\n",
    "\n",
    "with open(COHORT_T_S, \"w\") as f: \n",
    "    for t in sorted(cohort_stocks): f.write(t+\"\\n\")\n",
    "with open(COHORT_T_E, \"w\") as f: \n",
    "    for t in sorted(cohort_etfs): f.write(t+\"\\n\")\n",
    "\n",
    "def cohort_time_stats(perq_df: pd.DataFrame, cohort_tickers: Set[str], value_cols: List[str]) -> pd.DataFrame:\n",
    "    if not cohort_tickers:\n",
    "        return pd.DataFrame(columns=[TIME_COL] + sum([[f\"{c}_{s}\" for s in (\"mean\",\"p25\",\"p50\",\"p75\",\"std\")] for c in value_cols], []))\n",
    "    sub = perq_df[perq_df[TICKER_COL].astype(str).isin(cohort_tickers)].copy()\n",
    "    return grouped_descriptive_stats(sub, [TIME_COL], value_cols).sort_values(TIME_COL)\n",
    "\n",
    "stats_stock = cohort_time_stats(perq[perq[TYPE_COL].str.lower()==\"stock\"], cohort_stocks, target_cols_all)\n",
    "stats_etf   = cohort_time_stats(perq[perq[TYPE_COL].str.lower().isin([\"etf\",\"etp\"])], cohort_etfs, target_cols_all)\n",
    "\n",
    "stats_stock.to_csv(COHORT_S, index=False)\n",
    "stats_etf.to_csv(COHORT_E, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59267bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_metrics_in_df(df: pd.DataFrame) -> List[str]:\n",
    "    names = []\n",
    "    for c in df.columns:\n",
    "        if c == TIME_COL: continue\n",
    "        if \"_\" in c:\n",
    "            names.append(c.rsplit(\"_\",1)[0])\n",
    "    return sorted(list(set(names)))\n",
    "\n",
    "def plot_descriptive_stats(df: pd.DataFrame, cohort_name: str, base_q: str):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    metrics = list_metrics_in_df(df)\n",
    "    if not metrics:\n",
    "        return\n",
    "    for m in metrics:\n",
    "        cols = [c for c in df.columns if c.startswith(m+\"_\")]\n",
    "        if not cols: \n",
    "            continue\n",
    "        x = df[TIME_COL].astype(str).tolist()\n",
    "        plt.figure(figsize=(11,6))\n",
    "        for c in sorted(cols):\n",
    "            plt.plot(x, df[c].values, marker=\"o\", label=c.replace(m+\"_\",\"\"))\n",
    "        plt.title(f\"{cohort_name} ({base_q} cohort): {m} — descriptive stats over time\")\n",
    "        plt.xlabel(\"Quarter\")\n",
    "        plt.ylabel(m)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        out_path = PLOT_DIR / f\"{cohort_name}_{m}.png\"\n",
    "        plt.savefig(out_path, dpi=160)\n",
    "        plt.close()\n",
    "\n",
    "plot_descriptive_stats(stats_stock, \"StocksCohort\", base_quarter)\n",
    "plot_descriptive_stats(stats_etf, \"ETFsCohort\", base_quarter)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
