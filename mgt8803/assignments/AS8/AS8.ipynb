{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1774c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew, kurtosis\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5ba729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "plt.rcParams[\"figure.dpi\"] = 150\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df65f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRYPTO_LIST = [\"BTC\", \"ETH\", \"LTC\", \"BCH\"]\n",
    "MARKET_CHOICES = [\n",
    "    [\"FIAT\"],\n",
    "    [\"EQUITY\"],\n",
    "    [\"GOLD\"],\n",
    "    [\"ENERGY\"],\n",
    "    [\"FIAT\", \"EQUITY\"],\n",
    "    [\"FIAT\", \"GOLD\"],\n",
    "    [\"FIAT\", \"EQUITY\", \"GOLD\", \"ENERGY\"],\n",
    "]\n",
    "BASELINE_WINDOW = 12\n",
    "BASELINE_EPOCHS = 25\n",
    "BASELINE_HIDDEN = 4\n",
    "BASELINE_LR = 0.001\n",
    "BASELINE_ACT = \"tanh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9eceb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_prices():\n",
    "    base_path = \"data\"\n",
    "    files = {\n",
    "        \"BTC\": \"CBBTCUSD.csv\",\n",
    "        \"ETH\": \"CBETHUSD.csv\",\n",
    "        \"LTC\": \"CBLTCUSD.csv\",\n",
    "        \"BCH\": \"CBBCHUSD.csv\",\n",
    "        \"FIAT\": \"DTWEXBGS.csv\",\n",
    "        \"EQUITY\": \"CRSP.csv\",\n",
    "        \"GOLD\": \"NASDAQQGLDI.csv\",\n",
    "        \"ENERGY\": \"VDE.csv\",\n",
    "    }\n",
    "\n",
    "    dfs = {}\n",
    "\n",
    "    for name, filename in files.items():\n",
    "        path = os.path.join(base_path, filename)\n",
    "        if filename == \"CRSP.csv\":\n",
    "            df = pd.read_csv(path, low_memory=False,\n",
    "                             usecols=[\"DATE\", \"vwretd\"])\n",
    "            df.columns = [\"date\", \"vwretd\"]\n",
    "            df[\"date\"] = pd.to_datetime(\n",
    "                df[\"date\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "            df = df.dropna(subset=[\"date\"])\n",
    "            df = df.groupby(\"date\")[\"vwretd\"].first().reset_index()\n",
    "            df = df.sort_values(\"date\")\n",
    "            date_col = \"date\"\n",
    "            price_col = \"vwretd\"\n",
    "        else:\n",
    "            df = pd.read_csv(path)\n",
    "            df.columns = [c.lower().strip() for c in df.columns]\n",
    "\n",
    "            date_col = None\n",
    "            for col in df.columns:\n",
    "                if \"date\" in col:\n",
    "                    date_col = col\n",
    "                    break\n",
    "            if date_col is None:\n",
    "                raise ValueError(f\"No date column found in {filename}\")\n",
    "\n",
    "            df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "            df = df.dropna(subset=[date_col])\n",
    "            df = df.sort_values(date_col)\n",
    "\n",
    "        if filename != \"CRSP.csv\":\n",
    "            num_cols = [c for c in df.columns if c != date_col]\n",
    "            if len(num_cols) == 0:\n",
    "                raise ValueError(f\"No price/value column found in {filename}\")\n",
    "\n",
    "            if filename == \"VDE.csv\":\n",
    "                close_cols = [c for c in num_cols if \"close\" in c.lower()]\n",
    "                if close_cols:\n",
    "                    price_col = close_cols[0]\n",
    "                else:\n",
    "                    price_col = num_cols[0]\n",
    "                    print(\n",
    "                        f\"Warning: Close column not found in VDE.csv, using {price_col}\"\n",
    "                    )\n",
    "            else:\n",
    "                price_col = num_cols[0]\n",
    "\n",
    "        df[price_col] = pd.to_numeric(df[price_col], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[price_col])\n",
    "\n",
    "        if len(df) == 0:\n",
    "            print(f\"Warning: {filename} has no valid data after cleaning\")\n",
    "            continue\n",
    "\n",
    "        df = df.set_index(date_col)\n",
    "        df = df.resample(\"M\").last()\n",
    "        df[price_col] = pd.to_numeric(df[price_col], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[price_col])\n",
    "\n",
    "        if len(df) == 0:\n",
    "            print(f\"Warning: {filename} has no data after resampling\")\n",
    "            continue\n",
    "\n",
    "        if filename == \"CRSP.csv\":\n",
    "            df[name] = np.log(1 + df[price_col])\n",
    "        else:\n",
    "            df[name] = np.log(df[price_col]).diff()\n",
    "            df = df.dropna(subset=[name])\n",
    "\n",
    "        dfs[name] = df[[name]]\n",
    "\n",
    "    data = pd.concat(dfs.values(), axis=1)\n",
    "    data = data.dropna(how=\"all\")\n",
    "    data = data.dropna()\n",
    "    data.index.name = \"Date\"\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def split_sets(dates, X, y, ids):\n",
    "    mask_train = (dates >= \"2015-01-01\") & (dates <= \"2019-12-31\")\n",
    "    mask_val = (dates >= \"2020-01-01\") & (dates <= \"2021-12-31\")\n",
    "    mask_test = (dates >= \"2022-01-01\") & (dates <= \"2025-10-31\")\n",
    "\n",
    "    def subset(mask):\n",
    "        idx = np.array(mask)\n",
    "        return X[idx], y[idx], [ids[i] for i, v in enumerate(mask) if v]\n",
    "\n",
    "    return subset(mask_train), subset(mask_val), subset(mask_test)\n",
    "\n",
    "\n",
    "def p(msg):\n",
    "    print(msg, flush=True)\n",
    "\n",
    "\n",
    "def make_panel(data, market_assets, w):\n",
    "    required_cols = [\"BTC\", \"ETH\", \"LTC\", \"BCH\"] + market_assets\n",
    "    missing_cols = [col for col in required_cols if col not in data.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(\n",
    "            f\"Missing columns in data: {missing_cols}. Available columns: {list(data.columns)}\"\n",
    "        )\n",
    "\n",
    "    if len(data) < w + 1:\n",
    "        raise ValueError(\n",
    "            f\"Data has only {len(data)} rows, but need at least {w + 1} rows for window size {w}\"\n",
    "        )\n",
    "\n",
    "    X, y, ids = [], [], []\n",
    "    for crypto in [\"BTC\", \"ETH\", \"LTC\", \"BCH\"]:\n",
    "        for t in range(w, len(data) - 1):\n",
    "            past_crypto = data[crypto].iloc[t - w: t].values\n",
    "            past_mkts = data[market_assets].iloc[t - w: t].values.flatten()\n",
    "            X.append(np.concatenate([past_crypto, past_mkts]))\n",
    "            y.append(data[crypto].iloc[t + 1])\n",
    "            ids.append((data.index[t + 1], crypto))\n",
    "    return np.array(X), np.array(y), ids\n",
    "\n",
    "\n",
    "def build_panel_for_market(data, market_assets, w):\n",
    "    X, y, ids = make_panel(data, market_assets, w)\n",
    "\n",
    "    dates = pd.to_datetime([d for d, _ in ids])\n",
    "    (Xtr, ytr, ids_tr), (Xval, yval, ids_val), (Xte, yte, ids_te) = split_sets(\n",
    "        dates, X, y, ids\n",
    "    )\n",
    "    return {\n",
    "        \"market_assets\": market_assets,\n",
    "        \"w\": w,\n",
    "        \"Xtr\": Xtr,\n",
    "        \"ytr\": ytr,\n",
    "        \"ids_tr\": ids_tr,\n",
    "        \"Xval\": Xval,\n",
    "        \"yval\": yval,\n",
    "        \"ids_val\": ids_val,\n",
    "        \"Xte\": Xte,\n",
    "        \"yte\": yte,\n",
    "        \"ids_te\": ids_te,\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_betas_df(model, X, ids, asset_label):\n",
    "    if len(X) == 0:\n",
    "        return pd.DataFrame(columns=[\"Date\", \"Crypto\", \"Asset\", \"Beta\"])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_t = torch.tensor(X, dtype=torch.float32)\n",
    "        betas = model(X_t).cpu().numpy().ravel()\n",
    "    out = pd.DataFrame(\n",
    "        {\n",
    "            \"Date\": pd.to_datetime([d for d, _ in ids]),\n",
    "            \"Crypto\": [c for _, c in ids],\n",
    "            \"Asset\": asset_label,\n",
    "            \"Beta\": betas,\n",
    "        }\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2041e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralBetaNet(nn.Module):\n",
    "    def __init__(self, in_dim, hidden, activation):\n",
    "        super().__init__()\n",
    "        act = {\n",
    "            \"linear\": nn.Identity(),\n",
    "            \"sigmoid\": nn.Sigmoid(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"relu\": nn.ReLU(),\n",
    "        }[activation]\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), act, nn.Linear(hidden, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def rmse_loss(pred, target):\n",
    "    return torch.sqrt(torch.mean((pred - target) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f07e8c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(Xtr, ytr, Xval, yval, hidden, lr, act, epochs=10):\n",
    "    if len(Xtr) == 0:\n",
    "        raise ValueError(\n",
    "            \"Training set is empty. Check data loading and panel construction.\"\n",
    "        )\n",
    "    if len(Xtr.shape) < 2:\n",
    "        raise ValueError(\n",
    "            f\"Training set has wrong shape: {Xtr.shape}. Expected 2D array.\"\n",
    "        )\n",
    "    in_dim = Xtr.shape[1]\n",
    "    model = NeuralBetaNet(in_dim, hidden, act).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_hist, val_hist = [], []\n",
    "\n",
    "    Xtr_t = torch.tensor(Xtr, dtype=torch.float32).to(device)\n",
    "    ytr_t = torch.tensor(ytr, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    Xval_t = torch.tensor(Xval, dtype=torch.float32).to(device)\n",
    "    yval_t = torch.tensor(yval, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        out = model(Xtr_t)\n",
    "        loss = rmse_loss(out, ytr_t)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(Xval_t)\n",
    "            val_loss = rmse_loss(val_out, yval_t)\n",
    "        loss_hist.append(loss.item())\n",
    "        val_hist.append(val_loss.item())\n",
    "        print(\n",
    "            f\"Epoch {ep + 1:03d} | train RMSE {loss.item():.6f} | val RMSE {val_loss.item():.6f}\"\n",
    "        )\n",
    "    return model, loss_hist, val_hist\n",
    "\n",
    "\n",
    "def grid_search(Xtr, ytr, Xval, yval, market_assets):\n",
    "    params = {\n",
    "        \"hidden\": [4, 8, 16],\n",
    "        \"lr\": [0.001, 0.01, 0.1],\n",
    "        \"act\": [\"linear\", \"sigmoid\", \"tanh\", \"relu\"],\n",
    "        \"w\": [12, 24, 36],\n",
    "    }\n",
    "    results = []\n",
    "    for h in params[\"hidden\"]:\n",
    "        for lr in params[\"lr\"]:\n",
    "            for act in params[\"act\"]:\n",
    "                for w in params[\"w\"]:\n",
    "                    try:\n",
    "                        model, loss_hist, val_hist = train_nn(\n",
    "                            Xtr, ytr, Xval, yval, hidden=h, lr=lr, act=act, epochs=10\n",
    "                        )\n",
    "                        val_rmse = val_hist[-1]\n",
    "                        results.append(\n",
    "                            [market_assets, w, h, lr, act, val_rmse])\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error {market_assets}, w={w}, act={act}: {e}\")\n",
    "    df = pd.DataFrame(\n",
    "        results,\n",
    "        columns=[\"MarketAssets\", \"Window\", \"Hidden\",\n",
    "                 \"LR\", \"Activation\", \"Val_RMSE\"],\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc6e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(loss_hist, val_hist, label, out_path):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(loss_hist, label=\"Train RMSE\")\n",
    "    plt.plot(val_hist, label=\"Validation RMSE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(f\"Learning Curve – {label}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81831bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_descriptive_stats(betas):\n",
    "    desc = {\n",
    "        \"N\": len(betas),\n",
    "        \"Mean\": np.mean(betas),\n",
    "        \"Median\": np.median(betas),\n",
    "        \"Std\": np.std(betas, ddof=1),\n",
    "        \"Variance\": np.var(betas, ddof=1),\n",
    "        \"Skew\": skew(betas),\n",
    "        \"Kurtosis\": kurtosis(betas, fisher=False),\n",
    "        \"Min\": np.min(betas),\n",
    "        \"P1\": np.percentile(betas, 1),\n",
    "        \"P5\": np.percentile(betas, 5),\n",
    "        \"P25\": np.percentile(betas, 25),\n",
    "        \"P50\": np.percentile(betas, 50),\n",
    "        \"P75\": np.percentile(betas, 75),\n",
    "        \"P95\": np.percentile(betas, 95),\n",
    "        \"P99\": np.percentile(betas, 99),\n",
    "        \"Max\": np.max(betas),\n",
    "    }\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0224224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_annual_beta_dynamics(df_betas, out_path):\n",
    "    df_betas[\"Date\"] = pd.to_datetime(df_betas[\"Date\"], errors=\"coerce\")\n",
    "    df_betas[\"year\"] = df_betas[\"Date\"].dt.year\n",
    "    df_betas[\"month\"] = df_betas[\"Date\"].dt.month\n",
    "\n",
    "    unique_years = df_betas[\"year\"].dropna().unique()\n",
    "    if len(unique_years) == 1:\n",
    "        agg = (\n",
    "            df_betas.groupby([\"month\", \"Crypto\"])[\"Beta\"]\n",
    "            .agg([\"mean\", \"std\"])\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        for c in agg[\"Crypto\"].unique():\n",
    "            subset = agg[agg[\"Crypto\"] == c]\n",
    "            plt.plot(subset[\"month\"], subset[\"mean\"], label=c, marker=\"o\")\n",
    "        plt.xlabel(\"Month\")\n",
    "        plt.ylabel(\"Mean β\")\n",
    "        plt.title(f\"Monthly Mean Neural β Dynamics ({unique_years[0]})\")\n",
    "        plt.xticks(range(1, 13))\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        mean_path = out_path.replace(\".png\", \"_mean.png\")\n",
    "        plt.savefig(mean_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        for c in agg[\"Crypto\"].unique():\n",
    "            subset = agg[agg[\"Crypto\"] == c]\n",
    "            plt.plot(subset[\"month\"], subset[\"std\"], label=c, marker=\"s\")\n",
    "        plt.xlabel(\"Month\")\n",
    "        plt.ylabel(\"Std β\")\n",
    "        plt.title(f\"Monthly Std Neural β Dynamics ({unique_years[0]})\")\n",
    "        plt.xticks(range(1, 13))\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        std_path = out_path.replace(\".png\", \"_std.png\")\n",
    "        plt.savefig(std_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 8))\n",
    "        for c in agg[\"Crypto\"].unique():\n",
    "            subset = agg[agg[\"Crypto\"] == c]\n",
    "            ax1.plot(subset[\"month\"], subset[\"mean\"], label=c, marker=\"o\")\n",
    "            ax2.plot(subset[\"month\"], subset[\"std\"], label=c, marker=\"s\")\n",
    "        ax1.set_xlabel(\"Month\")\n",
    "        ax1.set_ylabel(\"Mean β\")\n",
    "        ax1.set_title(f\"Monthly Mean Neural β Dynamics ({unique_years[0]})\")\n",
    "        ax1.set_xticks(range(1, 13))\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        ax2.set_xlabel(\"Month\")\n",
    "        ax2.set_ylabel(\"Std β\")\n",
    "        ax2.set_title(f\"Monthly Std Neural β Dynamics ({unique_years[0]})\")\n",
    "        ax2.set_xticks(range(1, 13))\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        agg = (\n",
    "            df_betas.groupby([\"year\", \"Crypto\"])[\"Beta\"]\n",
    "            .agg([\"mean\", \"std\"])\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        for c in agg[\"Crypto\"].unique():\n",
    "            subset = agg[agg[\"Crypto\"] == c]\n",
    "            plt.plot(subset[\"year\"], subset[\"mean\"], label=c, marker=\"o\")\n",
    "        plt.xlabel(\"Year\")\n",
    "        plt.ylabel(\"Mean β\")\n",
    "        plt.title(\"Annual Mean Neural β Dynamics\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        mean_path = out_path.replace(\".png\", \"_mean.png\")\n",
    "        plt.savefig(mean_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        for c in agg[\"Crypto\"].unique():\n",
    "            subset = agg[agg[\"Crypto\"] == c]\n",
    "            plt.plot(subset[\"year\"], subset[\"std\"], label=c, marker=\"s\")\n",
    "        plt.xlabel(\"Year\")\n",
    "        plt.ylabel(\"Std β\")\n",
    "        plt.title(\"Annual Std Neural β Dynamics\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        std_path = out_path.replace(\".png\", \"_std.png\")\n",
    "        plt.savefig(std_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 8))\n",
    "        for c in agg[\"Crypto\"].unique():\n",
    "            subset = agg[agg[\"Crypto\"] == c]\n",
    "            ax1.plot(subset[\"year\"], subset[\"mean\"], label=c, marker=\"o\")\n",
    "            ax2.plot(subset[\"year\"], subset[\"std\"], label=c, marker=\"s\")\n",
    "        ax1.set_xlabel(\"Year\")\n",
    "        ax1.set_ylabel(\"Mean β\")\n",
    "        ax1.set_title(\"Annual Mean Neural β Dynamics\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        ax2.set_xlabel(\"Year\")\n",
    "        ax2.set_ylabel(\"Std β\")\n",
    "        ax2.set_title(\"Annual Std Neural β Dynamics\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20f9f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ff_factors(data, ff_df):\n",
    "    ff_df.columns = [c.strip() for c in ff_df.columns]\n",
    "\n",
    "    if \"date\" not in [c.lower() for c in ff_df.columns]:\n",
    "        ff_df.rename(columns={ff_df.columns[0]: \"date\"}, inplace=True)\n",
    "\n",
    "    ff_df = ff_df[ff_df[\"date\"].astype(str).str.match(r\"^\\d{6}$\", na=False)]\n",
    "\n",
    "    ff_df[\"date\"] = pd.to_datetime(\n",
    "        ff_df[\"date\"].astype(str) + \"01\", format=\"%Y%m%d\"\n",
    "    ) + pd.offsets.MonthEnd(0)\n",
    "\n",
    "    for col in ff_df.columns:\n",
    "        if col != \"date\":\n",
    "            ff_df[col] = pd.to_numeric(ff_df[col], errors=\"coerce\") / 100.0\n",
    "\n",
    "    ff_df = ff_df.set_index(\"date\").sort_index()\n",
    "\n",
    "    merged = data.join(ff_df, how=\"left\").dropna()\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "496bc4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_and_portfolio(df_betas, returns_df, value_weighted=False):\n",
    "    if value_weighted:\n",
    "        crypto_mean_abs_returns = {}\n",
    "        for crypto in [\"BTC\", \"ETH\", \"LTC\", \"BCH\"]:\n",
    "            if crypto in returns_df.columns:\n",
    "                mean_abs_ret = abs(returns_df[crypto]).mean()\n",
    "                crypto_mean_abs_returns[crypto] = mean_abs_ret\n",
    "        total_weight = sum(crypto_mean_abs_returns.values())\n",
    "        if total_weight > 0:\n",
    "            crypto_weights_global = {\n",
    "                k: v / total_weight for k, v in crypto_mean_abs_returns.items()\n",
    "            }\n",
    "        else:\n",
    "            crypto_weights_global = {\n",
    "                c: 1.0 / 4 for c in [\"BTC\", \"ETH\", \"LTC\", \"BCH\"]}\n",
    "    else:\n",
    "        crypto_weights_global = {\n",
    "            c: 1.0 / 4 for c in [\"BTC\", \"ETH\", \"LTC\", \"BCH\"]}\n",
    "\n",
    "    result = []\n",
    "    for asset in df_betas[\"Asset\"].unique():\n",
    "        temp = df_betas[df_betas[\"Asset\"] == asset]\n",
    "        for date, grp in temp.groupby(\"Date\"):\n",
    "            ranked = grp.sort_values(\"Beta\")\n",
    "            ranked[\"Quartile\"] = range(1, len(ranked) + 1)\n",
    "\n",
    "            quartile_data = {}\n",
    "            for q, sub in ranked.groupby(\"Quartile\"):\n",
    "                mean_b = sub[\"Beta\"].mean()\n",
    "\n",
    "                rets = []\n",
    "                weights = []\n",
    "                for c in sub[\"Crypto\"]:\n",
    "                    if (\n",
    "                        c in returns_df.columns\n",
    "                        and asset in returns_df.columns\n",
    "                        and date in returns_df.index\n",
    "                    ):\n",
    "                        excess_ret = (\n",
    "                            returns_df[c].loc[date] -\n",
    "                            returns_df[asset].loc[date]\n",
    "                        )\n",
    "                        rets.append(excess_ret)\n",
    "                        weights.append(\n",
    "                            crypto_weights_global.get(c, 1.0 / len(sub)))\n",
    "\n",
    "                if rets:\n",
    "                    if value_weighted:\n",
    "                        mean_ex = np.average(\n",
    "                            rets, weights=weights[: len(rets)])\n",
    "                    else:\n",
    "                        mean_ex = np.nanmean(rets)\n",
    "                    result.append([date, asset, q, mean_b, mean_ex])\n",
    "                    quartile_data[q] = {\"beta\": mean_b, \"ret\": mean_ex}\n",
    "\n",
    "            if 1 in quartile_data and 4 in quartile_data:\n",
    "                q1_beta = quartile_data[1][\"beta\"]\n",
    "                q4_beta = quartile_data[4][\"beta\"]\n",
    "                q1_ret = quartile_data[1][\"ret\"]\n",
    "                q4_ret = quartile_data[4][\"ret\"]\n",
    "                beta_diff = q4_beta - q1_beta\n",
    "                ret_diff = q4_ret - q1_ret\n",
    "                result.append([date, asset, \"Q4-Q1\", beta_diff, ret_diff])\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        result, columns=[\"Date\", \"Asset\",\n",
    "                         \"Quartile\", \"MeanBeta\", \"ExcessReturn\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e40a171",
   "metadata": {},
   "source": [
    "### Step 0 – Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "987a5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (51, 8)\n",
      "Data columns: ['BTC', 'ETH', 'LTC', 'BCH', 'FIAT', 'EQUITY', 'GOLD', 'ENERGY']\n",
      "Data date range: 2018-01-31 00:00:00 to 2023-11-30 00:00:00\n",
      "                 BTC       ETH       LTC       BCH      FIAT    EQUITY  \\\n",
      "Date                                                                     \n",
      "2018-01-31 -0.316698  0.399245 -0.338720 -0.474456 -0.030520  0.049340   \n",
      "2018-02-28  0.020315 -0.262385  0.221883 -0.208192  0.015064 -0.040254   \n",
      "2018-04-30  0.288312  0.529274  0.243438  0.681713  0.017133  0.004668   \n",
      "2018-05-31 -0.211054 -0.148979 -0.225709 -0.305480  0.023970  0.025810   \n",
      "2018-07-31  0.191088 -0.047297 -0.026645  0.036910 -0.004678  0.031085   \n",
      "\n",
      "                GOLD    ENERGY  \n",
      "Date                            \n",
      "2018-01-31  0.023697  0.031108  \n",
      "2018-02-28 -0.018993 -0.113646  \n",
      "2018-04-30 -0.010383  0.092493  \n",
      "2018-05-31 -0.016046  0.035246  \n",
      "2018-07-31 -0.025448  0.010855  \n",
      "Saved data successfully\n"
     ]
    }
   ],
   "source": [
    "data = load_and_prepare_prices()\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Data columns: {data.columns.tolist()}\")\n",
    "print(f\"Data date range: {data.index.min()} to {data.index.max()}\")\n",
    "print(data.head())\n",
    "if len(data) == 0:\n",
    "    print(\"WARNING: Data is empty! Check data files and date alignment.\")\n",
    "else:\n",
    "    data.to_csv(\"outputs/data_monthly_log_returns.csv\")\n",
    "    print(\"Saved data successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e76df06",
   "metadata": {},
   "source": [
    "### Step 1 – Neural‑network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d614c43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train RMSE 0.138542 | val RMSE 0.332085\n",
      "Epoch 002 | train RMSE 0.136611 | val RMSE 0.332702\n",
      "Epoch 003 | train RMSE 0.134753 | val RMSE 0.333349\n",
      "Epoch 004 | train RMSE 0.132968 | val RMSE 0.334026\n",
      "Epoch 005 | train RMSE 0.131254 | val RMSE 0.334725\n",
      "Epoch 006 | train RMSE 0.129605 | val RMSE 0.335435\n",
      "Epoch 007 | train RMSE 0.128016 | val RMSE 0.336147\n",
      "Epoch 008 | train RMSE 0.126485 | val RMSE 0.336853\n",
      "Epoch 009 | train RMSE 0.125009 | val RMSE 0.337547\n",
      "Epoch 010 | train RMSE 0.123588 | val RMSE 0.338223\n",
      "Epoch 011 | train RMSE 0.122219 | val RMSE 0.338873\n",
      "Epoch 012 | train RMSE 0.120901 | val RMSE 0.339490\n",
      "Epoch 013 | train RMSE 0.119631 | val RMSE 0.340070\n",
      "Epoch 014 | train RMSE 0.118406 | val RMSE 0.340606\n",
      "Epoch 015 | train RMSE 0.117224 | val RMSE 0.341096\n",
      "Epoch 016 | train RMSE 0.116081 | val RMSE 0.341536\n",
      "Epoch 017 | train RMSE 0.114975 | val RMSE 0.341927\n",
      "Epoch 018 | train RMSE 0.113905 | val RMSE 0.342271\n",
      "Epoch 019 | train RMSE 0.112868 | val RMSE 0.342570\n",
      "Epoch 020 | train RMSE 0.111865 | val RMSE 0.342828\n",
      "Epoch 021 | train RMSE 0.110895 | val RMSE 0.343051\n",
      "Epoch 022 | train RMSE 0.109959 | val RMSE 0.343244\n",
      "Epoch 023 | train RMSE 0.109057 | val RMSE 0.343413\n",
      "Epoch 024 | train RMSE 0.108191 | val RMSE 0.343565\n",
      "Epoch 025 | train RMSE 0.107362 | val RMSE 0.343706\n",
      "Saved baseline model and learning curve\n"
     ]
    }
   ],
   "source": [
    "panel_fiat = build_panel_for_market(data, [\"FIAT\"], BASELINE_WINDOW)\n",
    "model1, loss_hist1, val_hist1 = train_nn(\n",
    "    panel_fiat[\"Xtr\"],\n",
    "    panel_fiat[\"ytr\"],\n",
    "    panel_fiat[\"Xval\"],\n",
    "    panel_fiat[\"yval\"],\n",
    "    hidden=BASELINE_HIDDEN,\n",
    "    lr=BASELINE_LR,\n",
    "    act=BASELINE_ACT,\n",
    "    epochs=BASELINE_EPOCHS,\n",
    ")\n",
    "torch.save(model1.state_dict(), \"outputs/model_step1_fiat.pt\")\n",
    "plot_learning_curve(\n",
    "    loss_hist1,\n",
    "    val_hist1,\n",
    "    \"FIAT w=12 baseline\",\n",
    "    \"outputs/step1_learning_curve_fiat_w12.png\",\n",
    ")\n",
    "p(\"Saved baseline model and learning curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616d1e7",
   "metadata": {},
   "source": [
    "### Step 2 – Hyper‑parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ade4c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning market=FIAT\n",
      "Epoch 001 | train RMSE 0.194089 | val RMSE 0.423832\n",
      "Epoch 002 | train RMSE 0.191897 | val RMSE 0.422628\n",
      "Epoch 003 | train RMSE 0.189721 | val RMSE 0.421440\n",
      "Epoch 004 | train RMSE 0.187562 | val RMSE 0.420260\n",
      "Epoch 005 | train RMSE 0.185420 | val RMSE 0.419087\n",
      "Epoch 006 | train RMSE 0.183296 | val RMSE 0.417921\n",
      "Epoch 007 | train RMSE 0.181191 | val RMSE 0.416761\n",
      "Epoch 008 | train RMSE 0.179105 | val RMSE 0.415608\n",
      "Epoch 009 | train RMSE 0.177040 | val RMSE 0.414462\n",
      "Epoch 010 | train RMSE 0.174997 | val RMSE 0.413323\n",
      "  h=4 lr=0.001 act=linear -> val RMSE=0.41332\n",
      "Epoch 001 | train RMSE 0.341638 | val RMSE 0.552121\n",
      "Epoch 002 | train RMSE 0.339617 | val RMSE 0.550866\n",
      "Epoch 003 | train RMSE 0.337588 | val RMSE 0.549608\n",
      "Epoch 004 | train RMSE 0.335549 | val RMSE 0.548347\n",
      "Epoch 005 | train RMSE 0.333502 | val RMSE 0.547083\n",
      "Epoch 006 | train RMSE 0.331446 | val RMSE 0.545817\n",
      "Epoch 007 | train RMSE 0.329381 | val RMSE 0.544548\n",
      "Epoch 008 | train RMSE 0.327307 | val RMSE 0.543277\n",
      "Epoch 009 | train RMSE 0.325223 | val RMSE 0.542002\n",
      "Epoch 010 | train RMSE 0.323131 | val RMSE 0.540726\n",
      "  h=4 lr=0.001 act=tanh -> val RMSE=0.54073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train RMSE 0.129347 | val RMSE 0.353665\n",
      "Epoch 002 | train RMSE 0.128004 | val RMSE 0.352367\n",
      "Epoch 003 | train RMSE 0.126690 | val RMSE 0.351086\n",
      "Epoch 004 | train RMSE 0.125554 | val RMSE 0.349851\n",
      "Epoch 005 | train RMSE 0.124461 | val RMSE 0.348649\n",
      "Epoch 006 | train RMSE 0.123400 | val RMSE 0.347476\n",
      "Epoch 007 | train RMSE 0.122371 | val RMSE 0.346325\n",
      "Epoch 008 | train RMSE 0.121376 | val RMSE 0.345198\n",
      "Epoch 009 | train RMSE 0.120416 | val RMSE 0.344094\n",
      "Epoch 010 | train RMSE 0.119488 | val RMSE 0.343014\n",
      "  h=4 lr=0.001 act=relu -> val RMSE=0.34301\n",
      "Epoch 001 | train RMSE 0.143145 | val RMSE 0.314736\n",
      "Epoch 002 | train RMSE 0.133481 | val RMSE 0.315607\n",
      "Epoch 003 | train RMSE 0.125507 | val RMSE 0.313180\n",
      "Epoch 004 | train RMSE 0.118353 | val RMSE 0.310952\n",
      "Epoch 005 | train RMSE 0.112499 | val RMSE 0.310913\n",
      "Epoch 006 | train RMSE 0.107665 | val RMSE 0.312882\n",
      "Epoch 007 | train RMSE 0.103796 | val RMSE 0.315699\n",
      "Epoch 008 | train RMSE 0.101187 | val RMSE 0.317895\n",
      "Epoch 009 | train RMSE 0.099707 | val RMSE 0.318824\n",
      "Epoch 010 | train RMSE 0.098860 | val RMSE 0.319113\n",
      "  h=4 lr=0.01 act=linear -> val RMSE=0.31911\n",
      "Epoch 001 | train RMSE 0.123304 | val RMSE 0.283632\n",
      "Epoch 002 | train RMSE 0.108834 | val RMSE 0.293230\n",
      "Epoch 003 | train RMSE 0.104746 | val RMSE 0.301129\n",
      "Epoch 004 | train RMSE 0.105490 | val RMSE 0.305514\n",
      "Epoch 005 | train RMSE 0.105403 | val RMSE 0.306776\n",
      "Epoch 006 | train RMSE 0.103103 | val RMSE 0.306154\n",
      "Epoch 007 | train RMSE 0.099889 | val RMSE 0.304812\n",
      "Epoch 008 | train RMSE 0.097374 | val RMSE 0.303696\n",
      "Epoch 009 | train RMSE 0.096349 | val RMSE 0.303526\n",
      "Epoch 010 | train RMSE 0.096284 | val RMSE 0.304658\n",
      "  h=4 lr=0.01 act=tanh -> val RMSE=0.30466\n",
      "Epoch 001 | train RMSE 0.298033 | val RMSE 0.455610\n",
      "Epoch 002 | train RMSE 0.275060 | val RMSE 0.443877\n",
      "Epoch 003 | train RMSE 0.253928 | val RMSE 0.432503\n",
      "Epoch 004 | train RMSE 0.234431 | val RMSE 0.421257\n",
      "Epoch 005 | train RMSE 0.216079 | val RMSE 0.410093\n",
      "Epoch 006 | train RMSE 0.199988 | val RMSE 0.399180\n",
      "Epoch 007 | train RMSE 0.187738 | val RMSE 0.388730\n",
      "Epoch 008 | train RMSE 0.178588 | val RMSE 0.378819\n",
      "Epoch 009 | train RMSE 0.172464 | val RMSE 0.369359\n",
      "Epoch 010 | train RMSE 0.167010 | val RMSE 0.360645\n",
      "  h=4 lr=0.01 act=relu -> val RMSE=0.36064\n",
      "Epoch 001 | train RMSE 0.368030 | val RMSE 0.297091\n",
      "Epoch 002 | train RMSE 0.363319 | val RMSE 0.296160\n",
      "Epoch 003 | train RMSE 0.358609 | val RMSE 0.295249\n",
      "Epoch 004 | train RMSE 0.353900 | val RMSE 0.294356\n",
      "Epoch 005 | train RMSE 0.349193 | val RMSE 0.293481\n",
      "Epoch 006 | train RMSE 0.344488 | val RMSE 0.292624\n",
      "Epoch 007 | train RMSE 0.339786 | val RMSE 0.291785\n",
      "Epoch 008 | train RMSE 0.335086 | val RMSE 0.290963\n",
      "Epoch 009 | train RMSE 0.330389 | val RMSE 0.290157\n",
      "Epoch 010 | train RMSE 0.325696 | val RMSE 0.289368\n",
      "  h=8 lr=0.001 act=linear -> val RMSE=0.28937\n",
      "Epoch 001 | train RMSE 0.361535 | val RMSE 0.267947\n",
      "Epoch 002 | train RMSE 0.355545 | val RMSE 0.266869\n",
      "Epoch 003 | train RMSE 0.349598 | val RMSE 0.265827\n",
      "Epoch 004 | train RMSE 0.343694 | val RMSE 0.264810\n",
      "Epoch 005 | train RMSE 0.337834 | val RMSE 0.263817\n",
      "Epoch 006 | train RMSE 0.332015 | val RMSE 0.262851\n",
      "Epoch 007 | train RMSE 0.326237 | val RMSE 0.261917\n",
      "Epoch 008 | train RMSE 0.320502 | val RMSE 0.261018\n",
      "Epoch 009 | train RMSE 0.314809 | val RMSE 0.260157\n",
      "Epoch 010 | train RMSE 0.309163 | val RMSE 0.259335\n",
      "  h=8 lr=0.001 act=tanh -> val RMSE=0.25934\n",
      "Epoch 001 | train RMSE 0.249316 | val RMSE 0.286456\n",
      "Epoch 002 | train RMSE 0.246137 | val RMSE 0.285814\n",
      "Epoch 003 | train RMSE 0.242988 | val RMSE 0.285198\n",
      "Epoch 004 | train RMSE 0.239878 | val RMSE 0.284611\n",
      "Epoch 005 | train RMSE 0.236800 | val RMSE 0.284049\n",
      "Epoch 006 | train RMSE 0.233731 | val RMSE 0.283505\n",
      "Epoch 007 | train RMSE 0.230674 | val RMSE 0.282978\n",
      "Epoch 008 | train RMSE 0.227611 | val RMSE 0.282456\n",
      "Epoch 009 | train RMSE 0.224570 | val RMSE 0.281947\n",
      "Epoch 010 | train RMSE 0.221556 | val RMSE 0.281467\n",
      "  h=8 lr=0.001 act=relu -> val RMSE=0.28147\n",
      "Epoch 001 | train RMSE 0.138888 | val RMSE 0.325160\n",
      "Epoch 002 | train RMSE 0.127066 | val RMSE 0.328300\n",
      "Epoch 003 | train RMSE 0.115260 | val RMSE 0.334587\n",
      "Epoch 004 | train RMSE 0.105253 | val RMSE 0.340950\n",
      "Epoch 005 | train RMSE 0.098350 | val RMSE 0.344458\n",
      "Epoch 006 | train RMSE 0.094344 | val RMSE 0.345104\n",
      "Epoch 007 | train RMSE 0.092901 | val RMSE 0.345135\n",
      "Epoch 008 | train RMSE 0.093435 | val RMSE 0.346657\n",
      "Epoch 009 | train RMSE 0.093803 | val RMSE 0.349877\n",
      "Epoch 010 | train RMSE 0.092796 | val RMSE 0.353741\n",
      "  h=8 lr=0.01 act=linear -> val RMSE=0.35374\n",
      "Epoch 001 | train RMSE 0.315831 | val RMSE 0.313049\n",
      "Epoch 002 | train RMSE 0.272189 | val RMSE 0.303593\n",
      "Epoch 003 | train RMSE 0.233281 | val RMSE 0.296806\n",
      "Epoch 004 | train RMSE 0.200020 | val RMSE 0.292710\n",
      "Epoch 005 | train RMSE 0.173614 | val RMSE 0.291141\n",
      "Epoch 006 | train RMSE 0.155532 | val RMSE 0.291594\n",
      "Epoch 007 | train RMSE 0.146307 | val RMSE 0.293265\n",
      "Epoch 008 | train RMSE 0.144029 | val RMSE 0.295258\n",
      "Epoch 009 | train RMSE 0.144875 | val RMSE 0.296815\n",
      "Epoch 010 | train RMSE 0.145333 | val RMSE 0.297495\n",
      "  h=8 lr=0.01 act=tanh -> val RMSE=0.29750\n",
      "Epoch 001 | train RMSE 0.279162 | val RMSE 0.280893\n",
      "Epoch 002 | train RMSE 0.246321 | val RMSE 0.277088\n",
      "Epoch 003 | train RMSE 0.214444 | val RMSE 0.275594\n",
      "Epoch 004 | train RMSE 0.184848 | val RMSE 0.276617\n",
      "Epoch 005 | train RMSE 0.159092 | val RMSE 0.280182\n",
      "Epoch 006 | train RMSE 0.140340 | val RMSE 0.286031\n",
      "Epoch 007 | train RMSE 0.131839 | val RMSE 0.292947\n",
      "Epoch 008 | train RMSE 0.132737 | val RMSE 0.299140\n",
      "Epoch 009 | train RMSE 0.136498 | val RMSE 0.303325\n",
      "Epoch 010 | train RMSE 0.137475 | val RMSE 0.305088\n",
      "  h=8 lr=0.01 act=relu -> val RMSE=0.30509\n",
      "Epoch 001 | train RMSE 0.278717 | val RMSE 0.463045\n",
      "Epoch 002 | train RMSE 0.271854 | val RMSE 0.459834\n",
      "Epoch 003 | train RMSE 0.265040 | val RMSE 0.456653\n",
      "Epoch 004 | train RMSE 0.258279 | val RMSE 0.453505\n",
      "Epoch 005 | train RMSE 0.251573 | val RMSE 0.450396\n",
      "Epoch 006 | train RMSE 0.244924 | val RMSE 0.447331\n",
      "Epoch 007 | train RMSE 0.238333 | val RMSE 0.444317\n",
      "Epoch 008 | train RMSE 0.231800 | val RMSE 0.441360\n",
      "Epoch 009 | train RMSE 0.225325 | val RMSE 0.438461\n",
      "Epoch 010 | train RMSE 0.218907 | val RMSE 0.435618\n",
      "  h=16 lr=0.001 act=linear -> val RMSE=0.43562\n",
      "Epoch 001 | train RMSE 0.231945 | val RMSE 0.280500\n",
      "Epoch 002 | train RMSE 0.225568 | val RMSE 0.280251\n",
      "Epoch 003 | train RMSE 0.219283 | val RMSE 0.280059\n",
      "Epoch 004 | train RMSE 0.213099 | val RMSE 0.279916\n",
      "Epoch 005 | train RMSE 0.207023 | val RMSE 0.279816\n",
      "Epoch 006 | train RMSE 0.201064 | val RMSE 0.279761\n",
      "Epoch 007 | train RMSE 0.195230 | val RMSE 0.279756\n",
      "Epoch 008 | train RMSE 0.189533 | val RMSE 0.279806\n",
      "Epoch 009 | train RMSE 0.183982 | val RMSE 0.279913\n",
      "Epoch 010 | train RMSE 0.178589 | val RMSE 0.280079\n",
      "  h=16 lr=0.001 act=tanh -> val RMSE=0.28008\n",
      "Epoch 001 | train RMSE 0.181530 | val RMSE 0.367499\n",
      "Epoch 002 | train RMSE 0.177819 | val RMSE 0.365589\n",
      "Epoch 003 | train RMSE 0.174149 | val RMSE 0.363694\n",
      "Epoch 004 | train RMSE 0.170532 | val RMSE 0.361813\n",
      "Epoch 005 | train RMSE 0.166854 | val RMSE 0.359953\n",
      "Epoch 006 | train RMSE 0.163224 | val RMSE 0.358113\n",
      "Epoch 007 | train RMSE 0.159650 | val RMSE 0.356284\n",
      "Epoch 008 | train RMSE 0.156088 | val RMSE 0.354473\n",
      "Epoch 009 | train RMSE 0.152587 | val RMSE 0.352686\n",
      "Epoch 010 | train RMSE 0.149145 | val RMSE 0.350905\n",
      "  h=16 lr=0.001 act=relu -> val RMSE=0.35091\n",
      "Epoch 001 | train RMSE 0.190152 | val RMSE 0.274559\n",
      "Epoch 002 | train RMSE 0.142644 | val RMSE 0.284197\n",
      "Epoch 003 | train RMSE 0.122479 | val RMSE 0.294026\n",
      "Epoch 004 | train RMSE 0.117214 | val RMSE 0.300051\n",
      "Epoch 005 | train RMSE 0.112699 | val RMSE 0.302159\n",
      "Epoch 006 | train RMSE 0.106741 | val RMSE 0.302403\n",
      "Epoch 007 | train RMSE 0.102923 | val RMSE 0.302487\n",
      "Epoch 008 | train RMSE 0.103386 | val RMSE 0.303520\n",
      "Epoch 009 | train RMSE 0.105182 | val RMSE 0.306061\n",
      "Epoch 010 | train RMSE 0.103842 | val RMSE 0.310165\n",
      "  h=16 lr=0.01 act=linear -> val RMSE=0.31017\n",
      "Epoch 001 | train RMSE 0.395000 | val RMSE 0.358470\n",
      "Epoch 002 | train RMSE 0.326956 | val RMSE 0.345383\n",
      "Epoch 003 | train RMSE 0.264864 | val RMSE 0.337578\n",
      "Epoch 004 | train RMSE 0.212199 | val RMSE 0.335348\n",
      "Epoch 005 | train RMSE 0.176089 | val RMSE 0.337524\n",
      "Epoch 006 | train RMSE 0.162266 | val RMSE 0.341385\n",
      "Epoch 007 | train RMSE 0.164087 | val RMSE 0.344118\n",
      "Epoch 008 | train RMSE 0.166245 | val RMSE 0.344292\n",
      "Epoch 009 | train RMSE 0.160425 | val RMSE 0.341907\n",
      "Epoch 010 | train RMSE 0.146709 | val RMSE 0.337636\n",
      "  h=16 lr=0.01 act=tanh -> val RMSE=0.33764\n",
      "Epoch 001 | train RMSE 0.165526 | val RMSE 0.295515\n",
      "Epoch 002 | train RMSE 0.134476 | val RMSE 0.309570\n",
      "Epoch 003 | train RMSE 0.122291 | val RMSE 0.318207\n",
      "Epoch 004 | train RMSE 0.115517 | val RMSE 0.319706\n",
      "Epoch 005 | train RMSE 0.107480 | val RMSE 0.317458\n",
      "Epoch 006 | train RMSE 0.099701 | val RMSE 0.314903\n",
      "Epoch 007 | train RMSE 0.095093 | val RMSE 0.313787\n",
      "Epoch 008 | train RMSE 0.093520 | val RMSE 0.315028\n",
      "Epoch 009 | train RMSE 0.092508 | val RMSE 0.318611\n",
      "Epoch 010 | train RMSE 0.090208 | val RMSE 0.323905\n",
      "  h=16 lr=0.01 act=relu -> val RMSE=0.32390\n",
      "Tuning market=EQUITY\n",
      "Epoch 001 | train RMSE 0.177798 | val RMSE 0.285901\n",
      "Epoch 002 | train RMSE 0.174630 | val RMSE 0.285724\n",
      "Epoch 003 | train RMSE 0.171528 | val RMSE 0.285581\n",
      "Epoch 004 | train RMSE 0.168495 | val RMSE 0.285464\n",
      "Epoch 005 | train RMSE 0.165537 | val RMSE 0.285371\n",
      "Epoch 006 | train RMSE 0.162657 | val RMSE 0.285299\n",
      "Epoch 007 | train RMSE 0.159860 | val RMSE 0.285249\n",
      "Epoch 008 | train RMSE 0.157150 | val RMSE 0.285220\n",
      "Epoch 009 | train RMSE 0.154529 | val RMSE 0.285212\n",
      "Epoch 010 | train RMSE 0.152000 | val RMSE 0.285224\n",
      "  h=4 lr=0.001 act=linear -> val RMSE=0.28522\n",
      "Epoch 001 | train RMSE 0.219969 | val RMSE 0.290551\n",
      "Epoch 002 | train RMSE 0.215437 | val RMSE 0.290405\n",
      "Epoch 003 | train RMSE 0.210940 | val RMSE 0.290285\n",
      "Epoch 004 | train RMSE 0.206482 | val RMSE 0.290193\n",
      "Epoch 005 | train RMSE 0.202066 | val RMSE 0.290126\n",
      "Epoch 006 | train RMSE 0.197697 | val RMSE 0.290083\n",
      "Epoch 007 | train RMSE 0.193379 | val RMSE 0.290064\n",
      "Epoch 008 | train RMSE 0.189117 | val RMSE 0.290065\n",
      "Epoch 009 | train RMSE 0.184914 | val RMSE 0.290085\n",
      "Epoch 010 | train RMSE 0.180777 | val RMSE 0.290120\n",
      "  h=4 lr=0.001 act=tanh -> val RMSE=0.29012\n",
      "Epoch 001 | train RMSE 0.282083 | val RMSE 0.281398\n",
      "Epoch 002 | train RMSE 0.280271 | val RMSE 0.280898\n",
      "Epoch 003 | train RMSE 0.278464 | val RMSE 0.280410\n",
      "Epoch 004 | train RMSE 0.276671 | val RMSE 0.279931\n",
      "Epoch 005 | train RMSE 0.274885 | val RMSE 0.279462\n",
      "Epoch 006 | train RMSE 0.273128 | val RMSE 0.279002\n",
      "Epoch 007 | train RMSE 0.271382 | val RMSE 0.278552\n",
      "Epoch 008 | train RMSE 0.269641 | val RMSE 0.278111\n",
      "Epoch 009 | train RMSE 0.267904 | val RMSE 0.277680\n",
      "Epoch 010 | train RMSE 0.266176 | val RMSE 0.277258\n",
      "  h=4 lr=0.001 act=relu -> val RMSE=0.27726\n",
      "Epoch 001 | train RMSE 0.451468 | val RMSE 0.359748\n",
      "Epoch 002 | train RMSE 0.415515 | val RMSE 0.347386\n",
      "Epoch 003 | train RMSE 0.379999 | val RMSE 0.336187\n",
      "Epoch 004 | train RMSE 0.345035 | val RMSE 0.326302\n",
      "Epoch 005 | train RMSE 0.310765 | val RMSE 0.317866\n",
      "Epoch 006 | train RMSE 0.277369 | val RMSE 0.311000\n",
      "Epoch 007 | train RMSE 0.245108 | val RMSE 0.305829\n",
      "Epoch 008 | train RMSE 0.214424 | val RMSE 0.302475\n",
      "Epoch 009 | train RMSE 0.186138 | val RMSE 0.301014\n",
      "Epoch 010 | train RMSE 0.161776 | val RMSE 0.301453\n",
      "  h=4 lr=0.01 act=linear -> val RMSE=0.30145\n",
      "Epoch 001 | train RMSE 0.451070 | val RMSE 0.607945\n",
      "Epoch 002 | train RMSE 0.407397 | val RMSE 0.587424\n",
      "Epoch 003 | train RMSE 0.364616 | val RMSE 0.566926\n",
      "Epoch 004 | train RMSE 0.323091 | val RMSE 0.546575\n",
      "Epoch 005 | train RMSE 0.283266 | val RMSE 0.526462\n",
      "Epoch 006 | train RMSE 0.245693 | val RMSE 0.506631\n",
      "Epoch 007 | train RMSE 0.211002 | val RMSE 0.487153\n",
      "Epoch 008 | train RMSE 0.179965 | val RMSE 0.468117\n",
      "Epoch 009 | train RMSE 0.153917 | val RMSE 0.449690\n",
      "Epoch 010 | train RMSE 0.134989 | val RMSE 0.432350\n",
      "  h=4 lr=0.01 act=tanh -> val RMSE=0.43235\n",
      "Epoch 001 | train RMSE 0.467405 | val RMSE 0.391280\n",
      "Epoch 002 | train RMSE 0.456671 | val RMSE 0.383310\n",
      "Epoch 003 | train RMSE 0.446567 | val RMSE 0.375494\n",
      "Epoch 004 | train RMSE 0.436430 | val RMSE 0.367746\n",
      "Epoch 005 | train RMSE 0.425942 | val RMSE 0.360036\n",
      "Epoch 006 | train RMSE 0.414989 | val RMSE 0.352361\n",
      "Epoch 007 | train RMSE 0.403602 | val RMSE 0.344732\n",
      "Epoch 008 | train RMSE 0.391759 | val RMSE 0.337217\n",
      "Epoch 009 | train RMSE 0.379395 | val RMSE 0.329817\n",
      "Epoch 010 | train RMSE 0.366461 | val RMSE 0.322616\n",
      "  h=4 lr=0.01 act=relu -> val RMSE=0.32262\n",
      "Epoch 001 | train RMSE 0.205385 | val RMSE 0.426441\n",
      "Epoch 002 | train RMSE 0.199657 | val RMSE 0.423921\n",
      "Epoch 003 | train RMSE 0.194020 | val RMSE 0.421429\n",
      "Epoch 004 | train RMSE 0.188483 | val RMSE 0.418965\n",
      "Epoch 005 | train RMSE 0.183057 | val RMSE 0.416532\n",
      "Epoch 006 | train RMSE 0.177753 | val RMSE 0.414129\n",
      "Epoch 007 | train RMSE 0.172583 | val RMSE 0.411757\n",
      "Epoch 008 | train RMSE 0.167559 | val RMSE 0.409416\n",
      "Epoch 009 | train RMSE 0.162695 | val RMSE 0.407106\n",
      "Epoch 010 | train RMSE 0.158005 | val RMSE 0.404824\n",
      "  h=8 lr=0.001 act=linear -> val RMSE=0.40482\n",
      "Epoch 001 | train RMSE 0.416059 | val RMSE 0.333981\n",
      "Epoch 002 | train RMSE 0.410282 | val RMSE 0.332562\n",
      "Epoch 003 | train RMSE 0.404527 | val RMSE 0.331168\n",
      "Epoch 004 | train RMSE 0.398792 | val RMSE 0.329800\n",
      "Epoch 005 | train RMSE 0.393080 | val RMSE 0.328458\n",
      "Epoch 006 | train RMSE 0.387391 | val RMSE 0.327142\n",
      "Epoch 007 | train RMSE 0.381725 | val RMSE 0.325851\n",
      "Epoch 008 | train RMSE 0.376082 | val RMSE 0.324586\n",
      "Epoch 009 | train RMSE 0.370464 | val RMSE 0.323347\n",
      "Epoch 010 | train RMSE 0.364871 | val RMSE 0.322132\n",
      "  h=8 lr=0.001 act=tanh -> val RMSE=0.32213\n",
      "Epoch 001 | train RMSE 0.420848 | val RMSE 0.358874\n",
      "Epoch 002 | train RMSE 0.417631 | val RMSE 0.357456\n",
      "Epoch 003 | train RMSE 0.414426 | val RMSE 0.356042\n",
      "Epoch 004 | train RMSE 0.411252 | val RMSE 0.354637\n",
      "Epoch 005 | train RMSE 0.408074 | val RMSE 0.353245\n",
      "Epoch 006 | train RMSE 0.404936 | val RMSE 0.351858\n",
      "Epoch 007 | train RMSE 0.401829 | val RMSE 0.350475\n",
      "Epoch 008 | train RMSE 0.398735 | val RMSE 0.349103\n",
      "Epoch 009 | train RMSE 0.395653 | val RMSE 0.347743\n",
      "Epoch 010 | train RMSE 0.392583 | val RMSE 0.346393\n",
      "  h=8 lr=0.001 act=relu -> val RMSE=0.34639\n",
      "Epoch 001 | train RMSE 0.251765 | val RMSE 0.406429\n",
      "Epoch 002 | train RMSE 0.187955 | val RMSE 0.379027\n",
      "Epoch 003 | train RMSE 0.137140 | val RMSE 0.354895\n",
      "Epoch 004 | train RMSE 0.117119 | val RMSE 0.339914\n",
      "Epoch 005 | train RMSE 0.125578 | val RMSE 0.336823\n",
      "Epoch 006 | train RMSE 0.131900 | val RMSE 0.340802\n",
      "Epoch 007 | train RMSE 0.128067 | val RMSE 0.348211\n",
      "Epoch 008 | train RMSE 0.117550 | val RMSE 0.357021\n",
      "Epoch 009 | train RMSE 0.105941 | val RMSE 0.365857\n",
      "Epoch 010 | train RMSE 0.098372 | val RMSE 0.373455\n",
      "  h=8 lr=0.01 act=linear -> val RMSE=0.37346\n",
      "Epoch 001 | train RMSE 0.237399 | val RMSE 0.434501\n",
      "Epoch 002 | train RMSE 0.178750 | val RMSE 0.408564\n",
      "Epoch 003 | train RMSE 0.128465 | val RMSE 0.386333\n",
      "Epoch 004 | train RMSE 0.098570 | val RMSE 0.368606\n",
      "Epoch 005 | train RMSE 0.102135 | val RMSE 0.358357\n",
      "Epoch 006 | train RMSE 0.117725 | val RMSE 0.355116\n",
      "Epoch 007 | train RMSE 0.124058 | val RMSE 0.356744\n",
      "Epoch 008 | train RMSE 0.119812 | val RMSE 0.361596\n",
      "Epoch 009 | train RMSE 0.109113 | val RMSE 0.368476\n",
      "Epoch 010 | train RMSE 0.097023 | val RMSE 0.376266\n",
      "  h=8 lr=0.01 act=tanh -> val RMSE=0.37627\n",
      "Epoch 001 | train RMSE 0.168135 | val RMSE 0.306084\n",
      "Epoch 002 | train RMSE 0.144908 | val RMSE 0.314436\n",
      "Epoch 003 | train RMSE 0.128707 | val RMSE 0.317578\n",
      "Epoch 004 | train RMSE 0.118426 | val RMSE 0.315186\n",
      "Epoch 005 | train RMSE 0.107941 | val RMSE 0.310089\n",
      "Epoch 006 | train RMSE 0.098336 | val RMSE 0.305054\n",
      "Epoch 007 | train RMSE 0.092281 | val RMSE 0.302659\n",
      "Epoch 008 | train RMSE 0.089691 | val RMSE 0.304153\n",
      "Epoch 009 | train RMSE 0.087557 | val RMSE 0.308638\n",
      "Epoch 010 | train RMSE 0.085329 | val RMSE 0.314737\n",
      "  h=8 lr=0.01 act=relu -> val RMSE=0.31474\n",
      "Epoch 001 | train RMSE 0.179497 | val RMSE 0.267239\n",
      "Epoch 002 | train RMSE 0.174771 | val RMSE 0.267702\n",
      "Epoch 003 | train RMSE 0.170200 | val RMSE 0.268226\n",
      "Epoch 004 | train RMSE 0.165792 | val RMSE 0.268819\n",
      "Epoch 005 | train RMSE 0.161558 | val RMSE 0.269482\n",
      "Epoch 006 | train RMSE 0.157505 | val RMSE 0.270218\n",
      "Epoch 007 | train RMSE 0.153632 | val RMSE 0.271028\n",
      "Epoch 008 | train RMSE 0.149933 | val RMSE 0.271904\n",
      "Epoch 009 | train RMSE 0.146399 | val RMSE 0.272836\n",
      "Epoch 010 | train RMSE 0.143018 | val RMSE 0.273809\n",
      "  h=16 lr=0.001 act=linear -> val RMSE=0.27381\n",
      "Epoch 001 | train RMSE 0.314058 | val RMSE 0.299191\n",
      "Epoch 002 | train RMSE 0.307684 | val RMSE 0.298188\n",
      "Epoch 003 | train RMSE 0.301380 | val RMSE 0.297231\n",
      "Epoch 004 | train RMSE 0.295146 | val RMSE 0.296319\n",
      "Epoch 005 | train RMSE 0.288978 | val RMSE 0.295455\n",
      "Epoch 006 | train RMSE 0.282877 | val RMSE 0.294646\n",
      "Epoch 007 | train RMSE 0.276845 | val RMSE 0.293894\n",
      "Epoch 008 | train RMSE 0.270884 | val RMSE 0.293201\n",
      "Epoch 009 | train RMSE 0.264997 | val RMSE 0.292570\n",
      "Epoch 010 | train RMSE 0.259187 | val RMSE 0.291999\n",
      "  h=16 lr=0.001 act=tanh -> val RMSE=0.29200\n",
      "Epoch 001 | train RMSE 0.282326 | val RMSE 0.296345\n",
      "Epoch 002 | train RMSE 0.277835 | val RMSE 0.295052\n",
      "Epoch 003 | train RMSE 0.273362 | val RMSE 0.293795\n",
      "Epoch 004 | train RMSE 0.268906 | val RMSE 0.292578\n",
      "Epoch 005 | train RMSE 0.264471 | val RMSE 0.291399\n",
      "Epoch 006 | train RMSE 0.260054 | val RMSE 0.290262\n",
      "Epoch 007 | train RMSE 0.255637 | val RMSE 0.289170\n",
      "Epoch 008 | train RMSE 0.251227 | val RMSE 0.288121\n",
      "Epoch 009 | train RMSE 0.246830 | val RMSE 0.287115\n",
      "Epoch 010 | train RMSE 0.242422 | val RMSE 0.286150\n",
      "  h=16 lr=0.001 act=relu -> val RMSE=0.28615\n",
      "Epoch 001 | train RMSE 0.166451 | val RMSE 0.348502\n",
      "Epoch 002 | train RMSE 0.130787 | val RMSE 0.355949\n",
      "Epoch 003 | train RMSE 0.107794 | val RMSE 0.358098\n",
      "Epoch 004 | train RMSE 0.098519 | val RMSE 0.368878\n",
      "Epoch 005 | train RMSE 0.096833 | val RMSE 0.382086\n",
      "Epoch 006 | train RMSE 0.096648 | val RMSE 0.389711\n",
      "Epoch 007 | train RMSE 0.095549 | val RMSE 0.390289\n",
      "Epoch 008 | train RMSE 0.092407 | val RMSE 0.388149\n",
      "Epoch 009 | train RMSE 0.089215 | val RMSE 0.386936\n",
      "Epoch 010 | train RMSE 0.085690 | val RMSE 0.386915\n",
      "  h=16 lr=0.01 act=linear -> val RMSE=0.38692\n",
      "Epoch 001 | train RMSE 0.261223 | val RMSE 0.285258\n",
      "Epoch 002 | train RMSE 0.192642 | val RMSE 0.284728\n",
      "Epoch 003 | train RMSE 0.147653 | val RMSE 0.289202\n",
      "Epoch 004 | train RMSE 0.134017 | val RMSE 0.294343\n",
      "Epoch 005 | train RMSE 0.132218 | val RMSE 0.295453\n",
      "Epoch 006 | train RMSE 0.125673 | val RMSE 0.292953\n",
      "Epoch 007 | train RMSE 0.115089 | val RMSE 0.289405\n",
      "Epoch 008 | train RMSE 0.107047 | val RMSE 0.286648\n",
      "Epoch 009 | train RMSE 0.105702 | val RMSE 0.285340\n",
      "Epoch 010 | train RMSE 0.108286 | val RMSE 0.285659\n",
      "  h=16 lr=0.01 act=tanh -> val RMSE=0.28566\n",
      "Epoch 001 | train RMSE 0.211313 | val RMSE 0.261948\n",
      "Epoch 002 | train RMSE 0.174058 | val RMSE 0.269038\n",
      "Epoch 003 | train RMSE 0.144325 | val RMSE 0.278930\n",
      "Epoch 004 | train RMSE 0.127575 | val RMSE 0.289362\n",
      "Epoch 005 | train RMSE 0.123748 | val RMSE 0.295969\n",
      "Epoch 006 | train RMSE 0.121302 | val RMSE 0.298313\n",
      "Epoch 007 | train RMSE 0.114130 | val RMSE 0.297382\n",
      "Epoch 008 | train RMSE 0.104644 | val RMSE 0.294613\n",
      "Epoch 009 | train RMSE 0.095532 | val RMSE 0.291189\n",
      "Epoch 010 | train RMSE 0.089218 | val RMSE 0.288142\n",
      "  h=16 lr=0.01 act=relu -> val RMSE=0.28814\n",
      "Tuning market=GOLD\n",
      "Epoch 001 | train RMSE 0.247470 | val RMSE 0.455893\n",
      "Epoch 002 | train RMSE 0.243512 | val RMSE 0.453970\n",
      "Epoch 003 | train RMSE 0.239588 | val RMSE 0.452057\n",
      "Epoch 004 | train RMSE 0.235697 | val RMSE 0.450154\n",
      "Epoch 005 | train RMSE 0.231842 | val RMSE 0.448262\n",
      "Epoch 006 | train RMSE 0.228023 | val RMSE 0.446380\n",
      "Epoch 007 | train RMSE 0.224242 | val RMSE 0.444508\n",
      "Epoch 008 | train RMSE 0.220500 | val RMSE 0.442644\n",
      "Epoch 009 | train RMSE 0.216795 | val RMSE 0.440788\n",
      "Epoch 010 | train RMSE 0.213129 | val RMSE 0.438940\n",
      "  h=4 lr=0.001 act=linear -> val RMSE=0.43894\n",
      "Epoch 001 | train RMSE 0.491099 | val RMSE 0.415389\n",
      "Epoch 002 | train RMSE 0.487101 | val RMSE 0.413760\n",
      "Epoch 003 | train RMSE 0.483116 | val RMSE 0.412140\n",
      "Epoch 004 | train RMSE 0.479145 | val RMSE 0.410531\n",
      "Epoch 005 | train RMSE 0.475190 | val RMSE 0.408931\n",
      "Epoch 006 | train RMSE 0.471248 | val RMSE 0.407342\n",
      "Epoch 007 | train RMSE 0.467322 | val RMSE 0.405763\n",
      "Epoch 008 | train RMSE 0.463412 | val RMSE 0.404195\n",
      "Epoch 009 | train RMSE 0.459517 | val RMSE 0.402638\n",
      "Epoch 010 | train RMSE 0.455637 | val RMSE 0.401092\n",
      "  h=4 lr=0.001 act=tanh -> val RMSE=0.40109\n",
      "Epoch 001 | train RMSE 0.145294 | val RMSE 0.281557\n",
      "Epoch 002 | train RMSE 0.143030 | val RMSE 0.282287\n",
      "Epoch 003 | train RMSE 0.140809 | val RMSE 0.283011\n",
      "Epoch 004 | train RMSE 0.138632 | val RMSE 0.283734\n",
      "Epoch 005 | train RMSE 0.136515 | val RMSE 0.284457\n",
      "Epoch 006 | train RMSE 0.134472 | val RMSE 0.285184\n",
      "Epoch 007 | train RMSE 0.132474 | val RMSE 0.285915\n",
      "Epoch 008 | train RMSE 0.130525 | val RMSE 0.286649\n",
      "Epoch 009 | train RMSE 0.128625 | val RMSE 0.287397\n",
      "Epoch 010 | train RMSE 0.126777 | val RMSE 0.288156\n",
      "  h=4 lr=0.001 act=relu -> val RMSE=0.28816\n",
      "Epoch 001 | train RMSE 0.285172 | val RMSE 0.482276\n",
      "Epoch 002 | train RMSE 0.253823 | val RMSE 0.462931\n",
      "Epoch 003 | train RMSE 0.222681 | val RMSE 0.444582\n",
      "Epoch 004 | train RMSE 0.192213 | val RMSE 0.426939\n",
      "Epoch 005 | train RMSE 0.163425 | val RMSE 0.410008\n",
      "Epoch 006 | train RMSE 0.138248 | val RMSE 0.393956\n",
      "Epoch 007 | train RMSE 0.119965 | val RMSE 0.379117\n",
      "Epoch 008 | train RMSE 0.112230 | val RMSE 0.366511\n",
      "Epoch 009 | train RMSE 0.114760 | val RMSE 0.357629\n",
      "Epoch 010 | train RMSE 0.121491 | val RMSE 0.352978\n",
      "  h=4 lr=0.01 act=linear -> val RMSE=0.35298\n",
      "Epoch 001 | train RMSE 0.222882 | val RMSE 0.269611\n",
      "Epoch 002 | train RMSE 0.200047 | val RMSE 0.270576\n",
      "Epoch 003 | train RMSE 0.180387 | val RMSE 0.272607\n",
      "Epoch 004 | train RMSE 0.164153 | val RMSE 0.275382\n",
      "Epoch 005 | train RMSE 0.151574 | val RMSE 0.278709\n",
      "Epoch 006 | train RMSE 0.142753 | val RMSE 0.282352\n",
      "Epoch 007 | train RMSE 0.137354 | val RMSE 0.285962\n",
      "Epoch 008 | train RMSE 0.134294 | val RMSE 0.289157\n",
      "Epoch 009 | train RMSE 0.132178 | val RMSE 0.291652\n",
      "Epoch 010 | train RMSE 0.129979 | val RMSE 0.293312\n",
      "  h=4 lr=0.01 act=tanh -> val RMSE=0.29331\n",
      "Epoch 001 | train RMSE 0.407786 | val RMSE 0.345894\n",
      "Epoch 002 | train RMSE 0.389809 | val RMSE 0.336903\n",
      "Epoch 003 | train RMSE 0.373319 | val RMSE 0.328579\n",
      "Epoch 004 | train RMSE 0.358488 | val RMSE 0.320828\n",
      "Epoch 005 | train RMSE 0.345457 | val RMSE 0.313648\n",
      "Epoch 006 | train RMSE 0.334066 | val RMSE 0.307027\n",
      "Epoch 007 | train RMSE 0.323868 | val RMSE 0.300930\n",
      "Epoch 008 | train RMSE 0.314081 | val RMSE 0.295365\n",
      "Epoch 009 | train RMSE 0.304566 | val RMSE 0.290260\n",
      "Epoch 010 | train RMSE 0.295320 | val RMSE 0.285652\n",
      "  h=4 lr=0.01 act=relu -> val RMSE=0.28565\n",
      "Epoch 001 | train RMSE 0.197578 | val RMSE 0.282517\n",
      "Epoch 002 | train RMSE 0.192954 | val RMSE 0.283124\n",
      "Epoch 003 | train RMSE 0.188398 | val RMSE 0.283777\n",
      "Epoch 004 | train RMSE 0.183916 | val RMSE 0.284476\n",
      "Epoch 005 | train RMSE 0.179517 | val RMSE 0.285222\n",
      "Epoch 006 | train RMSE 0.175209 | val RMSE 0.286014\n",
      "Epoch 007 | train RMSE 0.170999 | val RMSE 0.286852\n",
      "Epoch 008 | train RMSE 0.166898 | val RMSE 0.287735\n",
      "Epoch 009 | train RMSE 0.162914 | val RMSE 0.288661\n",
      "Epoch 010 | train RMSE 0.159058 | val RMSE 0.289629\n",
      "  h=8 lr=0.001 act=linear -> val RMSE=0.28963\n",
      "Epoch 001 | train RMSE 0.252852 | val RMSE 0.281372\n",
      "Epoch 002 | train RMSE 0.249222 | val RMSE 0.281148\n",
      "Epoch 003 | train RMSE 0.245655 | val RMSE 0.280983\n",
      "Epoch 004 | train RMSE 0.242148 | val RMSE 0.280869\n",
      "Epoch 005 | train RMSE 0.238699 | val RMSE 0.280803\n",
      "Epoch 006 | train RMSE 0.235306 | val RMSE 0.280778\n",
      "Epoch 007 | train RMSE 0.231969 | val RMSE 0.280786\n",
      "Epoch 008 | train RMSE 0.228686 | val RMSE 0.280820\n",
      "Epoch 009 | train RMSE 0.225459 | val RMSE 0.280874\n",
      "Epoch 010 | train RMSE 0.222287 | val RMSE 0.280945\n",
      "  h=8 lr=0.001 act=tanh -> val RMSE=0.28095\n",
      "Epoch 001 | train RMSE 0.230338 | val RMSE 0.271496\n",
      "Epoch 002 | train RMSE 0.226975 | val RMSE 0.271539\n",
      "Epoch 003 | train RMSE 0.223636 | val RMSE 0.271615\n",
      "Epoch 004 | train RMSE 0.220332 | val RMSE 0.271717\n",
      "Epoch 005 | train RMSE 0.217043 | val RMSE 0.271841\n",
      "Epoch 006 | train RMSE 0.213795 | val RMSE 0.271987\n",
      "Epoch 007 | train RMSE 0.210596 | val RMSE 0.272148\n",
      "Epoch 008 | train RMSE 0.207405 | val RMSE 0.272330\n",
      "Epoch 009 | train RMSE 0.204267 | val RMSE 0.272536\n",
      "Epoch 010 | train RMSE 0.201185 | val RMSE 0.272763\n",
      "  h=8 lr=0.001 act=relu -> val RMSE=0.27276\n",
      "Epoch 001 | train RMSE 0.211737 | val RMSE 0.358755\n",
      "Epoch 002 | train RMSE 0.177105 | val RMSE 0.341290\n",
      "Epoch 003 | train RMSE 0.149108 | val RMSE 0.326601\n",
      "Epoch 004 | train RMSE 0.130987 | val RMSE 0.316728\n",
      "Epoch 005 | train RMSE 0.122383 | val RMSE 0.313237\n",
      "Epoch 006 | train RMSE 0.116236 | val RMSE 0.314724\n",
      "Epoch 007 | train RMSE 0.108597 | val RMSE 0.319391\n",
      "Epoch 008 | train RMSE 0.100668 | val RMSE 0.325881\n",
      "Epoch 009 | train RMSE 0.095036 | val RMSE 0.332947\n",
      "Epoch 010 | train RMSE 0.093143 | val RMSE 0.339285\n",
      "  h=8 lr=0.01 act=linear -> val RMSE=0.33928\n",
      "Epoch 001 | train RMSE 0.279996 | val RMSE 0.410360\n",
      "Epoch 002 | train RMSE 0.233383 | val RMSE 0.385984\n",
      "Epoch 003 | train RMSE 0.191194 | val RMSE 0.364268\n",
      "Epoch 004 | train RMSE 0.153969 | val RMSE 0.344799\n",
      "Epoch 005 | train RMSE 0.123944 | val RMSE 0.327765\n",
      "Epoch 006 | train RMSE 0.105020 | val RMSE 0.313995\n",
      "Epoch 007 | train RMSE 0.100774 | val RMSE 0.304926\n",
      "Epoch 008 | train RMSE 0.107241 | val RMSE 0.300856\n",
      "Epoch 009 | train RMSE 0.114834 | val RMSE 0.300548\n",
      "Epoch 010 | train RMSE 0.118439 | val RMSE 0.302722\n",
      "  h=8 lr=0.01 act=tanh -> val RMSE=0.30272\n",
      "Epoch 001 | train RMSE 0.429178 | val RMSE 0.354049\n",
      "Epoch 002 | train RMSE 0.397321 | val RMSE 0.342707\n",
      "Epoch 003 | train RMSE 0.365640 | val RMSE 0.332338\n",
      "Epoch 004 | train RMSE 0.332732 | val RMSE 0.322833\n",
      "Epoch 005 | train RMSE 0.298275 | val RMSE 0.314166\n",
      "Epoch 006 | train RMSE 0.263875 | val RMSE 0.306522\n",
      "Epoch 007 | train RMSE 0.230213 | val RMSE 0.299872\n",
      "Epoch 008 | train RMSE 0.199553 | val RMSE 0.294517\n",
      "Epoch 009 | train RMSE 0.173090 | val RMSE 0.290469\n",
      "Epoch 010 | train RMSE 0.154200 | val RMSE 0.288086\n",
      "  h=8 lr=0.01 act=relu -> val RMSE=0.28809\n",
      "Epoch 001 | train RMSE 0.165014 | val RMSE 0.345848\n",
      "Epoch 002 | train RMSE 0.161221 | val RMSE 0.343419\n",
      "Epoch 003 | train RMSE 0.157603 | val RMSE 0.341069\n",
      "Epoch 004 | train RMSE 0.154172 | val RMSE 0.338825\n",
      "Epoch 005 | train RMSE 0.150936 | val RMSE 0.336724\n",
      "Epoch 006 | train RMSE 0.147897 | val RMSE 0.334814\n",
      "Epoch 007 | train RMSE 0.145051 | val RMSE 0.333151\n",
      "Epoch 008 | train RMSE 0.142381 | val RMSE 0.331789\n",
      "Epoch 009 | train RMSE 0.139864 | val RMSE 0.330766\n",
      "Epoch 010 | train RMSE 0.137467 | val RMSE 0.330094\n",
      "  h=16 lr=0.001 act=linear -> val RMSE=0.33009\n",
      "Epoch 001 | train RMSE 0.149584 | val RMSE 0.312044\n",
      "Epoch 002 | train RMSE 0.147079 | val RMSE 0.313285\n",
      "Epoch 003 | train RMSE 0.144652 | val RMSE 0.314506\n",
      "Epoch 004 | train RMSE 0.142305 | val RMSE 0.315694\n",
      "Epoch 005 | train RMSE 0.140036 | val RMSE 0.316844\n",
      "Epoch 006 | train RMSE 0.137844 | val RMSE 0.317939\n",
      "Epoch 007 | train RMSE 0.135726 | val RMSE 0.318960\n",
      "Epoch 008 | train RMSE 0.133677 | val RMSE 0.319887\n",
      "Epoch 009 | train RMSE 0.131692 | val RMSE 0.320705\n",
      "Epoch 010 | train RMSE 0.129766 | val RMSE 0.321401\n",
      "  h=16 lr=0.001 act=tanh -> val RMSE=0.32140\n",
      "Epoch 001 | train RMSE 0.162187 | val RMSE 0.305691\n",
      "Epoch 002 | train RMSE 0.158719 | val RMSE 0.306994\n",
      "Epoch 003 | train RMSE 0.155484 | val RMSE 0.308312\n",
      "Epoch 004 | train RMSE 0.152348 | val RMSE 0.309665\n",
      "Epoch 005 | train RMSE 0.149297 | val RMSE 0.311059\n",
      "Epoch 006 | train RMSE 0.146364 | val RMSE 0.312484\n",
      "Epoch 007 | train RMSE 0.143628 | val RMSE 0.313956\n",
      "Epoch 008 | train RMSE 0.141018 | val RMSE 0.315449\n",
      "Epoch 009 | train RMSE 0.138479 | val RMSE 0.316952\n",
      "Epoch 010 | train RMSE 0.136026 | val RMSE 0.318472\n",
      "  h=16 lr=0.001 act=relu -> val RMSE=0.31847\n",
      "Epoch 001 | train RMSE 0.136118 | val RMSE 0.305028\n",
      "Epoch 002 | train RMSE 0.126111 | val RMSE 0.318105\n",
      "Epoch 003 | train RMSE 0.107600 | val RMSE 0.333563\n",
      "Epoch 004 | train RMSE 0.100553 | val RMSE 0.340561\n",
      "Epoch 005 | train RMSE 0.093644 | val RMSE 0.341190\n",
      "Epoch 006 | train RMSE 0.087295 | val RMSE 0.342134\n",
      "Epoch 007 | train RMSE 0.088800 | val RMSE 0.347846\n",
      "Epoch 008 | train RMSE 0.089527 | val RMSE 0.357205\n",
      "Epoch 009 | train RMSE 0.086378 | val RMSE 0.367014\n",
      "Epoch 010 | train RMSE 0.083169 | val RMSE 0.373539\n",
      "  h=16 lr=0.01 act=linear -> val RMSE=0.37354\n",
      "Epoch 001 | train RMSE 0.254433 | val RMSE 0.286838\n",
      "Epoch 002 | train RMSE 0.184671 | val RMSE 0.297040\n",
      "Epoch 003 | train RMSE 0.130453 | val RMSE 0.313012\n",
      "Epoch 004 | train RMSE 0.111503 | val RMSE 0.328717\n",
      "Epoch 005 | train RMSE 0.125554 | val RMSE 0.337905\n",
      "Epoch 006 | train RMSE 0.136499 | val RMSE 0.340436\n",
      "Epoch 007 | train RMSE 0.135260 | val RMSE 0.338221\n",
      "Epoch 008 | train RMSE 0.125687 | val RMSE 0.333199\n",
      "Epoch 009 | train RMSE 0.113227 | val RMSE 0.327046\n",
      "Epoch 010 | train RMSE 0.103107 | val RMSE 0.321166\n",
      "  h=16 lr=0.01 act=tanh -> val RMSE=0.32117\n",
      "Epoch 001 | train RMSE 0.225530 | val RMSE 0.362620\n",
      "Epoch 002 | train RMSE 0.186752 | val RMSE 0.344524\n",
      "Epoch 003 | train RMSE 0.154120 | val RMSE 0.328541\n",
      "Epoch 004 | train RMSE 0.129620 | val RMSE 0.315071\n",
      "Epoch 005 | train RMSE 0.113772 | val RMSE 0.304571\n",
      "Epoch 006 | train RMSE 0.108248 | val RMSE 0.298129\n",
      "Epoch 007 | train RMSE 0.109253 | val RMSE 0.295611\n",
      "Epoch 008 | train RMSE 0.108748 | val RMSE 0.295902\n",
      "Epoch 009 | train RMSE 0.104154 | val RMSE 0.298041\n",
      "Epoch 010 | train RMSE 0.097271 | val RMSE 0.301504\n",
      "  h=16 lr=0.01 act=relu -> val RMSE=0.30150\n",
      "Tuning market=ENERGY\n",
      "Epoch 001 | train RMSE 0.229949 | val RMSE 0.424429\n",
      "Epoch 002 | train RMSE 0.226255 | val RMSE 0.422609\n",
      "Epoch 003 | train RMSE 0.222565 | val RMSE 0.420793\n",
      "Epoch 004 | train RMSE 0.218880 | val RMSE 0.418979\n",
      "Epoch 005 | train RMSE 0.215202 | val RMSE 0.417169\n",
      "Epoch 006 | train RMSE 0.211532 | val RMSE 0.415363\n",
      "Epoch 007 | train RMSE 0.207870 | val RMSE 0.413559\n",
      "Epoch 008 | train RMSE 0.204218 | val RMSE 0.411760\n",
      "Epoch 009 | train RMSE 0.200577 | val RMSE 0.409965\n",
      "Epoch 010 | train RMSE 0.196948 | val RMSE 0.408174\n",
      "  h=4 lr=0.001 act=linear -> val RMSE=0.40817\n",
      "Epoch 001 | train RMSE 0.402786 | val RMSE 0.632518\n",
      "Epoch 002 | train RMSE 0.399822 | val RMSE 0.630777\n",
      "Epoch 003 | train RMSE 0.396861 | val RMSE 0.629037\n",
      "Epoch 004 | train RMSE 0.393904 | val RMSE 0.627300\n",
      "Epoch 005 | train RMSE 0.390951 | val RMSE 0.625566\n",
      "Epoch 006 | train RMSE 0.388001 | val RMSE 0.623836\n",
      "Epoch 007 | train RMSE 0.385054 | val RMSE 0.622110\n",
      "Epoch 008 | train RMSE 0.382107 | val RMSE 0.620388\n",
      "Epoch 009 | train RMSE 0.379161 | val RMSE 0.618670\n",
      "Epoch 010 | train RMSE 0.376211 | val RMSE 0.616956\n",
      "  h=4 lr=0.001 act=tanh -> val RMSE=0.61696\n",
      "Epoch 001 | train RMSE 0.170673 | val RMSE 0.391730\n",
      "Epoch 002 | train RMSE 0.169027 | val RMSE 0.390502\n",
      "Epoch 003 | train RMSE 0.167395 | val RMSE 0.389278\n",
      "Epoch 004 | train RMSE 0.165779 | val RMSE 0.388059\n",
      "Epoch 005 | train RMSE 0.164179 | val RMSE 0.386848\n",
      "Epoch 006 | train RMSE 0.162596 | val RMSE 0.385645\n",
      "Epoch 007 | train RMSE 0.161030 | val RMSE 0.384448\n",
      "Epoch 008 | train RMSE 0.159482 | val RMSE 0.383257\n",
      "Epoch 009 | train RMSE 0.157951 | val RMSE 0.382072\n",
      "Epoch 010 | train RMSE 0.156437 | val RMSE 0.380896\n",
      "  h=4 lr=0.001 act=relu -> val RMSE=0.38090\n",
      "Epoch 001 | train RMSE 0.224797 | val RMSE 0.267482\n",
      "Epoch 002 | train RMSE 0.186252 | val RMSE 0.269878\n",
      "Epoch 003 | train RMSE 0.153868 | val RMSE 0.274088\n",
      "Epoch 004 | train RMSE 0.131280 | val RMSE 0.279549\n",
      "Epoch 005 | train RMSE 0.120397 | val RMSE 0.285200\n",
      "Epoch 006 | train RMSE 0.119035 | val RMSE 0.289268\n",
      "Epoch 007 | train RMSE 0.119537 | val RMSE 0.290854\n",
      "Epoch 008 | train RMSE 0.117012 | val RMSE 0.290067\n",
      "Epoch 009 | train RMSE 0.111340 | val RMSE 0.287613\n",
      "Epoch 010 | train RMSE 0.104690 | val RMSE 0.284398\n",
      "  h=4 lr=0.01 act=linear -> val RMSE=0.28440\n",
      "Epoch 001 | train RMSE 0.347128 | val RMSE 0.492207\n",
      "Epoch 002 | train RMSE 0.308072 | val RMSE 0.471725\n",
      "Epoch 003 | train RMSE 0.270986 | val RMSE 0.452254\n",
      "Epoch 004 | train RMSE 0.236179 | val RMSE 0.433969\n",
      "Epoch 005 | train RMSE 0.203812 | val RMSE 0.416693\n",
      "Epoch 006 | train RMSE 0.174124 | val RMSE 0.400235\n",
      "Epoch 007 | train RMSE 0.147761 | val RMSE 0.384556\n",
      "Epoch 008 | train RMSE 0.126144 | val RMSE 0.369837\n",
      "Epoch 009 | train RMSE 0.111535 | val RMSE 0.356558\n",
      "Epoch 010 | train RMSE 0.105754 | val RMSE 0.345477\n",
      "  h=4 lr=0.01 act=tanh -> val RMSE=0.34548\n",
      "Epoch 001 | train RMSE 0.650944 | val RMSE 0.465659\n",
      "Epoch 002 | train RMSE 0.610382 | val RMSE 0.449571\n",
      "Epoch 003 | train RMSE 0.570952 | val RMSE 0.434132\n",
      "Epoch 004 | train RMSE 0.533091 | val RMSE 0.419285\n",
      "Epoch 005 | train RMSE 0.498053 | val RMSE 0.404514\n",
      "Epoch 006 | train RMSE 0.466300 | val RMSE 0.390497\n",
      "Epoch 007 | train RMSE 0.436213 | val RMSE 0.377248\n",
      "Epoch 008 | train RMSE 0.407631 | val RMSE 0.364759\n",
      "Epoch 009 | train RMSE 0.383135 | val RMSE 0.353050\n",
      "Epoch 010 | train RMSE 0.360079 | val RMSE 0.342261\n",
      "  h=4 lr=0.01 act=relu -> val RMSE=0.34226\n",
      "Epoch 001 | train RMSE 0.142261 | val RMSE 0.358516\n",
      "Epoch 002 | train RMSE 0.138810 | val RMSE 0.356874\n",
      "Epoch 003 | train RMSE 0.135433 | val RMSE 0.355218\n",
      "Epoch 004 | train RMSE 0.132139 | val RMSE 0.353570\n",
      "Epoch 005 | train RMSE 0.128936 | val RMSE 0.351940\n",
      "Epoch 006 | train RMSE 0.125837 | val RMSE 0.350331\n",
      "Epoch 007 | train RMSE 0.122853 | val RMSE 0.348749\n",
      "Epoch 008 | train RMSE 0.119994 | val RMSE 0.347196\n",
      "Epoch 009 | train RMSE 0.117273 | val RMSE 0.345677\n",
      "Epoch 010 | train RMSE 0.114702 | val RMSE 0.344196\n",
      "  h=8 lr=0.001 act=linear -> val RMSE=0.34420\n",
      "Epoch 001 | train RMSE 0.552230 | val RMSE 0.478228\n",
      "Epoch 002 | train RMSE 0.545249 | val RMSE 0.475437\n",
      "Epoch 003 | train RMSE 0.538296 | val RMSE 0.472669\n",
      "Epoch 004 | train RMSE 0.531373 | val RMSE 0.469926\n",
      "Epoch 005 | train RMSE 0.524479 | val RMSE 0.467208\n",
      "Epoch 006 | train RMSE 0.517616 | val RMSE 0.464515\n",
      "Epoch 007 | train RMSE 0.510785 | val RMSE 0.461849\n",
      "Epoch 008 | train RMSE 0.503986 | val RMSE 0.459209\n",
      "Epoch 009 | train RMSE 0.497221 | val RMSE 0.456597\n",
      "Epoch 010 | train RMSE 0.490489 | val RMSE 0.454014\n",
      "  h=8 lr=0.001 act=tanh -> val RMSE=0.45401\n",
      "Epoch 001 | train RMSE 0.118439 | val RMSE 0.313632\n",
      "Epoch 002 | train RMSE 0.117775 | val RMSE 0.314225\n",
      "Epoch 003 | train RMSE 0.117127 | val RMSE 0.314811\n",
      "Epoch 004 | train RMSE 0.116478 | val RMSE 0.315392\n",
      "Epoch 005 | train RMSE 0.115830 | val RMSE 0.315969\n",
      "Epoch 006 | train RMSE 0.115181 | val RMSE 0.316542\n",
      "Epoch 007 | train RMSE 0.114523 | val RMSE 0.317108\n",
      "Epoch 008 | train RMSE 0.113879 | val RMSE 0.317667\n",
      "Epoch 009 | train RMSE 0.113249 | val RMSE 0.318225\n",
      "Epoch 010 | train RMSE 0.112623 | val RMSE 0.318780\n",
      "  h=8 lr=0.001 act=relu -> val RMSE=0.31878\n",
      "Epoch 001 | train RMSE 0.187539 | val RMSE 0.323167\n",
      "Epoch 002 | train RMSE 0.140767 | val RMSE 0.335180\n",
      "Epoch 003 | train RMSE 0.121898 | val RMSE 0.345631\n",
      "Epoch 004 | train RMSE 0.125001 | val RMSE 0.351281\n",
      "Epoch 005 | train RMSE 0.128043 | val RMSE 0.352093\n",
      "Epoch 006 | train RMSE 0.124085 | val RMSE 0.349887\n",
      "Epoch 007 | train RMSE 0.115514 | val RMSE 0.346310\n",
      "Epoch 008 | train RMSE 0.106501 | val RMSE 0.342666\n",
      "Epoch 009 | train RMSE 0.100357 | val RMSE 0.340034\n",
      "Epoch 010 | train RMSE 0.097789 | val RMSE 0.339139\n",
      "  h=8 lr=0.01 act=linear -> val RMSE=0.33914\n",
      "Epoch 001 | train RMSE 0.258948 | val RMSE 0.324815\n",
      "Epoch 002 | train RMSE 0.198502 | val RMSE 0.318223\n",
      "Epoch 003 | train RMSE 0.151682 | val RMSE 0.315987\n",
      "Epoch 004 | train RMSE 0.128360 | val RMSE 0.315816\n",
      "Epoch 005 | train RMSE 0.128212 | val RMSE 0.315815\n",
      "Epoch 006 | train RMSE 0.134881 | val RMSE 0.314771\n",
      "Epoch 007 | train RMSE 0.136793 | val RMSE 0.312646\n",
      "Epoch 008 | train RMSE 0.132937 | val RMSE 0.309918\n",
      "Epoch 009 | train RMSE 0.125915 | val RMSE 0.307071\n",
      "Epoch 010 | train RMSE 0.118588 | val RMSE 0.304392\n",
      "  h=8 lr=0.01 act=tanh -> val RMSE=0.30439\n",
      "Epoch 001 | train RMSE 0.180095 | val RMSE 0.379191\n",
      "Epoch 002 | train RMSE 0.159205 | val RMSE 0.365895\n",
      "Epoch 003 | train RMSE 0.142928 | val RMSE 0.354141\n",
      "Epoch 004 | train RMSE 0.132196 | val RMSE 0.344274\n",
      "Epoch 005 | train RMSE 0.127467 | val RMSE 0.336402\n",
      "Epoch 006 | train RMSE 0.124241 | val RMSE 0.330313\n",
      "Epoch 007 | train RMSE 0.121746 | val RMSE 0.325869\n",
      "Epoch 008 | train RMSE 0.119119 | val RMSE 0.322837\n",
      "Epoch 009 | train RMSE 0.116122 | val RMSE 0.320876\n",
      "Epoch 010 | train RMSE 0.113028 | val RMSE 0.320082\n",
      "  h=8 lr=0.01 act=relu -> val RMSE=0.32008\n",
      "Epoch 001 | train RMSE 0.188540 | val RMSE 0.346165\n",
      "Epoch 002 | train RMSE 0.181571 | val RMSE 0.343373\n",
      "Epoch 003 | train RMSE 0.174833 | val RMSE 0.340877\n",
      "Epoch 004 | train RMSE 0.168328 | val RMSE 0.338568\n",
      "Epoch 005 | train RMSE 0.162070 | val RMSE 0.336373\n",
      "Epoch 006 | train RMSE 0.156082 | val RMSE 0.334268\n",
      "Epoch 007 | train RMSE 0.150391 | val RMSE 0.332243\n",
      "Epoch 008 | train RMSE 0.145025 | val RMSE 0.330295\n",
      "Epoch 009 | train RMSE 0.140011 | val RMSE 0.328428\n",
      "Epoch 010 | train RMSE 0.135376 | val RMSE 0.326646\n",
      "  h=16 lr=0.001 act=linear -> val RMSE=0.32665\n",
      "Epoch 001 | train RMSE 0.304366 | val RMSE 0.327289\n",
      "Epoch 002 | train RMSE 0.297096 | val RMSE 0.325499\n",
      "Epoch 003 | train RMSE 0.289867 | val RMSE 0.323770\n",
      "Epoch 004 | train RMSE 0.282679 | val RMSE 0.322090\n",
      "Epoch 005 | train RMSE 0.275537 | val RMSE 0.320457\n",
      "Epoch 006 | train RMSE 0.268445 | val RMSE 0.318872\n",
      "Epoch 007 | train RMSE 0.261408 | val RMSE 0.317334\n",
      "Epoch 008 | train RMSE 0.254433 | val RMSE 0.315845\n",
      "Epoch 009 | train RMSE 0.247527 | val RMSE 0.314405\n",
      "Epoch 010 | train RMSE 0.240695 | val RMSE 0.313015\n",
      "  h=16 lr=0.001 act=tanh -> val RMSE=0.31301\n",
      "Epoch 001 | train RMSE 0.113266 | val RMSE 0.328814\n",
      "Epoch 002 | train RMSE 0.111507 | val RMSE 0.328551\n",
      "Epoch 003 | train RMSE 0.109800 | val RMSE 0.328691\n",
      "Epoch 004 | train RMSE 0.108127 | val RMSE 0.328989\n",
      "Epoch 005 | train RMSE 0.106492 | val RMSE 0.329342\n",
      "Epoch 006 | train RMSE 0.104881 | val RMSE 0.329697\n",
      "Epoch 007 | train RMSE 0.103308 | val RMSE 0.330030\n",
      "Epoch 008 | train RMSE 0.101773 | val RMSE 0.330335\n",
      "Epoch 009 | train RMSE 0.100276 | val RMSE 0.330605\n",
      "Epoch 010 | train RMSE 0.098828 | val RMSE 0.330844\n",
      "  h=16 lr=0.001 act=relu -> val RMSE=0.33084\n",
      "Epoch 001 | train RMSE 0.293512 | val RMSE 0.274926\n",
      "Epoch 002 | train RMSE 0.216515 | val RMSE 0.282870\n",
      "Epoch 003 | train RMSE 0.154689 | val RMSE 0.297874\n",
      "Epoch 004 | train RMSE 0.126940 | val RMSE 0.313079\n",
      "Epoch 005 | train RMSE 0.136527 | val RMSE 0.321407\n",
      "Epoch 006 | train RMSE 0.147056 | val RMSE 0.322571\n",
      "Epoch 007 | train RMSE 0.144942 | val RMSE 0.319002\n",
      "Epoch 008 | train RMSE 0.133475 | val RMSE 0.313176\n",
      "Epoch 009 | train RMSE 0.118798 | val RMSE 0.306983\n",
      "Epoch 010 | train RMSE 0.107107 | val RMSE 0.301831\n",
      "  h=16 lr=0.01 act=linear -> val RMSE=0.30183\n",
      "Epoch 001 | train RMSE 0.255159 | val RMSE 0.395616\n",
      "Epoch 002 | train RMSE 0.183620 | val RMSE 0.365878\n",
      "Epoch 003 | train RMSE 0.127126 | val RMSE 0.339504\n",
      "Epoch 004 | train RMSE 0.100584 | val RMSE 0.321451\n",
      "Epoch 005 | train RMSE 0.106524 | val RMSE 0.315115\n",
      "Epoch 006 | train RMSE 0.114962 | val RMSE 0.316124\n",
      "Epoch 007 | train RMSE 0.114242 | val RMSE 0.320814\n",
      "Epoch 008 | train RMSE 0.107212 | val RMSE 0.326911\n",
      "Epoch 009 | train RMSE 0.098838 | val RMSE 0.332965\n",
      "Epoch 010 | train RMSE 0.093155 | val RMSE 0.337876\n",
      "  h=16 lr=0.01 act=tanh -> val RMSE=0.33788\n",
      "Epoch 001 | train RMSE 0.411603 | val RMSE 0.344788\n",
      "Epoch 002 | train RMSE 0.361477 | val RMSE 0.328676\n",
      "Epoch 003 | train RMSE 0.316314 | val RMSE 0.315342\n",
      "Epoch 004 | train RMSE 0.277481 | val RMSE 0.304952\n",
      "Epoch 005 | train RMSE 0.243332 | val RMSE 0.297211\n",
      "Epoch 006 | train RMSE 0.213222 | val RMSE 0.291825\n",
      "Epoch 007 | train RMSE 0.186168 | val RMSE 0.288542\n",
      "Epoch 008 | train RMSE 0.162418 | val RMSE 0.287277\n",
      "Epoch 009 | train RMSE 0.142038 | val RMSE 0.288052\n",
      "Epoch 010 | train RMSE 0.126800 | val RMSE 0.290540\n",
      "  h=16 lr=0.01 act=relu -> val RMSE=0.29054\n",
      "Tuning market=FIAT+EQUITY\n",
      "Epoch 001 | train RMSE 0.181181 | val RMSE 0.287937\n",
      "Epoch 002 | train RMSE 0.178074 | val RMSE 0.288750\n",
      "Epoch 003 | train RMSE 0.175026 | val RMSE 0.289570\n",
      "Epoch 004 | train RMSE 0.172043 | val RMSE 0.290384\n",
      "Epoch 005 | train RMSE 0.169125 | val RMSE 0.291181\n",
      "Epoch 006 | train RMSE 0.166276 | val RMSE 0.291959\n",
      "Epoch 007 | train RMSE 0.163496 | val RMSE 0.292724\n",
      "Epoch 008 | train RMSE 0.160788 | val RMSE 0.293480\n",
      "Epoch 009 | train RMSE 0.158153 | val RMSE 0.294231\n",
      "Epoch 010 | train RMSE 0.155590 | val RMSE 0.294979\n",
      "  h=4 lr=0.001 act=linear -> val RMSE=0.29498\n",
      "Epoch 001 | train RMSE 0.472593 | val RMSE 0.463458\n",
      "Epoch 002 | train RMSE 0.467819 | val RMSE 0.461305\n",
      "Epoch 003 | train RMSE 0.463053 | val RMSE 0.459188\n",
      "Epoch 004 | train RMSE 0.458295 | val RMSE 0.457130\n",
      "Epoch 005 | train RMSE 0.453542 | val RMSE 0.455124\n",
      "Epoch 006 | train RMSE 0.448792 | val RMSE 0.453154\n",
      "Epoch 007 | train RMSE 0.444042 | val RMSE 0.451207\n",
      "Epoch 008 | train RMSE 0.439292 | val RMSE 0.449278\n",
      "Epoch 009 | train RMSE 0.434543 | val RMSE 0.447363\n",
      "Epoch 010 | train RMSE 0.429795 | val RMSE 0.445460\n",
      "  h=4 lr=0.001 act=tanh -> val RMSE=0.44546\n",
      "Epoch 001 | train RMSE 0.357800 | val RMSE 0.554341\n",
      "Epoch 002 | train RMSE 0.355339 | val RMSE 0.552836\n",
      "Epoch 003 | train RMSE 0.352887 | val RMSE 0.551335\n",
      "Epoch 004 | train RMSE 0.350449 | val RMSE 0.549840\n",
      "Epoch 005 | train RMSE 0.348020 | val RMSE 0.548351\n",
      "Epoch 006 | train RMSE 0.345600 | val RMSE 0.546869\n",
      "Epoch 007 | train RMSE 0.343206 | val RMSE 0.545390\n",
      "Epoch 008 | train RMSE 0.340830 | val RMSE 0.543923\n",
      "Epoch 009 | train RMSE 0.338463 | val RMSE 0.542463\n",
      "Epoch 010 | train RMSE 0.336110 | val RMSE 0.541006\n",
      "  h=4 lr=0.001 act=relu -> val RMSE=0.54101\n",
      "Epoch 001 | train RMSE 0.146642 | val RMSE 0.313016\n",
      "Epoch 002 | train RMSE 0.133517 | val RMSE 0.318087\n",
      "Epoch 003 | train RMSE 0.123613 | val RMSE 0.316673\n",
      "Epoch 004 | train RMSE 0.114259 | val RMSE 0.313375\n",
      "Epoch 005 | train RMSE 0.106725 | val RMSE 0.311526\n",
      "Epoch 006 | train RMSE 0.101766 | val RMSE 0.313196\n",
      "Epoch 007 | train RMSE 0.098427 | val RMSE 0.317530\n",
      "Epoch 008 | train RMSE 0.096126 | val RMSE 0.322863\n",
      "Epoch 009 | train RMSE 0.094793 | val RMSE 0.327170\n",
      "Epoch 010 | train RMSE 0.093823 | val RMSE 0.329152\n",
      "  h=4 lr=0.01 act=linear -> val RMSE=0.32915\n",
      "Epoch 001 | train RMSE 0.147348 | val RMSE 0.309771\n",
      "Epoch 002 | train RMSE 0.138428 | val RMSE 0.314475\n",
      "Epoch 003 | train RMSE 0.131199 | val RMSE 0.315932\n",
      "Epoch 004 | train RMSE 0.124327 | val RMSE 0.315220\n",
      "Epoch 005 | train RMSE 0.117635 | val RMSE 0.314541\n",
      "Epoch 006 | train RMSE 0.111660 | val RMSE 0.315721\n",
      "Epoch 007 | train RMSE 0.106339 | val RMSE 0.318937\n",
      "Epoch 008 | train RMSE 0.101612 | val RMSE 0.323536\n",
      "Epoch 009 | train RMSE 0.097917 | val RMSE 0.328349\n",
      "Epoch 010 | train RMSE 0.095528 | val RMSE 0.331644\n",
      "  h=4 lr=0.01 act=tanh -> val RMSE=0.33164\n",
      "Epoch 001 | train RMSE 0.203364 | val RMSE 0.273793\n",
      "Epoch 002 | train RMSE 0.175827 | val RMSE 0.277372\n",
      "Epoch 003 | train RMSE 0.152260 | val RMSE 0.282469\n",
      "Epoch 004 | train RMSE 0.134457 | val RMSE 0.288137\n",
      "Epoch 005 | train RMSE 0.122669 | val RMSE 0.294411\n",
      "Epoch 006 | train RMSE 0.114944 | val RMSE 0.300944\n",
      "Epoch 007 | train RMSE 0.110949 | val RMSE 0.307089\n",
      "Epoch 008 | train RMSE 0.109628 | val RMSE 0.312101\n",
      "Epoch 009 | train RMSE 0.109407 | val RMSE 0.315498\n",
      "Epoch 010 | train RMSE 0.108820 | val RMSE 0.317188\n",
      "  h=4 lr=0.01 act=relu -> val RMSE=0.31719\n",
      "Epoch 001 | train RMSE 0.129725 | val RMSE 0.312896\n",
      "Epoch 002 | train RMSE 0.126736 | val RMSE 0.313751\n",
      "Epoch 003 | train RMSE 0.123885 | val RMSE 0.314622\n",
      "Epoch 004 | train RMSE 0.121183 | val RMSE 0.315509\n",
      "Epoch 005 | train RMSE 0.118645 | val RMSE 0.316408\n",
      "Epoch 006 | train RMSE 0.116280 | val RMSE 0.317318\n",
      "Epoch 007 | train RMSE 0.114096 | val RMSE 0.318234\n",
      "Epoch 008 | train RMSE 0.112100 | val RMSE 0.319153\n",
      "Epoch 009 | train RMSE 0.110288 | val RMSE 0.320072\n",
      "Epoch 010 | train RMSE 0.108654 | val RMSE 0.320985\n",
      "  h=8 lr=0.001 act=linear -> val RMSE=0.32099\n",
      "Epoch 001 | train RMSE 0.159894 | val RMSE 0.312776\n",
      "Epoch 002 | train RMSE 0.156394 | val RMSE 0.311445\n",
      "Epoch 003 | train RMSE 0.153061 | val RMSE 0.310158\n",
      "Epoch 004 | train RMSE 0.149906 | val RMSE 0.308921\n",
      "Epoch 005 | train RMSE 0.146937 | val RMSE 0.307741\n",
      "Epoch 006 | train RMSE 0.144164 | val RMSE 0.306628\n",
      "Epoch 007 | train RMSE 0.141592 | val RMSE 0.305593\n",
      "Epoch 008 | train RMSE 0.139221 | val RMSE 0.304647\n",
      "Epoch 009 | train RMSE 0.137051 | val RMSE 0.303799\n",
      "Epoch 010 | train RMSE 0.135073 | val RMSE 0.303053\n",
      "  h=8 lr=0.001 act=tanh -> val RMSE=0.30305\n",
      "Epoch 001 | train RMSE 0.387454 | val RMSE 0.336956\n",
      "Epoch 002 | train RMSE 0.383829 | val RMSE 0.335746\n",
      "Epoch 003 | train RMSE 0.380131 | val RMSE 0.334553\n",
      "Epoch 004 | train RMSE 0.376379 | val RMSE 0.333372\n",
      "Epoch 005 | train RMSE 0.372630 | val RMSE 0.332201\n",
      "Epoch 006 | train RMSE 0.368878 | val RMSE 0.331038\n",
      "Epoch 007 | train RMSE 0.365119 | val RMSE 0.329885\n",
      "Epoch 008 | train RMSE 0.361347 | val RMSE 0.328740\n",
      "Epoch 009 | train RMSE 0.357565 | val RMSE 0.327604\n",
      "Epoch 010 | train RMSE 0.353756 | val RMSE 0.326472\n",
      "  h=8 lr=0.001 act=relu -> val RMSE=0.32647\n",
      "Epoch 001 | train RMSE 0.121030 | val RMSE 0.325812\n",
      "Epoch 002 | train RMSE 0.105024 | val RMSE 0.321572\n",
      "Epoch 003 | train RMSE 0.101639 | val RMSE 0.326639\n",
      "Epoch 004 | train RMSE 0.098222 | val RMSE 0.333918\n",
      "Epoch 005 | train RMSE 0.096518 | val RMSE 0.339321\n",
      "Epoch 006 | train RMSE 0.095442 | val RMSE 0.340985\n",
      "Epoch 007 | train RMSE 0.092314 | val RMSE 0.339855\n",
      "Epoch 008 | train RMSE 0.088037 | val RMSE 0.337885\n",
      "Epoch 009 | train RMSE 0.084926 | val RMSE 0.337013\n",
      "Epoch 010 | train RMSE 0.083276 | val RMSE 0.338267\n",
      "  h=8 lr=0.01 act=linear -> val RMSE=0.33827\n",
      "Epoch 001 | train RMSE 0.308799 | val RMSE 0.480175\n",
      "Epoch 002 | train RMSE 0.253508 | val RMSE 0.453003\n",
      "Epoch 003 | train RMSE 0.203480 | val RMSE 0.428193\n",
      "Epoch 004 | train RMSE 0.160054 | val RMSE 0.405831\n",
      "Epoch 005 | train RMSE 0.125469 | val RMSE 0.385948\n",
      "Epoch 006 | train RMSE 0.103539 | val RMSE 0.368731\n",
      "Epoch 007 | train RMSE 0.097296 | val RMSE 0.355018\n",
      "Epoch 008 | train RMSE 0.102176 | val RMSE 0.345459\n",
      "Epoch 009 | train RMSE 0.108783 | val RMSE 0.339712\n",
      "Epoch 010 | train RMSE 0.111723 | val RMSE 0.336986\n",
      "  h=8 lr=0.01 act=tanh -> val RMSE=0.33699\n",
      "Epoch 001 | train RMSE 0.309633 | val RMSE 0.281688\n",
      "Epoch 002 | train RMSE 0.282967 | val RMSE 0.277026\n",
      "Epoch 003 | train RMSE 0.255352 | val RMSE 0.273012\n",
      "Epoch 004 | train RMSE 0.228405 | val RMSE 0.269800\n",
      "Epoch 005 | train RMSE 0.203796 | val RMSE 0.267596\n",
      "Epoch 006 | train RMSE 0.181655 | val RMSE 0.266457\n",
      "Epoch 007 | train RMSE 0.164398 | val RMSE 0.266440\n",
      "Epoch 008 | train RMSE 0.150967 | val RMSE 0.267133\n",
      "Epoch 009 | train RMSE 0.142324 | val RMSE 0.268148\n",
      "Epoch 010 | train RMSE 0.137927 | val RMSE 0.269257\n",
      "  h=8 lr=0.01 act=relu -> val RMSE=0.26926\n",
      "Epoch 001 | train RMSE 0.287044 | val RMSE 0.293392\n",
      "Epoch 002 | train RMSE 0.278920 | val RMSE 0.292883\n",
      "Epoch 003 | train RMSE 0.270847 | val RMSE 0.292449\n",
      "Epoch 004 | train RMSE 0.262830 | val RMSE 0.292087\n",
      "Epoch 005 | train RMSE 0.254877 | val RMSE 0.291797\n",
      "Epoch 006 | train RMSE 0.246997 | val RMSE 0.291575\n",
      "Epoch 007 | train RMSE 0.239197 | val RMSE 0.291419\n",
      "Epoch 008 | train RMSE 0.231487 | val RMSE 0.291328\n",
      "Epoch 009 | train RMSE 0.223877 | val RMSE 0.291306\n",
      "Epoch 010 | train RMSE 0.216379 | val RMSE 0.291355\n",
      "  h=16 lr=0.001 act=linear -> val RMSE=0.29135\n",
      "Epoch 001 | train RMSE 0.156499 | val RMSE 0.268636\n",
      "Epoch 002 | train RMSE 0.152488 | val RMSE 0.270115\n",
      "Epoch 003 | train RMSE 0.148678 | val RMSE 0.271640\n",
      "Epoch 004 | train RMSE 0.145081 | val RMSE 0.273187\n",
      "Epoch 005 | train RMSE 0.141699 | val RMSE 0.274723\n",
      "Epoch 006 | train RMSE 0.138530 | val RMSE 0.276217\n",
      "Epoch 007 | train RMSE 0.135566 | val RMSE 0.277658\n",
      "Epoch 008 | train RMSE 0.132791 | val RMSE 0.279044\n",
      "Epoch 009 | train RMSE 0.130186 | val RMSE 0.280382\n",
      "Epoch 010 | train RMSE 0.127737 | val RMSE 0.281675\n",
      "  h=16 lr=0.001 act=tanh -> val RMSE=0.28168\n",
      "Epoch 001 | train RMSE 0.146515 | val RMSE 0.299224\n",
      "Epoch 002 | train RMSE 0.145268 | val RMSE 0.299690\n",
      "Epoch 003 | train RMSE 0.144078 | val RMSE 0.300165\n",
      "Epoch 004 | train RMSE 0.142940 | val RMSE 0.300653\n",
      "Epoch 005 | train RMSE 0.141829 | val RMSE 0.301155\n",
      "Epoch 006 | train RMSE 0.140745 | val RMSE 0.301660\n",
      "Epoch 007 | train RMSE 0.139682 | val RMSE 0.302159\n",
      "Epoch 008 | train RMSE 0.138645 | val RMSE 0.302648\n",
      "Epoch 009 | train RMSE 0.137632 | val RMSE 0.303116\n",
      "Epoch 010 | train RMSE 0.136635 | val RMSE 0.303555\n",
      "  h=16 lr=0.001 act=relu -> val RMSE=0.30355\n",
      "Epoch 001 | train RMSE 0.181451 | val RMSE 0.349914\n",
      "Epoch 002 | train RMSE 0.122984 | val RMSE 0.330630\n",
      "Epoch 003 | train RMSE 0.113853 | val RMSE 0.327559\n",
      "Epoch 004 | train RMSE 0.118752 | val RMSE 0.333651\n",
      "Epoch 005 | train RMSE 0.111897 | val RMSE 0.343842\n",
      "Epoch 006 | train RMSE 0.100674 | val RMSE 0.354255\n",
      "Epoch 007 | train RMSE 0.094266 | val RMSE 0.361933\n",
      "Epoch 008 | train RMSE 0.094731 | val RMSE 0.364800\n",
      "Epoch 009 | train RMSE 0.095853 | val RMSE 0.362841\n",
      "Epoch 010 | train RMSE 0.093320 | val RMSE 0.357675\n",
      "  h=16 lr=0.01 act=linear -> val RMSE=0.35767\n",
      "Epoch 001 | train RMSE 0.163193 | val RMSE 0.291229\n",
      "Epoch 002 | train RMSE 0.124775 | val RMSE 0.314999\n",
      "Epoch 003 | train RMSE 0.115848 | val RMSE 0.327209\n",
      "Epoch 004 | train RMSE 0.112359 | val RMSE 0.328770\n",
      "Epoch 005 | train RMSE 0.103221 | val RMSE 0.325314\n",
      "Epoch 006 | train RMSE 0.095686 | val RMSE 0.321650\n",
      "Epoch 007 | train RMSE 0.094848 | val RMSE 0.321344\n",
      "Epoch 008 | train RMSE 0.095722 | val RMSE 0.325184\n",
      "Epoch 009 | train RMSE 0.092699 | val RMSE 0.332377\n",
      "Epoch 010 | train RMSE 0.086764 | val RMSE 0.341485\n",
      "  h=16 lr=0.01 act=tanh -> val RMSE=0.34148\n",
      "Epoch 001 | train RMSE 0.144408 | val RMSE 0.296803\n",
      "Epoch 002 | train RMSE 0.125734 | val RMSE 0.305312\n",
      "Epoch 003 | train RMSE 0.117188 | val RMSE 0.309017\n",
      "Epoch 004 | train RMSE 0.111505 | val RMSE 0.308076\n",
      "Epoch 005 | train RMSE 0.106112 | val RMSE 0.304685\n",
      "Epoch 006 | train RMSE 0.101740 | val RMSE 0.300609\n",
      "Epoch 007 | train RMSE 0.098162 | val RMSE 0.297665\n",
      "Epoch 008 | train RMSE 0.095270 | val RMSE 0.296722\n",
      "Epoch 009 | train RMSE 0.092032 | val RMSE 0.297894\n",
      "Epoch 010 | train RMSE 0.087644 | val RMSE 0.300686\n",
      "  h=16 lr=0.01 act=relu -> val RMSE=0.30069\n",
      "Tuning market=FIAT+GOLD\n",
      "Epoch 001 | train RMSE 0.283219 | val RMSE 0.504911\n",
      "Epoch 002 | train RMSE 0.280408 | val RMSE 0.503255\n",
      "Epoch 003 | train RMSE 0.277614 | val RMSE 0.501605\n",
      "Epoch 004 | train RMSE 0.274837 | val RMSE 0.499960\n",
      "Epoch 005 | train RMSE 0.272078 | val RMSE 0.498320\n",
      "Epoch 006 | train RMSE 0.269337 | val RMSE 0.496684\n",
      "Epoch 007 | train RMSE 0.266613 | val RMSE 0.495050\n",
      "Epoch 008 | train RMSE 0.263907 | val RMSE 0.493418\n",
      "Epoch 009 | train RMSE 0.261216 | val RMSE 0.491786\n",
      "Epoch 010 | train RMSE 0.258541 | val RMSE 0.490153\n",
      "  h=4 lr=0.001 act=linear -> val RMSE=0.49015\n",
      "Epoch 001 | train RMSE 0.374065 | val RMSE 0.363723\n",
      "Epoch 002 | train RMSE 0.369593 | val RMSE 0.362571\n",
      "Epoch 003 | train RMSE 0.365108 | val RMSE 0.361432\n",
      "Epoch 004 | train RMSE 0.360610 | val RMSE 0.360306\n",
      "Epoch 005 | train RMSE 0.356099 | val RMSE 0.359193\n",
      "Epoch 006 | train RMSE 0.351576 | val RMSE 0.358094\n",
      "Epoch 007 | train RMSE 0.347040 | val RMSE 0.357009\n",
      "Epoch 008 | train RMSE 0.342493 | val RMSE 0.355937\n",
      "Epoch 009 | train RMSE 0.337935 | val RMSE 0.354880\n",
      "Epoch 010 | train RMSE 0.333365 | val RMSE 0.353837\n",
      "  h=4 lr=0.001 act=tanh -> val RMSE=0.35384\n",
      "Epoch 001 | train RMSE 0.252910 | val RMSE 0.277809\n",
      "Epoch 002 | train RMSE 0.250814 | val RMSE 0.277368\n",
      "Epoch 003 | train RMSE 0.248736 | val RMSE 0.276940\n",
      "Epoch 004 | train RMSE 0.246674 | val RMSE 0.276527\n",
      "Epoch 005 | train RMSE 0.244630 | val RMSE 0.276127\n",
      "Epoch 006 | train RMSE 0.242600 | val RMSE 0.275741\n",
      "Epoch 007 | train RMSE 0.240584 | val RMSE 0.275368\n",
      "Epoch 008 | train RMSE 0.238581 | val RMSE 0.275012\n",
      "Epoch 009 | train RMSE 0.236588 | val RMSE 0.274668\n",
      "Epoch 010 | train RMSE 0.234608 | val RMSE 0.274340\n",
      "  h=4 lr=0.001 act=relu -> val RMSE=0.27434\n",
      "Epoch 001 | train RMSE 0.143509 | val RMSE 0.285682\n",
      "Epoch 002 | train RMSE 0.122299 | val RMSE 0.295221\n",
      "Epoch 003 | train RMSE 0.111373 | val RMSE 0.303943\n",
      "Epoch 004 | train RMSE 0.108913 | val RMSE 0.310404\n",
      "Epoch 005 | train RMSE 0.109222 | val RMSE 0.313648\n",
      "Epoch 006 | train RMSE 0.108266 | val RMSE 0.314082\n",
      "Epoch 007 | train RMSE 0.105407 | val RMSE 0.312708\n",
      "Epoch 008 | train RMSE 0.101864 | val RMSE 0.310616\n",
      "Epoch 009 | train RMSE 0.099067 | val RMSE 0.308737\n",
      "Epoch 010 | train RMSE 0.097767 | val RMSE 0.307649\n",
      "  h=4 lr=0.01 act=linear -> val RMSE=0.30765\n",
      "Epoch 001 | train RMSE 0.214510 | val RMSE 0.275261\n",
      "Epoch 002 | train RMSE 0.181255 | val RMSE 0.276753\n",
      "Epoch 003 | train RMSE 0.155077 | val RMSE 0.279272\n",
      "Epoch 004 | train RMSE 0.139547 | val RMSE 0.282788\n",
      "Epoch 005 | train RMSE 0.133614 | val RMSE 0.286318\n",
      "Epoch 006 | train RMSE 0.131776 | val RMSE 0.288703\n",
      "Epoch 007 | train RMSE 0.128840 | val RMSE 0.289557\n",
      "Epoch 008 | train RMSE 0.123277 | val RMSE 0.289193\n",
      "Epoch 009 | train RMSE 0.116361 | val RMSE 0.288258\n",
      "Epoch 010 | train RMSE 0.110333 | val RMSE 0.287385\n",
      "  h=4 lr=0.01 act=tanh -> val RMSE=0.28739\n",
      "Epoch 001 | train RMSE 0.330613 | val RMSE 0.279678\n",
      "Epoch 002 | train RMSE 0.305852 | val RMSE 0.273580\n",
      "Epoch 003 | train RMSE 0.279212 | val RMSE 0.268313\n",
      "Epoch 004 | train RMSE 0.252256 | val RMSE 0.264199\n",
      "Epoch 005 | train RMSE 0.226031 | val RMSE 0.261595\n",
      "Epoch 006 | train RMSE 0.200460 | val RMSE 0.260758\n",
      "Epoch 007 | train RMSE 0.177472 | val RMSE 0.261838\n",
      "Epoch 008 | train RMSE 0.159853 | val RMSE 0.264378\n",
      "Epoch 009 | train RMSE 0.148568 | val RMSE 0.267716\n",
      "Epoch 010 | train RMSE 0.142436 | val RMSE 0.271280\n",
      "  h=4 lr=0.01 act=relu -> val RMSE=0.27128\n",
      "Epoch 001 | train RMSE 0.112379 | val RMSE 0.272466\n",
      "Epoch 002 | train RMSE 0.111074 | val RMSE 0.273397\n",
      "Epoch 003 | train RMSE 0.109920 | val RMSE 0.274373\n",
      "Epoch 004 | train RMSE 0.108891 | val RMSE 0.275377\n",
      "Epoch 005 | train RMSE 0.107978 | val RMSE 0.276403\n",
      "Epoch 006 | train RMSE 0.107172 | val RMSE 0.277441\n",
      "Epoch 007 | train RMSE 0.106456 | val RMSE 0.278482\n",
      "Epoch 008 | train RMSE 0.105814 | val RMSE 0.279513\n",
      "Epoch 009 | train RMSE 0.105226 | val RMSE 0.280520\n",
      "Epoch 010 | train RMSE 0.104673 | val RMSE 0.281490\n",
      "  h=8 lr=0.001 act=linear -> val RMSE=0.28149\n",
      "Epoch 001 | train RMSE 0.329407 | val RMSE 0.474506\n",
      "Epoch 002 | train RMSE 0.323233 | val RMSE 0.471952\n",
      "Epoch 003 | train RMSE 0.317107 | val RMSE 0.469417\n",
      "Epoch 004 | train RMSE 0.311030 | val RMSE 0.466900\n",
      "Epoch 005 | train RMSE 0.305004 | val RMSE 0.464402\n",
      "Epoch 006 | train RMSE 0.299028 | val RMSE 0.461921\n",
      "Epoch 007 | train RMSE 0.293104 | val RMSE 0.459458\n",
      "Epoch 008 | train RMSE 0.287228 | val RMSE 0.457011\n",
      "Epoch 009 | train RMSE 0.281401 | val RMSE 0.454577\n",
      "Epoch 010 | train RMSE 0.275619 | val RMSE 0.452156\n",
      "  h=8 lr=0.001 act=tanh -> val RMSE=0.45216\n",
      "Epoch 001 | train RMSE 0.366767 | val RMSE 0.320745\n",
      "Epoch 002 | train RMSE 0.364462 | val RMSE 0.319914\n",
      "Epoch 003 | train RMSE 0.362016 | val RMSE 0.319107\n",
      "Epoch 004 | train RMSE 0.359540 | val RMSE 0.318314\n",
      "Epoch 005 | train RMSE 0.357041 | val RMSE 0.317537\n",
      "Epoch 006 | train RMSE 0.354430 | val RMSE 0.316772\n",
      "Epoch 007 | train RMSE 0.351775 | val RMSE 0.316016\n",
      "Epoch 008 | train RMSE 0.349030 | val RMSE 0.315270\n",
      "Epoch 009 | train RMSE 0.346259 | val RMSE 0.314536\n",
      "Epoch 010 | train RMSE 0.343476 | val RMSE 0.313823\n",
      "  h=8 lr=0.001 act=relu -> val RMSE=0.31382\n",
      "Epoch 001 | train RMSE 0.266482 | val RMSE 0.423948\n",
      "Epoch 002 | train RMSE 0.216110 | val RMSE 0.401086\n",
      "Epoch 003 | train RMSE 0.170984 | val RMSE 0.379790\n",
      "Epoch 004 | train RMSE 0.132819 | val RMSE 0.360292\n",
      "Epoch 005 | train RMSE 0.105651 | val RMSE 0.343422\n",
      "Epoch 006 | train RMSE 0.095923 | val RMSE 0.330614\n",
      "Epoch 007 | train RMSE 0.102935 | val RMSE 0.323004\n",
      "Epoch 008 | train RMSE 0.113560 | val RMSE 0.320023\n",
      "Epoch 009 | train RMSE 0.118564 | val RMSE 0.320394\n",
      "Epoch 010 | train RMSE 0.116749 | val RMSE 0.323049\n",
      "  h=8 lr=0.01 act=linear -> val RMSE=0.32305\n",
      "Epoch 001 | train RMSE 0.255549 | val RMSE 0.255039\n",
      "Epoch 002 | train RMSE 0.207674 | val RMSE 0.257780\n",
      "Epoch 003 | train RMSE 0.166464 | val RMSE 0.263352\n",
      "Epoch 004 | train RMSE 0.134640 | val RMSE 0.271373\n",
      "Epoch 005 | train RMSE 0.116626 | val RMSE 0.280836\n",
      "Epoch 006 | train RMSE 0.114489 | val RMSE 0.289807\n",
      "Epoch 007 | train RMSE 0.121134 | val RMSE 0.296427\n",
      "Epoch 008 | train RMSE 0.126937 | val RMSE 0.300040\n",
      "Epoch 009 | train RMSE 0.127596 | val RMSE 0.300911\n",
      "Epoch 010 | train RMSE 0.123144 | val RMSE 0.299663\n",
      "  h=8 lr=0.01 act=tanh -> val RMSE=0.29966\n",
      "Epoch 001 | train RMSE 0.139704 | val RMSE 0.297106\n",
      "Epoch 002 | train RMSE 0.132304 | val RMSE 0.294197\n",
      "Epoch 003 | train RMSE 0.127445 | val RMSE 0.291649\n",
      "Epoch 004 | train RMSE 0.125278 | val RMSE 0.290058\n",
      "Epoch 005 | train RMSE 0.124264 | val RMSE 0.289635\n",
      "Epoch 006 | train RMSE 0.123222 | val RMSE 0.290128\n",
      "Epoch 007 | train RMSE 0.121905 | val RMSE 0.291387\n",
      "Epoch 008 | train RMSE 0.121022 | val RMSE 0.292265\n",
      "Epoch 009 | train RMSE 0.119444 | val RMSE 0.293311\n",
      "Epoch 010 | train RMSE 0.118012 | val RMSE 0.294297\n",
      "  h=8 lr=0.01 act=relu -> val RMSE=0.29430\n",
      "Epoch 001 | train RMSE 0.249709 | val RMSE 0.253584\n",
      "Epoch 002 | train RMSE 0.242433 | val RMSE 0.254272\n",
      "Epoch 003 | train RMSE 0.235285 | val RMSE 0.255030\n",
      "Epoch 004 | train RMSE 0.228279 | val RMSE 0.255857\n",
      "Epoch 005 | train RMSE 0.221426 | val RMSE 0.256750\n",
      "Epoch 006 | train RMSE 0.214740 | val RMSE 0.257705\n",
      "Epoch 007 | train RMSE 0.208234 | val RMSE 0.258720\n",
      "Epoch 008 | train RMSE 0.201922 | val RMSE 0.259789\n",
      "Epoch 009 | train RMSE 0.195819 | val RMSE 0.260909\n",
      "Epoch 010 | train RMSE 0.189940 | val RMSE 0.262071\n",
      "  h=16 lr=0.001 act=linear -> val RMSE=0.26207\n",
      "Epoch 001 | train RMSE 0.210414 | val RMSE 0.435458\n",
      "Epoch 002 | train RMSE 0.203633 | val RMSE 0.432578\n",
      "Epoch 003 | train RMSE 0.196949 | val RMSE 0.429679\n",
      "Epoch 004 | train RMSE 0.190371 | val RMSE 0.426775\n",
      "Epoch 005 | train RMSE 0.183913 | val RMSE 0.423874\n",
      "Epoch 006 | train RMSE 0.177591 | val RMSE 0.420981\n",
      "Epoch 007 | train RMSE 0.171422 | val RMSE 0.418102\n",
      "Epoch 008 | train RMSE 0.165426 | val RMSE 0.415241\n",
      "Epoch 009 | train RMSE 0.159624 | val RMSE 0.412403\n",
      "Epoch 010 | train RMSE 0.154039 | val RMSE 0.409592\n",
      "  h=16 lr=0.001 act=tanh -> val RMSE=0.40959\n",
      "Epoch 001 | train RMSE 0.183401 | val RMSE 0.278490\n",
      "Epoch 002 | train RMSE 0.181007 | val RMSE 0.279097\n",
      "Epoch 003 | train RMSE 0.178636 | val RMSE 0.279718\n",
      "Epoch 004 | train RMSE 0.176295 | val RMSE 0.280350\n",
      "Epoch 005 | train RMSE 0.173986 | val RMSE 0.280995\n",
      "Epoch 006 | train RMSE 0.171684 | val RMSE 0.281652\n",
      "Epoch 007 | train RMSE 0.169386 | val RMSE 0.282320\n",
      "Epoch 008 | train RMSE 0.167121 | val RMSE 0.282997\n",
      "Epoch 009 | train RMSE 0.164892 | val RMSE 0.283688\n",
      "Epoch 010 | train RMSE 0.162697 | val RMSE 0.284385\n",
      "  h=16 lr=0.001 act=relu -> val RMSE=0.28438\n",
      "Epoch 001 | train RMSE 0.134549 | val RMSE 0.319663\n",
      "Epoch 002 | train RMSE 0.112798 | val RMSE 0.332689\n",
      "Epoch 003 | train RMSE 0.103519 | val RMSE 0.334436\n",
      "Epoch 004 | train RMSE 0.096836 | val RMSE 0.333537\n",
      "Epoch 005 | train RMSE 0.095127 | val RMSE 0.334659\n",
      "Epoch 006 | train RMSE 0.095986 | val RMSE 0.338889\n",
      "Epoch 007 | train RMSE 0.093783 | val RMSE 0.344733\n",
      "Epoch 008 | train RMSE 0.089716 | val RMSE 0.350148\n",
      "Epoch 009 | train RMSE 0.086702 | val RMSE 0.353167\n",
      "Epoch 010 | train RMSE 0.084702 | val RMSE 0.353406\n",
      "  h=16 lr=0.01 act=linear -> val RMSE=0.35341\n",
      "Epoch 001 | train RMSE 0.214773 | val RMSE 0.285590\n",
      "Epoch 002 | train RMSE 0.152704 | val RMSE 0.299280\n",
      "Epoch 003 | train RMSE 0.116286 | val RMSE 0.315556\n",
      "Epoch 004 | train RMSE 0.116337 | val RMSE 0.325276\n",
      "Epoch 005 | train RMSE 0.123775 | val RMSE 0.326751\n",
      "Epoch 006 | train RMSE 0.120220 | val RMSE 0.323005\n",
      "Epoch 007 | train RMSE 0.110239 | val RMSE 0.317086\n",
      "Epoch 008 | train RMSE 0.101933 | val RMSE 0.311506\n",
      "Epoch 009 | train RMSE 0.099971 | val RMSE 0.307917\n",
      "Epoch 010 | train RMSE 0.102094 | val RMSE 0.306892\n",
      "  h=16 lr=0.01 act=tanh -> val RMSE=0.30689\n",
      "Epoch 001 | train RMSE 0.371289 | val RMSE 0.325239\n",
      "Epoch 002 | train RMSE 0.338199 | val RMSE 0.315928\n",
      "Epoch 003 | train RMSE 0.308515 | val RMSE 0.308018\n",
      "Epoch 004 | train RMSE 0.279141 | val RMSE 0.301206\n",
      "Epoch 005 | train RMSE 0.249571 | val RMSE 0.295735\n",
      "Epoch 006 | train RMSE 0.220523 | val RMSE 0.291753\n",
      "Epoch 007 | train RMSE 0.192709 | val RMSE 0.289393\n",
      "Epoch 008 | train RMSE 0.167855 | val RMSE 0.288654\n",
      "Epoch 009 | train RMSE 0.149559 | val RMSE 0.289454\n",
      "Epoch 010 | train RMSE 0.141253 | val RMSE 0.291396\n",
      "  h=16 lr=0.01 act=relu -> val RMSE=0.29140\n",
      "Tuning market=FIAT+EQUITY+GOLD+ENERGY\n",
      "Epoch 001 | train RMSE 0.216657 | val RMSE 0.432791\n",
      "Epoch 002 | train RMSE 0.213363 | val RMSE 0.431036\n",
      "Epoch 003 | train RMSE 0.210101 | val RMSE 0.429291\n",
      "Epoch 004 | train RMSE 0.206869 | val RMSE 0.427563\n",
      "Epoch 005 | train RMSE 0.203670 | val RMSE 0.425855\n",
      "Epoch 006 | train RMSE 0.200502 | val RMSE 0.424176\n",
      "Epoch 007 | train RMSE 0.197364 | val RMSE 0.422526\n",
      "Epoch 008 | train RMSE 0.194252 | val RMSE 0.420904\n",
      "Epoch 009 | train RMSE 0.191166 | val RMSE 0.419306\n",
      "Epoch 010 | train RMSE 0.188104 | val RMSE 0.417727\n",
      "  h=4 lr=0.001 act=linear -> val RMSE=0.41773\n",
      "Epoch 001 | train RMSE 0.170243 | val RMSE 0.263245\n",
      "Epoch 002 | train RMSE 0.167061 | val RMSE 0.263741\n",
      "Epoch 003 | train RMSE 0.163918 | val RMSE 0.264251\n",
      "Epoch 004 | train RMSE 0.160815 | val RMSE 0.264772\n",
      "Epoch 005 | train RMSE 0.157754 | val RMSE 0.265301\n",
      "Epoch 006 | train RMSE 0.154732 | val RMSE 0.265837\n",
      "Epoch 007 | train RMSE 0.151749 | val RMSE 0.266379\n",
      "Epoch 008 | train RMSE 0.148806 | val RMSE 0.266928\n",
      "Epoch 009 | train RMSE 0.145902 | val RMSE 0.267484\n",
      "Epoch 010 | train RMSE 0.143039 | val RMSE 0.268045\n",
      "  h=4 lr=0.001 act=tanh -> val RMSE=0.26805\n",
      "Epoch 001 | train RMSE 0.295683 | val RMSE 0.512829\n",
      "Epoch 002 | train RMSE 0.291592 | val RMSE 0.511025\n",
      "Epoch 003 | train RMSE 0.287536 | val RMSE 0.509228\n",
      "Epoch 004 | train RMSE 0.283489 | val RMSE 0.507433\n",
      "Epoch 005 | train RMSE 0.279452 | val RMSE 0.505633\n",
      "Epoch 006 | train RMSE 0.275426 | val RMSE 0.503826\n",
      "Epoch 007 | train RMSE 0.271411 | val RMSE 0.502016\n",
      "Epoch 008 | train RMSE 0.267407 | val RMSE 0.500210\n",
      "Epoch 009 | train RMSE 0.263415 | val RMSE 0.498408\n",
      "Epoch 010 | train RMSE 0.259435 | val RMSE 0.496610\n",
      "  h=4 lr=0.001 act=relu -> val RMSE=0.49661\n",
      "Epoch 001 | train RMSE 0.441123 | val RMSE 0.348068\n",
      "Epoch 002 | train RMSE 0.413677 | val RMSE 0.337756\n",
      "Epoch 003 | train RMSE 0.387270 | val RMSE 0.328191\n",
      "Epoch 004 | train RMSE 0.361078 | val RMSE 0.319304\n",
      "Epoch 005 | train RMSE 0.334400 | val RMSE 0.311064\n",
      "Epoch 006 | train RMSE 0.306593 | val RMSE 0.303568\n",
      "Epoch 007 | train RMSE 0.277503 | val RMSE 0.296993\n",
      "Epoch 008 | train RMSE 0.247377 | val RMSE 0.291553\n",
      "Epoch 009 | train RMSE 0.216778 | val RMSE 0.287471\n",
      "Epoch 010 | train RMSE 0.186749 | val RMSE 0.284968\n",
      "  h=4 lr=0.01 act=linear -> val RMSE=0.28497\n",
      "Epoch 001 | train RMSE 0.291687 | val RMSE 0.461786\n",
      "Epoch 002 | train RMSE 0.251233 | val RMSE 0.443146\n",
      "Epoch 003 | train RMSE 0.212345 | val RMSE 0.424724\n",
      "Epoch 004 | train RMSE 0.175523 | val RMSE 0.406686\n",
      "Epoch 005 | train RMSE 0.141972 | val RMSE 0.389345\n",
      "Epoch 006 | train RMSE 0.115076 | val RMSE 0.372966\n",
      "Epoch 007 | train RMSE 0.100858 | val RMSE 0.358728\n",
      "Epoch 008 | train RMSE 0.102234 | val RMSE 0.348730\n",
      "Epoch 009 | train RMSE 0.110964 | val RMSE 0.343633\n",
      "Epoch 010 | train RMSE 0.117303 | val RMSE 0.342697\n",
      "  h=4 lr=0.01 act=tanh -> val RMSE=0.34270\n",
      "Epoch 001 | train RMSE 0.242482 | val RMSE 0.268610\n",
      "Epoch 002 | train RMSE 0.201223 | val RMSE 0.268394\n",
      "Epoch 003 | train RMSE 0.169145 | val RMSE 0.269798\n",
      "Epoch 004 | train RMSE 0.148446 | val RMSE 0.271637\n",
      "Epoch 005 | train RMSE 0.135395 | val RMSE 0.274170\n",
      "Epoch 006 | train RMSE 0.126554 | val RMSE 0.277396\n",
      "Epoch 007 | train RMSE 0.120093 | val RMSE 0.280819\n",
      "Epoch 008 | train RMSE 0.115080 | val RMSE 0.284412\n",
      "Epoch 009 | train RMSE 0.111164 | val RMSE 0.287737\n",
      "Epoch 010 | train RMSE 0.107849 | val RMSE 0.290538\n",
      "  h=4 lr=0.01 act=relu -> val RMSE=0.29054\n",
      "Epoch 001 | train RMSE 0.308583 | val RMSE 0.482287\n",
      "Epoch 002 | train RMSE 0.303299 | val RMSE 0.479748\n",
      "Epoch 003 | train RMSE 0.298056 | val RMSE 0.477224\n",
      "Epoch 004 | train RMSE 0.292853 | val RMSE 0.474716\n",
      "Epoch 005 | train RMSE 0.287693 | val RMSE 0.472225\n",
      "Epoch 006 | train RMSE 0.282576 | val RMSE 0.469753\n",
      "Epoch 007 | train RMSE 0.277502 | val RMSE 0.467302\n",
      "Epoch 008 | train RMSE 0.272474 | val RMSE 0.464874\n",
      "Epoch 009 | train RMSE 0.267490 | val RMSE 0.462474\n",
      "Epoch 010 | train RMSE 0.262551 | val RMSE 0.460103\n",
      "  h=8 lr=0.001 act=linear -> val RMSE=0.46010\n",
      "Epoch 001 | train RMSE 0.138468 | val RMSE 0.283471\n",
      "Epoch 002 | train RMSE 0.135499 | val RMSE 0.283794\n",
      "Epoch 003 | train RMSE 0.132608 | val RMSE 0.283792\n",
      "Epoch 004 | train RMSE 0.129782 | val RMSE 0.283436\n",
      "Epoch 005 | train RMSE 0.127007 | val RMSE 0.282886\n",
      "Epoch 006 | train RMSE 0.124292 | val RMSE 0.282313\n",
      "Epoch 007 | train RMSE 0.121648 | val RMSE 0.281855\n",
      "Epoch 008 | train RMSE 0.119075 | val RMSE 0.281567\n",
      "Epoch 009 | train RMSE 0.116570 | val RMSE 0.281428\n",
      "Epoch 010 | train RMSE 0.114135 | val RMSE 0.281392\n",
      "  h=8 lr=0.001 act=tanh -> val RMSE=0.28139\n",
      "Epoch 001 | train RMSE 0.127502 | val RMSE 0.316415\n",
      "Epoch 002 | train RMSE 0.126061 | val RMSE 0.315512\n",
      "Epoch 003 | train RMSE 0.124797 | val RMSE 0.314620\n",
      "Epoch 004 | train RMSE 0.123590 | val RMSE 0.313742\n",
      "Epoch 005 | train RMSE 0.122418 | val RMSE 0.312875\n",
      "Epoch 006 | train RMSE 0.121296 | val RMSE 0.312025\n",
      "Epoch 007 | train RMSE 0.120199 | val RMSE 0.311193\n",
      "Epoch 008 | train RMSE 0.119109 | val RMSE 0.310383\n",
      "Epoch 009 | train RMSE 0.118057 | val RMSE 0.309602\n",
      "Epoch 010 | train RMSE 0.117142 | val RMSE 0.308859\n",
      "  h=8 lr=0.001 act=relu -> val RMSE=0.30886\n",
      "Epoch 001 | train RMSE 0.167128 | val RMSE 0.372955\n",
      "Epoch 002 | train RMSE 0.127940 | val RMSE 0.355678\n",
      "Epoch 003 | train RMSE 0.105525 | val RMSE 0.342999\n",
      "Epoch 004 | train RMSE 0.101816 | val RMSE 0.337782\n",
      "Epoch 005 | train RMSE 0.103527 | val RMSE 0.338218\n",
      "Epoch 006 | train RMSE 0.100887 | val RMSE 0.341576\n",
      "Epoch 007 | train RMSE 0.094517 | val RMSE 0.346067\n",
      "Epoch 008 | train RMSE 0.088104 | val RMSE 0.350632\n",
      "Epoch 009 | train RMSE 0.085224 | val RMSE 0.353934\n",
      "Epoch 010 | train RMSE 0.085847 | val RMSE 0.354791\n",
      "  h=8 lr=0.01 act=linear -> val RMSE=0.35479\n",
      "Epoch 001 | train RMSE 0.158431 | val RMSE 0.263721\n",
      "Epoch 002 | train RMSE 0.125505 | val RMSE 0.271814\n",
      "Epoch 003 | train RMSE 0.111164 | val RMSE 0.279153\n",
      "Epoch 004 | train RMSE 0.109304 | val RMSE 0.282256\n",
      "Epoch 005 | train RMSE 0.106124 | val RMSE 0.281379\n",
      "Epoch 006 | train RMSE 0.099317 | val RMSE 0.278546\n",
      "Epoch 007 | train RMSE 0.093481 | val RMSE 0.275711\n",
      "Epoch 008 | train RMSE 0.091846 | val RMSE 0.274298\n",
      "Epoch 009 | train RMSE 0.092620 | val RMSE 0.274864\n",
      "Epoch 010 | train RMSE 0.092141 | val RMSE 0.277276\n",
      "  h=8 lr=0.01 act=tanh -> val RMSE=0.27728\n",
      "Epoch 001 | train RMSE 0.166370 | val RMSE 0.274119\n",
      "Epoch 002 | train RMSE 0.151305 | val RMSE 0.277478\n",
      "Epoch 003 | train RMSE 0.138495 | val RMSE 0.281616\n",
      "Epoch 004 | train RMSE 0.127507 | val RMSE 0.287696\n",
      "Epoch 005 | train RMSE 0.117938 | val RMSE 0.294091\n",
      "Epoch 006 | train RMSE 0.109098 | val RMSE 0.299639\n",
      "Epoch 007 | train RMSE 0.099684 | val RMSE 0.304587\n",
      "Epoch 008 | train RMSE 0.092044 | val RMSE 0.308600\n",
      "Epoch 009 | train RMSE 0.085458 | val RMSE 0.311388\n",
      "Epoch 010 | train RMSE 0.080448 | val RMSE 0.313014\n",
      "  h=8 lr=0.01 act=relu -> val RMSE=0.31301\n",
      "Epoch 001 | train RMSE 0.230732 | val RMSE 0.422633\n",
      "Epoch 002 | train RMSE 0.222095 | val RMSE 0.419116\n",
      "Epoch 003 | train RMSE 0.213592 | val RMSE 0.415641\n",
      "Epoch 004 | train RMSE 0.205234 | val RMSE 0.412209\n",
      "Epoch 005 | train RMSE 0.197033 | val RMSE 0.408820\n",
      "Epoch 006 | train RMSE 0.188999 | val RMSE 0.405472\n",
      "Epoch 007 | train RMSE 0.181145 | val RMSE 0.402165\n",
      "Epoch 008 | train RMSE 0.173486 | val RMSE 0.398901\n",
      "Epoch 009 | train RMSE 0.166036 | val RMSE 0.395682\n",
      "Epoch 010 | train RMSE 0.158814 | val RMSE 0.392512\n",
      "  h=16 lr=0.001 act=linear -> val RMSE=0.39251\n",
      "Epoch 001 | train RMSE 0.191497 | val RMSE 0.385044\n",
      "Epoch 002 | train RMSE 0.181955 | val RMSE 0.381299\n",
      "Epoch 003 | train RMSE 0.172588 | val RMSE 0.377579\n",
      "Epoch 004 | train RMSE 0.163427 | val RMSE 0.373885\n",
      "Epoch 005 | train RMSE 0.154499 | val RMSE 0.370219\n",
      "Epoch 006 | train RMSE 0.145833 | val RMSE 0.366589\n",
      "Epoch 007 | train RMSE 0.137467 | val RMSE 0.363004\n",
      "Epoch 008 | train RMSE 0.129452 | val RMSE 0.359475\n",
      "Epoch 009 | train RMSE 0.121856 | val RMSE 0.356010\n",
      "Epoch 010 | train RMSE 0.114755 | val RMSE 0.352621\n",
      "  h=16 lr=0.001 act=tanh -> val RMSE=0.35262\n",
      "Epoch 001 | train RMSE 0.174764 | val RMSE 0.273014\n",
      "Epoch 002 | train RMSE 0.169756 | val RMSE 0.273128\n",
      "Epoch 003 | train RMSE 0.164828 | val RMSE 0.273282\n",
      "Epoch 004 | train RMSE 0.159995 | val RMSE 0.273466\n",
      "Epoch 005 | train RMSE 0.155249 | val RMSE 0.273645\n",
      "Epoch 006 | train RMSE 0.150620 | val RMSE 0.273834\n",
      "Epoch 007 | train RMSE 0.146162 | val RMSE 0.274036\n",
      "Epoch 008 | train RMSE 0.141907 | val RMSE 0.274254\n",
      "Epoch 009 | train RMSE 0.137863 | val RMSE 0.274495\n",
      "Epoch 010 | train RMSE 0.133999 | val RMSE 0.274747\n",
      "  h=16 lr=0.001 act=relu -> val RMSE=0.27475\n",
      "Epoch 001 | train RMSE 0.162823 | val RMSE 0.310625\n",
      "Epoch 002 | train RMSE 0.119826 | val RMSE 0.299150\n",
      "Epoch 003 | train RMSE 0.097951 | val RMSE 0.303318\n",
      "Epoch 004 | train RMSE 0.087384 | val RMSE 0.313827\n",
      "Epoch 005 | train RMSE 0.091055 | val RMSE 0.321344\n",
      "Epoch 006 | train RMSE 0.096593 | val RMSE 0.322692\n",
      "Epoch 007 | train RMSE 0.094879 | val RMSE 0.320313\n",
      "Epoch 008 | train RMSE 0.088604 | val RMSE 0.317027\n",
      "Epoch 009 | train RMSE 0.082097 | val RMSE 0.315103\n",
      "Epoch 010 | train RMSE 0.077617 | val RMSE 0.315599\n",
      "  h=16 lr=0.01 act=linear -> val RMSE=0.31560\n",
      "Epoch 001 | train RMSE 0.287665 | val RMSE 0.276135\n",
      "Epoch 002 | train RMSE 0.211533 | val RMSE 0.277395\n",
      "Epoch 003 | train RMSE 0.148867 | val RMSE 0.284770\n",
      "Epoch 004 | train RMSE 0.114784 | val RMSE 0.294137\n",
      "Epoch 005 | train RMSE 0.118745 | val RMSE 0.299943\n",
      "Epoch 006 | train RMSE 0.128673 | val RMSE 0.300684\n",
      "Epoch 007 | train RMSE 0.127327 | val RMSE 0.297653\n",
      "Epoch 008 | train RMSE 0.116980 | val RMSE 0.292643\n",
      "Epoch 009 | train RMSE 0.103785 | val RMSE 0.287179\n",
      "Epoch 010 | train RMSE 0.094012 | val RMSE 0.282477\n",
      "  h=16 lr=0.01 act=tanh -> val RMSE=0.28248\n",
      "Epoch 001 | train RMSE 0.174408 | val RMSE 0.272352\n",
      "Epoch 002 | train RMSE 0.149560 | val RMSE 0.277441\n",
      "Epoch 003 | train RMSE 0.128852 | val RMSE 0.283736\n",
      "Epoch 004 | train RMSE 0.115033 | val RMSE 0.290991\n",
      "Epoch 005 | train RMSE 0.107025 | val RMSE 0.298106\n",
      "Epoch 006 | train RMSE 0.104264 | val RMSE 0.303291\n",
      "Epoch 007 | train RMSE 0.103966 | val RMSE 0.305473\n",
      "Epoch 008 | train RMSE 0.102132 | val RMSE 0.304931\n",
      "Epoch 009 | train RMSE 0.097151 | val RMSE 0.302464\n",
      "Epoch 010 | train RMSE 0.090925 | val RMSE 0.298998\n",
      "  h=16 lr=0.01 act=relu -> val RMSE=0.29900\n",
      "Saved grid results and best per market CSVs\n"
     ]
    }
   ],
   "source": [
    "grid_out_all = []\n",
    "best_by_market = []\n",
    "\n",
    "\n",
    "hidden_list = [4, 8, 16]\n",
    "lr_list = [0.001, 0.01]\n",
    "act_list = [\"linear\", \"tanh\", \"relu\"]\n",
    "w_list = [12]\n",
    "\n",
    "for market_assets in MARKET_CHOICES:\n",
    "    p(f\"Tuning market={'+'.join(market_assets)}\")\n",
    "    pnl = build_panel_for_market(data, market_assets, BASELINE_WINDOW)\n",
    "    if len(pnl[\"Xtr\"]) == 0 or len(pnl[\"Xval\"]) == 0:\n",
    "        p(\"Skipping due to empty train or val\")\n",
    "        continue\n",
    "    results = []\n",
    "    for h in hidden_list:\n",
    "        for lr in lr_list:\n",
    "            for act in act_list:\n",
    "                try:\n",
    "                    model_tmp, tr_h, va_h = train_nn(\n",
    "                        pnl[\"Xtr\"],\n",
    "                        pnl[\"ytr\"],\n",
    "                        pnl[\"Xval\"],\n",
    "                        pnl[\"yval\"],\n",
    "                        hidden=h,\n",
    "                        lr=lr,\n",
    "                        act=act,\n",
    "                        epochs=10,\n",
    "                    )\n",
    "                    val_rmse = va_h[-1]\n",
    "                    results.append(\n",
    "                        [\n",
    "                            \"+\".join(market_assets),\n",
    "                            BASELINE_WINDOW,\n",
    "                            h,\n",
    "                            lr,\n",
    "                            act,\n",
    "                            float(val_rmse),\n",
    "                        ]\n",
    "                    )\n",
    "                    p(f\"  h={h} lr={lr} act={act} -> val RMSE={val_rmse:.5f}\")\n",
    "                except Exception as e:\n",
    "                    p(f\"  error h={h} lr={lr} act={act}: {e}\")\n",
    "    if results:\n",
    "        df_res = pd.DataFrame(\n",
    "            results,\n",
    "            columns=[\n",
    "                \"MarketAssets\",\n",
    "                \"Window\",\n",
    "                \"Hidden\",\n",
    "                \"LR\",\n",
    "                \"Activation\",\n",
    "                \"Val_RMSE\",\n",
    "            ],\n",
    "        )\n",
    "        grid_out_all.append(df_res)\n",
    "\n",
    "        best_row = df_res.sort_values(\n",
    "            \"Val_RMSE\", ascending=True).iloc[0].to_dict()\n",
    "        best_by_market.append(best_row)\n",
    "\n",
    "if grid_out_all:\n",
    "    df_all = pd.concat(grid_out_all, ignore_index=True)\n",
    "    df_all.to_csv(\"outputs/step2_grid_results.csv\", index=False)\n",
    "    pd.DataFrame(best_by_market).to_csv(\n",
    "        \"outputs/step2_best_by_market.csv\", index=False)\n",
    "    p(\"Saved grid results and best per market CSVs\")\n",
    "else:\n",
    "    p(\"No grid results created. Check data lengths or grid size.\")\n",
    "\n",
    "\n",
    "best_map = {}\n",
    "if os.path.exists(\"outputs/step2_best_by_market.csv\"):\n",
    "    best_df = pd.read_csv(\"outputs/step2_best_by_market.csv\")\n",
    "    for _, r in best_df.iterrows():\n",
    "        best_map[r[\"MarketAssets\"]] = dict(\n",
    "            w=int(r[\"Window\"]),\n",
    "            hidden=int(r[\"Hidden\"]),\n",
    "            lr=float(r[\"LR\"]),\n",
    "            act=str(r[\"Activation\"]),\n",
    "        )\n",
    "else:\n",
    "    for m in [\n",
    "        \"FIAT\",\n",
    "        \"EQUITY\",\n",
    "        \"GOLD\",\n",
    "        \"ENERGY\",\n",
    "        \"FIAT+EQUITY\",\n",
    "        \"FIAT+GOLD\",\n",
    "        \"FIAT+EQUITY+GOLD+ENERGY\",\n",
    "    ]:\n",
    "        best_map[m] = dict(\n",
    "            w=BASELINE_WINDOW, hidden=BASELINE_HIDDEN, lr=BASELINE_LR, act=BASELINE_ACT\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0680e9d3",
   "metadata": {},
   "source": [
    "### Step 3 – Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aefca5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train RMSE 0.383330 | val RMSE 0.328050\n",
      "Epoch 002 | train RMSE 0.377260 | val RMSE 0.326259\n",
      "Epoch 003 | train RMSE 0.371208 | val RMSE 0.324499\n",
      "Epoch 004 | train RMSE 0.365174 | val RMSE 0.322771\n",
      "Epoch 005 | train RMSE 0.359160 | val RMSE 0.321075\n",
      "Epoch 006 | train RMSE 0.353167 | val RMSE 0.319411\n",
      "Epoch 007 | train RMSE 0.347196 | val RMSE 0.317780\n",
      "Epoch 008 | train RMSE 0.341251 | val RMSE 0.316184\n",
      "Epoch 009 | train RMSE 0.335331 | val RMSE 0.314621\n",
      "Epoch 010 | train RMSE 0.329439 | val RMSE 0.313094\n",
      "Epoch 011 | train RMSE 0.323576 | val RMSE 0.311602\n",
      "Epoch 012 | train RMSE 0.317744 | val RMSE 0.310147\n",
      "Epoch 013 | train RMSE 0.311946 | val RMSE 0.308728\n",
      "Epoch 014 | train RMSE 0.306184 | val RMSE 0.307346\n",
      "Epoch 015 | train RMSE 0.300458 | val RMSE 0.306001\n",
      "Epoch 016 | train RMSE 0.294773 | val RMSE 0.304695\n",
      "Epoch 017 | train RMSE 0.289129 | val RMSE 0.303428\n",
      "Epoch 018 | train RMSE 0.283530 | val RMSE 0.302199\n",
      "Epoch 019 | train RMSE 0.277979 | val RMSE 0.301010\n",
      "Epoch 020 | train RMSE 0.272478 | val RMSE 0.299861\n",
      "Epoch 021 | train RMSE 0.267030 | val RMSE 0.298751\n",
      "Epoch 022 | train RMSE 0.261639 | val RMSE 0.297682\n",
      "Epoch 023 | train RMSE 0.256307 | val RMSE 0.296653\n",
      "Epoch 024 | train RMSE 0.251039 | val RMSE 0.295664\n",
      "Epoch 025 | train RMSE 0.245838 | val RMSE 0.294717\n",
      "Epoch 001 | train RMSE 0.198878 | val RMSE 0.469178\n",
      "Epoch 002 | train RMSE 0.190727 | val RMSE 0.465543\n",
      "Epoch 003 | train RMSE 0.182705 | val RMSE 0.461928\n",
      "Epoch 004 | train RMSE 0.174831 | val RMSE 0.458333\n",
      "Epoch 005 | train RMSE 0.167129 | val RMSE 0.454759\n",
      "Epoch 006 | train RMSE 0.159622 | val RMSE 0.451207\n",
      "Epoch 007 | train RMSE 0.152340 | val RMSE 0.447680\n",
      "Epoch 008 | train RMSE 0.145312 | val RMSE 0.444192\n",
      "Epoch 009 | train RMSE 0.138571 | val RMSE 0.440759\n",
      "Epoch 010 | train RMSE 0.132152 | val RMSE 0.437396\n",
      "Epoch 011 | train RMSE 0.126086 | val RMSE 0.434120\n",
      "Epoch 012 | train RMSE 0.120408 | val RMSE 0.430941\n",
      "Epoch 013 | train RMSE 0.115147 | val RMSE 0.427865\n",
      "Epoch 014 | train RMSE 0.110334 | val RMSE 0.424895\n",
      "Epoch 015 | train RMSE 0.105996 | val RMSE 0.422030\n",
      "Epoch 016 | train RMSE 0.102158 | val RMSE 0.419271\n",
      "Epoch 017 | train RMSE 0.098835 | val RMSE 0.416621\n",
      "Epoch 018 | train RMSE 0.096031 | val RMSE 0.414085\n",
      "Epoch 019 | train RMSE 0.093737 | val RMSE 0.411673\n",
      "Epoch 020 | train RMSE 0.091925 | val RMSE 0.409401\n",
      "Epoch 021 | train RMSE 0.090554 | val RMSE 0.407281\n",
      "Epoch 022 | train RMSE 0.089567 | val RMSE 0.405330\n",
      "Epoch 023 | train RMSE 0.088897 | val RMSE 0.403560\n",
      "Epoch 024 | train RMSE 0.088471 | val RMSE 0.401983\n",
      "Epoch 025 | train RMSE 0.088214 | val RMSE 0.400605\n",
      "Epoch 001 | train RMSE 0.139244 | val RMSE 0.337312\n",
      "Epoch 002 | train RMSE 0.137471 | val RMSE 0.336191\n",
      "Epoch 003 | train RMSE 0.135753 | val RMSE 0.335084\n",
      "Epoch 004 | train RMSE 0.134094 | val RMSE 0.333991\n",
      "Epoch 005 | train RMSE 0.132496 | val RMSE 0.332908\n",
      "Epoch 006 | train RMSE 0.130973 | val RMSE 0.331842\n",
      "Epoch 007 | train RMSE 0.129510 | val RMSE 0.330796\n",
      "Epoch 008 | train RMSE 0.128111 | val RMSE 0.329776\n",
      "Epoch 009 | train RMSE 0.126774 | val RMSE 0.328773\n",
      "Epoch 010 | train RMSE 0.125497 | val RMSE 0.327798\n",
      "Epoch 011 | train RMSE 0.124272 | val RMSE 0.326858\n",
      "Epoch 012 | train RMSE 0.123091 | val RMSE 0.325955\n",
      "Epoch 013 | train RMSE 0.121965 | val RMSE 0.325093\n",
      "Epoch 014 | train RMSE 0.120887 | val RMSE 0.324280\n",
      "Epoch 015 | train RMSE 0.119844 | val RMSE 0.323519\n",
      "Epoch 016 | train RMSE 0.118845 | val RMSE 0.322812\n",
      "Epoch 017 | train RMSE 0.117893 | val RMSE 0.322161\n",
      "Epoch 018 | train RMSE 0.117015 | val RMSE 0.321570\n",
      "Epoch 019 | train RMSE 0.116162 | val RMSE 0.321036\n",
      "Epoch 020 | train RMSE 0.115348 | val RMSE 0.320576\n",
      "Epoch 021 | train RMSE 0.114580 | val RMSE 0.320190\n",
      "Epoch 022 | train RMSE 0.113828 | val RMSE 0.319882\n",
      "Epoch 023 | train RMSE 0.113089 | val RMSE 0.319649\n",
      "Epoch 024 | train RMSE 0.112371 | val RMSE 0.319492\n",
      "Epoch 025 | train RMSE 0.111655 | val RMSE 0.319405\n",
      "Epoch 001 | train RMSE 0.402256 | val RMSE 0.335207\n",
      "Epoch 002 | train RMSE 0.352710 | val RMSE 0.322565\n",
      "Epoch 003 | train RMSE 0.303801 | val RMSE 0.311800\n",
      "Epoch 004 | train RMSE 0.255516 | val RMSE 0.303452\n",
      "Epoch 005 | train RMSE 0.208937 | val RMSE 0.298086\n",
      "Epoch 006 | train RMSE 0.166598 | val RMSE 0.296205\n",
      "Epoch 007 | train RMSE 0.134005 | val RMSE 0.297983\n",
      "Epoch 008 | train RMSE 0.120186 | val RMSE 0.302370\n",
      "Epoch 009 | train RMSE 0.127907 | val RMSE 0.307190\n",
      "Epoch 010 | train RMSE 0.144093 | val RMSE 0.310687\n",
      "Epoch 011 | train RMSE 0.156102 | val RMSE 0.312210\n",
      "Epoch 012 | train RMSE 0.160000 | val RMSE 0.311821\n",
      "Epoch 013 | train RMSE 0.156264 | val RMSE 0.309907\n",
      "Epoch 014 | train RMSE 0.146820 | val RMSE 0.306969\n",
      "Epoch 015 | train RMSE 0.134108 | val RMSE 0.303518\n",
      "Epoch 016 | train RMSE 0.120893 | val RMSE 0.300009\n",
      "Epoch 017 | train RMSE 0.110108 | val RMSE 0.296807\n",
      "Epoch 018 | train RMSE 0.104119 | val RMSE 0.294165\n",
      "Epoch 019 | train RMSE 0.103357 | val RMSE 0.292206\n",
      "Epoch 020 | train RMSE 0.105940 | val RMSE 0.290948\n",
      "Epoch 021 | train RMSE 0.109183 | val RMSE 0.290365\n",
      "Epoch 022 | train RMSE 0.111099 | val RMSE 0.290443\n",
      "Epoch 023 | train RMSE 0.110751 | val RMSE 0.291207\n",
      "Epoch 024 | train RMSE 0.108026 | val RMSE 0.292706\n",
      "Epoch 025 | train RMSE 0.103388 | val RMSE 0.294982\n",
      "Epoch 001 | train RMSE 0.279093 | val RMSE 0.266792\n",
      "Epoch 002 | train RMSE 0.250886 | val RMSE 0.264623\n",
      "Epoch 003 | train RMSE 0.224442 | val RMSE 0.263868\n",
      "Epoch 004 | train RMSE 0.198915 | val RMSE 0.264447\n",
      "Epoch 005 | train RMSE 0.175060 | val RMSE 0.266416\n",
      "Epoch 006 | train RMSE 0.154623 | val RMSE 0.269660\n",
      "Epoch 007 | train RMSE 0.137225 | val RMSE 0.274258\n",
      "Epoch 008 | train RMSE 0.124094 | val RMSE 0.279853\n",
      "Epoch 009 | train RMSE 0.116217 | val RMSE 0.285938\n",
      "Epoch 010 | train RMSE 0.115081 | val RMSE 0.291549\n",
      "Epoch 011 | train RMSE 0.117682 | val RMSE 0.295866\n",
      "Epoch 012 | train RMSE 0.120041 | val RMSE 0.298236\n",
      "Epoch 013 | train RMSE 0.119423 | val RMSE 0.298746\n",
      "Epoch 014 | train RMSE 0.115675 | val RMSE 0.297707\n",
      "Epoch 015 | train RMSE 0.110027 | val RMSE 0.295610\n",
      "Epoch 016 | train RMSE 0.104284 | val RMSE 0.292937\n",
      "Epoch 017 | train RMSE 0.099754 | val RMSE 0.290018\n",
      "Epoch 018 | train RMSE 0.096172 | val RMSE 0.287170\n",
      "Epoch 019 | train RMSE 0.094190 | val RMSE 0.284629\n",
      "Epoch 020 | train RMSE 0.093236 | val RMSE 0.282567\n",
      "Epoch 021 | train RMSE 0.092906 | val RMSE 0.281036\n",
      "Epoch 022 | train RMSE 0.092695 | val RMSE 0.280044\n",
      "Epoch 023 | train RMSE 0.092167 | val RMSE 0.279582\n",
      "Epoch 024 | train RMSE 0.091086 | val RMSE 0.279597\n",
      "Epoch 025 | train RMSE 0.089584 | val RMSE 0.280049\n",
      "Epoch 001 | train RMSE 0.239893 | val RMSE 0.262248\n",
      "Epoch 002 | train RMSE 0.232005 | val RMSE 0.262886\n",
      "Epoch 003 | train RMSE 0.224225 | val RMSE 0.263598\n",
      "Epoch 004 | train RMSE 0.216566 | val RMSE 0.264381\n",
      "Epoch 005 | train RMSE 0.209041 | val RMSE 0.265234\n",
      "Epoch 006 | train RMSE 0.201667 | val RMSE 0.266155\n",
      "Epoch 007 | train RMSE 0.194463 | val RMSE 0.267144\n",
      "Epoch 008 | train RMSE 0.187447 | val RMSE 0.268196\n",
      "Epoch 009 | train RMSE 0.180641 | val RMSE 0.269310\n",
      "Epoch 010 | train RMSE 0.174068 | val RMSE 0.270482\n",
      "Epoch 011 | train RMSE 0.167753 | val RMSE 0.271709\n",
      "Epoch 012 | train RMSE 0.161722 | val RMSE 0.272986\n",
      "Epoch 013 | train RMSE 0.156001 | val RMSE 0.274306\n",
      "Epoch 014 | train RMSE 0.150617 | val RMSE 0.275662\n",
      "Epoch 015 | train RMSE 0.145598 | val RMSE 0.277045\n",
      "Epoch 016 | train RMSE 0.140964 | val RMSE 0.278445\n",
      "Epoch 017 | train RMSE 0.136736 | val RMSE 0.279852\n",
      "Epoch 018 | train RMSE 0.132925 | val RMSE 0.281252\n",
      "Epoch 019 | train RMSE 0.129535 | val RMSE 0.282634\n",
      "Epoch 020 | train RMSE 0.126559 | val RMSE 0.283986\n",
      "Epoch 021 | train RMSE 0.123984 | val RMSE 0.285296\n",
      "Epoch 022 | train RMSE 0.121782 | val RMSE 0.286553\n",
      "Epoch 023 | train RMSE 0.119921 | val RMSE 0.287748\n",
      "Epoch 024 | train RMSE 0.118360 | val RMSE 0.288873\n",
      "Epoch 025 | train RMSE 0.117054 | val RMSE 0.289921\n",
      "Epoch 001 | train RMSE 0.263720 | val RMSE 0.494072\n",
      "Epoch 002 | train RMSE 0.258812 | val RMSE 0.491837\n",
      "Epoch 003 | train RMSE 0.253916 | val RMSE 0.489602\n",
      "Epoch 004 | train RMSE 0.249035 | val RMSE 0.487368\n",
      "Epoch 005 | train RMSE 0.244170 | val RMSE 0.485134\n",
      "Epoch 006 | train RMSE 0.239324 | val RMSE 0.482900\n",
      "Epoch 007 | train RMSE 0.234499 | val RMSE 0.480669\n",
      "Epoch 008 | train RMSE 0.229697 | val RMSE 0.478438\n",
      "Epoch 009 | train RMSE 0.224921 | val RMSE 0.476211\n",
      "Epoch 010 | train RMSE 0.220173 | val RMSE 0.473986\n",
      "Epoch 011 | train RMSE 0.215455 | val RMSE 0.471764\n",
      "Epoch 012 | train RMSE 0.210771 | val RMSE 0.469546\n",
      "Epoch 013 | train RMSE 0.206124 | val RMSE 0.467333\n",
      "Epoch 014 | train RMSE 0.201515 | val RMSE 0.465125\n",
      "Epoch 015 | train RMSE 0.196948 | val RMSE 0.462925\n",
      "Epoch 016 | train RMSE 0.192427 | val RMSE 0.460734\n",
      "Epoch 017 | train RMSE 0.187953 | val RMSE 0.458554\n",
      "Epoch 018 | train RMSE 0.183532 | val RMSE 0.456387\n",
      "Epoch 019 | train RMSE 0.179166 | val RMSE 0.454236\n",
      "Epoch 020 | train RMSE 0.174861 | val RMSE 0.452102\n",
      "Epoch 021 | train RMSE 0.170621 | val RMSE 0.449988\n",
      "Epoch 022 | train RMSE 0.166450 | val RMSE 0.447896\n",
      "Epoch 023 | train RMSE 0.162355 | val RMSE 0.445830\n",
      "Epoch 024 | train RMSE 0.158340 | val RMSE 0.443790\n",
      "Epoch 025 | train RMSE 0.154412 | val RMSE 0.441779\n",
      "Saved learning curves\n"
     ]
    }
   ],
   "source": [
    "curves_index = []\n",
    "for market_assets in MARKET_CHOICES:\n",
    "    label = \"+\".join(market_assets)\n",
    "    cfg = best_map.get(\n",
    "        label,\n",
    "        dict(\n",
    "            w=BASELINE_WINDOW, hidden=BASELINE_HIDDEN, lr=BASELINE_LR, act=BASELINE_ACT\n",
    "        ),\n",
    "    )\n",
    "    w = int(cfg[\"w\"])\n",
    "    pnl = build_panel_for_market(data, market_assets, w)\n",
    "    if len(pnl[\"Xtr\"]) == 0 or len(pnl[\"Xval\"]) == 0:\n",
    "        p(f\"Skipping curves for {label} due to empty split\")\n",
    "        continue\n",
    "    model_best, tr_hist, va_hist = train_nn(\n",
    "        pnl[\"Xtr\"],\n",
    "        pnl[\"ytr\"],\n",
    "        pnl[\"Xval\"],\n",
    "        pnl[\"yval\"],\n",
    "        hidden=int(cfg[\"hidden\"]),\n",
    "        lr=float(cfg[\"lr\"]),\n",
    "        act=str(cfg[\"act\"]),\n",
    "        epochs=BASELINE_EPOCHS,\n",
    "    )\n",
    "    model_path = f\"outputs/model_step3_{label.replace('+', '_')}.pt\"\n",
    "    torch.save(model_best.state_dict(), model_path)\n",
    "    plot_path = f\"outputs/step3_learning_curve_{label.replace('+', '_')}.png\"\n",
    "    plot_learning_curve(\n",
    "        tr_hist, va_hist, f\"{label} w={w} act={cfg['act']}\", plot_path)\n",
    "    curves_index.append(\n",
    "        dict(market=label, model_path=model_path, curve_path=plot_path, cfg=cfg)\n",
    "    )\n",
    "\n",
    "with open(\"outputs/step3_curves_index.json\", \"w\") as f:\n",
    "    json.dump(curves_index, f, indent=2)\n",
    "\n",
    "p(\"Saved learning curves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919793a",
   "metadata": {},
   "source": [
    "### Step 4 – Descriptive statistics of neural 𝛽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7ea627d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train RMSE 0.342523 | val RMSE 0.371415\n",
      "Epoch 002 | train RMSE 0.339274 | val RMSE 0.367903\n",
      "Epoch 003 | train RMSE 0.336060 | val RMSE 0.364419\n",
      "Epoch 004 | train RMSE 0.332880 | val RMSE 0.360963\n",
      "Epoch 005 | train RMSE 0.329736 | val RMSE 0.357534\n",
      "Epoch 006 | train RMSE 0.326628 | val RMSE 0.354135\n",
      "Epoch 007 | train RMSE 0.323555 | val RMSE 0.350764\n",
      "Epoch 008 | train RMSE 0.320519 | val RMSE 0.347425\n",
      "Epoch 009 | train RMSE 0.317520 | val RMSE 0.344116\n",
      "Epoch 010 | train RMSE 0.314559 | val RMSE 0.340841\n",
      "Epoch 011 | train RMSE 0.311637 | val RMSE 0.337600\n",
      "Epoch 012 | train RMSE 0.308754 | val RMSE 0.334394\n",
      "Epoch 013 | train RMSE 0.305912 | val RMSE 0.331225\n",
      "Epoch 014 | train RMSE 0.303113 | val RMSE 0.328094\n",
      "Epoch 015 | train RMSE 0.300355 | val RMSE 0.325002\n",
      "Epoch 016 | train RMSE 0.297642 | val RMSE 0.321949\n",
      "Epoch 017 | train RMSE 0.294973 | val RMSE 0.318938\n",
      "Epoch 018 | train RMSE 0.292349 | val RMSE 0.315969\n",
      "Epoch 019 | train RMSE 0.289771 | val RMSE 0.313043\n",
      "Epoch 020 | train RMSE 0.287241 | val RMSE 0.310161\n",
      "Epoch 021 | train RMSE 0.284758 | val RMSE 0.307324\n",
      "Epoch 022 | train RMSE 0.282324 | val RMSE 0.304533\n",
      "Epoch 023 | train RMSE 0.279938 | val RMSE 0.301788\n",
      "Epoch 024 | train RMSE 0.277603 | val RMSE 0.299091\n",
      "Epoch 025 | train RMSE 0.275317 | val RMSE 0.296441\n",
      "Epoch 001 | train RMSE 0.262443 | val RMSE 0.279056\n",
      "Epoch 002 | train RMSE 0.260757 | val RMSE 0.277097\n",
      "Epoch 003 | train RMSE 0.259130 | val RMSE 0.275196\n",
      "Epoch 004 | train RMSE 0.257561 | val RMSE 0.273355\n",
      "Epoch 005 | train RMSE 0.256050 | val RMSE 0.271571\n",
      "Epoch 006 | train RMSE 0.254596 | val RMSE 0.269845\n",
      "Epoch 007 | train RMSE 0.253199 | val RMSE 0.268178\n",
      "Epoch 008 | train RMSE 0.251857 | val RMSE 0.266568\n",
      "Epoch 009 | train RMSE 0.250567 | val RMSE 0.265014\n",
      "Epoch 010 | train RMSE 0.249326 | val RMSE 0.263516\n",
      "Epoch 011 | train RMSE 0.248130 | val RMSE 0.262069\n",
      "Epoch 012 | train RMSE 0.246972 | val RMSE 0.260671\n",
      "Epoch 013 | train RMSE 0.245848 | val RMSE 0.259318\n",
      "Epoch 014 | train RMSE 0.244751 | val RMSE 0.258008\n",
      "Epoch 015 | train RMSE 0.243677 | val RMSE 0.256737\n",
      "Epoch 016 | train RMSE 0.242620 | val RMSE 0.255502\n",
      "Epoch 017 | train RMSE 0.241577 | val RMSE 0.254299\n",
      "Epoch 018 | train RMSE 0.240543 | val RMSE 0.253127\n",
      "Epoch 019 | train RMSE 0.239516 | val RMSE 0.251982\n",
      "Epoch 020 | train RMSE 0.238495 | val RMSE 0.250863\n",
      "Epoch 021 | train RMSE 0.237478 | val RMSE 0.249768\n",
      "Epoch 022 | train RMSE 0.236466 | val RMSE 0.248695\n",
      "Epoch 023 | train RMSE 0.235459 | val RMSE 0.247642\n",
      "Epoch 024 | train RMSE 0.234457 | val RMSE 0.246609\n",
      "Epoch 025 | train RMSE 0.233461 | val RMSE 0.245594\n",
      "Epoch 001 | train RMSE 0.279233 | val RMSE 0.301722\n",
      "Epoch 002 | train RMSE 0.277871 | val RMSE 0.300139\n",
      "Epoch 003 | train RMSE 0.276526 | val RMSE 0.298574\n",
      "Epoch 004 | train RMSE 0.275202 | val RMSE 0.297028\n",
      "Epoch 005 | train RMSE 0.273902 | val RMSE 0.295500\n",
      "Epoch 006 | train RMSE 0.272623 | val RMSE 0.293988\n",
      "Epoch 007 | train RMSE 0.271364 | val RMSE 0.292496\n",
      "Epoch 008 | train RMSE 0.270127 | val RMSE 0.291025\n",
      "Epoch 009 | train RMSE 0.268914 | val RMSE 0.289576\n",
      "Epoch 010 | train RMSE 0.267724 | val RMSE 0.288147\n",
      "Epoch 011 | train RMSE 0.266554 | val RMSE 0.286741\n",
      "Epoch 012 | train RMSE 0.265411 | val RMSE 0.285363\n",
      "Epoch 013 | train RMSE 0.264296 | val RMSE 0.284007\n",
      "Epoch 014 | train RMSE 0.263210 | val RMSE 0.282672\n",
      "Epoch 015 | train RMSE 0.262147 | val RMSE 0.281360\n",
      "Epoch 016 | train RMSE 0.261114 | val RMSE 0.280087\n",
      "Epoch 017 | train RMSE 0.260119 | val RMSE 0.278835\n",
      "Epoch 018 | train RMSE 0.259145 | val RMSE 0.277601\n",
      "Epoch 019 | train RMSE 0.258192 | val RMSE 0.276385\n",
      "Epoch 020 | train RMSE 0.257256 | val RMSE 0.275188\n",
      "Epoch 021 | train RMSE 0.256341 | val RMSE 0.274017\n",
      "Epoch 022 | train RMSE 0.255452 | val RMSE 0.272871\n",
      "Epoch 023 | train RMSE 0.254589 | val RMSE 0.271742\n",
      "Epoch 024 | train RMSE 0.253744 | val RMSE 0.270633\n",
      "Epoch 025 | train RMSE 0.252918 | val RMSE 0.269542\n",
      "Epoch 001 | train RMSE 0.263916 | val RMSE 0.274617\n",
      "Epoch 002 | train RMSE 0.257991 | val RMSE 0.271091\n",
      "Epoch 003 | train RMSE 0.253754 | val RMSE 0.268568\n",
      "Epoch 004 | train RMSE 0.250749 | val RMSE 0.266228\n",
      "Epoch 005 | train RMSE 0.248269 | val RMSE 0.263576\n",
      "Epoch 006 | train RMSE 0.245806 | val RMSE 0.260507\n",
      "Epoch 007 | train RMSE 0.243184 | val RMSE 0.257114\n",
      "Epoch 008 | train RMSE 0.240421 | val RMSE 0.253547\n",
      "Epoch 009 | train RMSE 0.237590 | val RMSE 0.249941\n",
      "Epoch 010 | train RMSE 0.234727 | val RMSE 0.246344\n",
      "Epoch 011 | train RMSE 0.231764 | val RMSE 0.242691\n",
      "Epoch 012 | train RMSE 0.228564 | val RMSE 0.238900\n",
      "Epoch 013 | train RMSE 0.225055 | val RMSE 0.234948\n",
      "Epoch 014 | train RMSE 0.221265 | val RMSE 0.230890\n",
      "Epoch 015 | train RMSE 0.217312 | val RMSE 0.226836\n",
      "Epoch 016 | train RMSE 0.213365 | val RMSE 0.222917\n",
      "Epoch 017 | train RMSE 0.209608 | val RMSE 0.219225\n",
      "Epoch 018 | train RMSE 0.206174 | val RMSE 0.215753\n",
      "Epoch 019 | train RMSE 0.203075 | val RMSE 0.212400\n",
      "Epoch 020 | train RMSE 0.200214 | val RMSE 0.209090\n",
      "Epoch 021 | train RMSE 0.197510 | val RMSE 0.205837\n",
      "Epoch 022 | train RMSE 0.194983 | val RMSE 0.202694\n",
      "Epoch 023 | train RMSE 0.192703 | val RMSE 0.199661\n",
      "Epoch 024 | train RMSE 0.190659 | val RMSE 0.196671\n",
      "Epoch 025 | train RMSE 0.188707 | val RMSE 0.193689\n",
      "Epoch 001 | train RMSE 0.269640 | val RMSE 0.272685\n",
      "Epoch 002 | train RMSE 0.259902 | val RMSE 0.266558\n",
      "Epoch 003 | train RMSE 0.252015 | val RMSE 0.261248\n",
      "Epoch 004 | train RMSE 0.245452 | val RMSE 0.255700\n",
      "Epoch 005 | train RMSE 0.239411 | val RMSE 0.249612\n",
      "Epoch 006 | train RMSE 0.233503 | val RMSE 0.242488\n",
      "Epoch 007 | train RMSE 0.227062 | val RMSE 0.234931\n",
      "Epoch 008 | train RMSE 0.220441 | val RMSE 0.227391\n",
      "Epoch 009 | train RMSE 0.213978 | val RMSE 0.220112\n",
      "Epoch 010 | train RMSE 0.207831 | val RMSE 0.212948\n",
      "Epoch 011 | train RMSE 0.201750 | val RMSE 0.205891\n",
      "Epoch 012 | train RMSE 0.195647 | val RMSE 0.198944\n",
      "Epoch 013 | train RMSE 0.189716 | val RMSE 0.192327\n",
      "Epoch 014 | train RMSE 0.184041 | val RMSE 0.186244\n",
      "Epoch 015 | train RMSE 0.178724 | val RMSE 0.180926\n",
      "Epoch 016 | train RMSE 0.173931 | val RMSE 0.176166\n",
      "Epoch 017 | train RMSE 0.169631 | val RMSE 0.172027\n",
      "Epoch 018 | train RMSE 0.165904 | val RMSE 0.168411\n",
      "Epoch 019 | train RMSE 0.162695 | val RMSE 0.164990\n",
      "Epoch 020 | train RMSE 0.159738 | val RMSE 0.161566\n",
      "Epoch 021 | train RMSE 0.156849 | val RMSE 0.157975\n",
      "Epoch 022 | train RMSE 0.153848 | val RMSE 0.154131\n",
      "Epoch 023 | train RMSE 0.150689 | val RMSE 0.150019\n",
      "Epoch 024 | train RMSE 0.147318 | val RMSE 0.145930\n",
      "Epoch 025 | train RMSE 0.143893 | val RMSE 0.141951\n",
      "Epoch 001 | train RMSE 0.436066 | val RMSE 0.463092\n",
      "Epoch 002 | train RMSE 0.430763 | val RMSE 0.457626\n",
      "Epoch 003 | train RMSE 0.425508 | val RMSE 0.452212\n",
      "Epoch 004 | train RMSE 0.420303 | val RMSE 0.446851\n",
      "Epoch 005 | train RMSE 0.415150 | val RMSE 0.441544\n",
      "Epoch 006 | train RMSE 0.410049 | val RMSE 0.436294\n",
      "Epoch 007 | train RMSE 0.405003 | val RMSE 0.431100\n",
      "Epoch 008 | train RMSE 0.400012 | val RMSE 0.425963\n",
      "Epoch 009 | train RMSE 0.395077 | val RMSE 0.420883\n",
      "Epoch 010 | train RMSE 0.390198 | val RMSE 0.415862\n",
      "Epoch 011 | train RMSE 0.385376 | val RMSE 0.410899\n",
      "Epoch 012 | train RMSE 0.380612 | val RMSE 0.405996\n",
      "Epoch 013 | train RMSE 0.375906 | val RMSE 0.401152\n",
      "Epoch 014 | train RMSE 0.371260 | val RMSE 0.396370\n",
      "Epoch 015 | train RMSE 0.366674 | val RMSE 0.391650\n",
      "Epoch 016 | train RMSE 0.362149 | val RMSE 0.386992\n",
      "Epoch 017 | train RMSE 0.357687 | val RMSE 0.382398\n",
      "Epoch 018 | train RMSE 0.353288 | val RMSE 0.377868\n",
      "Epoch 019 | train RMSE 0.348954 | val RMSE 0.373402\n",
      "Epoch 020 | train RMSE 0.344685 | val RMSE 0.369002\n",
      "Epoch 021 | train RMSE 0.340482 | val RMSE 0.364667\n",
      "Epoch 022 | train RMSE 0.336347 | val RMSE 0.360399\n",
      "Epoch 023 | train RMSE 0.332279 | val RMSE 0.356197\n",
      "Epoch 024 | train RMSE 0.328279 | val RMSE 0.352062\n",
      "Epoch 025 | train RMSE 0.324350 | val RMSE 0.347993\n",
      "Epoch 001 | train RMSE 0.498837 | val RMSE 0.525336\n",
      "Epoch 002 | train RMSE 0.495567 | val RMSE 0.521992\n",
      "Epoch 003 | train RMSE 0.492309 | val RMSE 0.518662\n",
      "Epoch 004 | train RMSE 0.489065 | val RMSE 0.515347\n",
      "Epoch 005 | train RMSE 0.485834 | val RMSE 0.512046\n",
      "Epoch 006 | train RMSE 0.482617 | val RMSE 0.508759\n",
      "Epoch 007 | train RMSE 0.479414 | val RMSE 0.505487\n",
      "Epoch 008 | train RMSE 0.476225 | val RMSE 0.502230\n",
      "Epoch 009 | train RMSE 0.473050 | val RMSE 0.498989\n",
      "Epoch 010 | train RMSE 0.469889 | val RMSE 0.495762\n",
      "Epoch 011 | train RMSE 0.466743 | val RMSE 0.492551\n",
      "Epoch 012 | train RMSE 0.463611 | val RMSE 0.489355\n",
      "Epoch 013 | train RMSE 0.460493 | val RMSE 0.486174\n",
      "Epoch 014 | train RMSE 0.457389 | val RMSE 0.483008\n",
      "Epoch 015 | train RMSE 0.454300 | val RMSE 0.479857\n",
      "Epoch 016 | train RMSE 0.451224 | val RMSE 0.476720\n",
      "Epoch 017 | train RMSE 0.448162 | val RMSE 0.473598\n",
      "Epoch 018 | train RMSE 0.445113 | val RMSE 0.470490\n",
      "Epoch 019 | train RMSE 0.442078 | val RMSE 0.467395\n",
      "Epoch 020 | train RMSE 0.439056 | val RMSE 0.464313\n",
      "Epoch 021 | train RMSE 0.436046 | val RMSE 0.461244\n",
      "Epoch 022 | train RMSE 0.433048 | val RMSE 0.458187\n",
      "Epoch 023 | train RMSE 0.430061 | val RMSE 0.455141\n",
      "Epoch 024 | train RMSE 0.427086 | val RMSE 0.452106\n",
      "Epoch 025 | train RMSE 0.424121 | val RMSE 0.449080\n",
      "Saved test betas and descriptive stats\n"
     ]
    }
   ],
   "source": [
    "all_betas = []\n",
    "for market_assets in MARKET_CHOICES:\n",
    "    label = \"+\".join(market_assets)\n",
    "    cfg = best_map.get(\n",
    "        label,\n",
    "        dict(\n",
    "            w=BASELINE_WINDOW, hidden=BASELINE_HIDDEN, lr=BASELINE_LR, act=BASELINE_ACT\n",
    "        ),\n",
    "    )\n",
    "    w = int(cfg[\"w\"])\n",
    "    pnl = build_panel_for_market(data, market_assets, w)\n",
    "    if len(pnl[\"Xtr\"]) == 0 or len(pnl[\"Xval\"]) == 0 or len(pnl[\"Xte\"]) == 0:\n",
    "        p(f\"Skipping {label} stats due to empty split\")\n",
    "        continue\n",
    "\n",
    "    X_tv = np.vstack([pnl[\"Xtr\"], pnl[\"Xval\"]])\n",
    "    y_tv = np.hstack([pnl[\"ytr\"], pnl[\"yval\"]])\n",
    "    model_tv, _, _ = train_nn(\n",
    "        X_tv,\n",
    "        y_tv,\n",
    "        pnl[\"Xval\"],\n",
    "        pnl[\"yval\"],\n",
    "        hidden=int(cfg[\"hidden\"]),\n",
    "        lr=float(cfg[\"lr\"]),\n",
    "        act=str(cfg[\"act\"]),\n",
    "        epochs=BASELINE_EPOCHS,\n",
    "    )\n",
    "    df_b = predict_betas_df(model_tv, pnl[\"Xte\"], pnl[\"ids_te\"], label)\n",
    "    all_betas.append(df_b)\n",
    "\n",
    "if all_betas:\n",
    "    df_betas = pd.concat(all_betas, ignore_index=True)\n",
    "\n",
    "    df_betas = df_betas[\n",
    "        (df_betas[\"Date\"] >= \"2023-01-01\") & (df_betas[\"Date\"] <= \"2025-10-31\")\n",
    "    ]\n",
    "    df_betas.to_csv(\"outputs/step4_test_betas_panel.csv\", index=False)\n",
    "\n",
    "    rows = []\n",
    "    for (c, a), g in df_betas.groupby([\"Crypto\", \"Asset\"]):\n",
    "        b = g[\"Beta\"].values\n",
    "        d = compute_descriptive_stats(b)\n",
    "        d.update({\"Crypto\": c, \"Asset\": a})\n",
    "        rows.append(d)\n",
    "    stats_df = pd.DataFrame(rows)[\n",
    "        [\n",
    "            \"Crypto\",\n",
    "            \"Asset\",\n",
    "            \"N\",\n",
    "            \"Mean\",\n",
    "            \"Median\",\n",
    "            \"Std\",\n",
    "            \"Variance\",\n",
    "            \"Skew\",\n",
    "            \"Kurtosis\",\n",
    "            \"Min\",\n",
    "            \"P1\",\n",
    "            \"P5\",\n",
    "            \"P25\",\n",
    "            \"P50\",\n",
    "            \"P75\",\n",
    "            \"P95\",\n",
    "            \"P99\",\n",
    "            \"Max\",\n",
    "        ]\n",
    "    ].sort_values([\"Asset\", \"Crypto\"])\n",
    "    stats_df.to_csv(\"outputs/step4_neural_beta_descriptives.csv\", index=False)\n",
    "    p(\"Saved test betas and descriptive stats\")\n",
    "else:\n",
    "    p(\"No betas computed. Check earlier steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cb104b",
   "metadata": {},
   "source": [
    "### Step 5 – Dynamics of neural 𝛽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de07e0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved beta dynamics plot\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"outputs/step4_test_betas_panel.csv\"):\n",
    "    df_betas = pd.read_csv(\n",
    "        \"outputs/step4_test_betas_panel.csv\", parse_dates=[\"Date\"])\n",
    "    plot_annual_beta_dynamics(df_betas, \"outputs/step5_beta_dynamics.png\")\n",
    "    p(\"Saved beta dynamics plot\")\n",
    "else:\n",
    "    p(\"No betas panel found. Skipping..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ebe263",
   "metadata": {},
   "source": [
    "### Step 6 – Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf77545",
   "metadata": {},
   "source": [
    "Across the sample period, the neural betas (β) estimated for BTC, ETH, LTC, and BCH reveal a consistent pattern of weak or negative relationships with traditional market factors. On average, β values range between –0.05 and –0.07, indicating that cryptocurrencies exhibit limited sensitivity to conventional macroeconomic assets such as fiat currencies, equities, gold, and energy commodities. This overall independence supports the idea that crypto markets continue to operate largely as a separate asset class, driven by their own liquidity and sentiment cycles rather than traditional financial fundamentals.\n",
    "\n",
    "Among the asset classes, equity factors exerted the strongest and most variable influence on crypto returns. During months of heightened market activity, betas against equity markets tended to rise, suggesting that cryptos partially align with equity risk-on/risk-off dynamics during turbulent periods. By contrast, fiat betas remained close to zero and largely stable, implying minimal connection to broad currency fluctuations. Gold betas were small but occasionally positive, consistent with gold’s weakly diversifying relationship to risk assets. Energy betas showed higher volatility in the middle of the year, hinting at short-lived correlations between crypto and commodity price cycles.\n",
    "\n",
    "Across cryptocurrencies, BTC and ETH displayed the most stable and consistent β trajectories, reacting similarly to macroeconomic changes. LTC and BCH, however, exhibited more erratic movements and wider month-to-month variation, likely reflecting thinner market depth and greater exposure to idiosyncratic shocks. Temporally, neural betas were less negative in early 2023, dipped during mid-year, and recovered slightly toward year-end, broadly mirroring the rebound in overall crypto valuations. \n",
    "\n",
    "Overall, these findings suggest that while cryptos occasionally move in tandem with equities or commodities during high-volatility periods, their market dynamics remain distinct and largely insulated from traditional macroeconomic factors. The relationships identified are transient rather than structural, highlighting the evolving and independent nature of digital asset risk exposures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a423c191",
   "metadata": {},
   "source": [
    "### Step 7 – Including additional factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e35e3ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged table with FF factors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found FF factor columns: ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']\n",
      "Epoch 001 | train RMSE 0.366979 | val RMSE 0.350445\n",
      "Epoch 002 | train RMSE 0.363243 | val RMSE 0.347097\n",
      "Epoch 003 | train RMSE 0.359543 | val RMSE 0.343788\n",
      "Epoch 004 | train RMSE 0.355881 | val RMSE 0.340517\n",
      "Epoch 005 | train RMSE 0.352255 | val RMSE 0.337282\n",
      "Epoch 006 | train RMSE 0.348667 | val RMSE 0.334082\n",
      "Epoch 007 | train RMSE 0.345114 | val RMSE 0.330915\n",
      "Epoch 008 | train RMSE 0.341597 | val RMSE 0.327781\n",
      "Epoch 009 | train RMSE 0.338113 | val RMSE 0.324680\n",
      "Epoch 010 | train RMSE 0.334662 | val RMSE 0.321612\n",
      "Epoch 011 | train RMSE 0.331246 | val RMSE 0.318577\n",
      "Epoch 012 | train RMSE 0.327862 | val RMSE 0.315574\n",
      "Epoch 013 | train RMSE 0.324512 | val RMSE 0.312604\n",
      "Epoch 014 | train RMSE 0.321195 | val RMSE 0.309665\n",
      "Epoch 015 | train RMSE 0.317910 | val RMSE 0.306756\n",
      "Epoch 016 | train RMSE 0.314656 | val RMSE 0.303876\n",
      "Epoch 017 | train RMSE 0.311434 | val RMSE 0.301026\n",
      "Epoch 018 | train RMSE 0.308242 | val RMSE 0.298203\n",
      "Epoch 019 | train RMSE 0.305080 | val RMSE 0.295408\n",
      "Epoch 020 | train RMSE 0.301946 | val RMSE 0.292640\n",
      "Epoch 021 | train RMSE 0.298842 | val RMSE 0.289899\n",
      "Epoch 022 | train RMSE 0.295765 | val RMSE 0.287183\n",
      "Epoch 023 | train RMSE 0.292717 | val RMSE 0.284494\n",
      "Epoch 024 | train RMSE 0.289695 | val RMSE 0.281829\n",
      "Epoch 025 | train RMSE 0.286701 | val RMSE 0.279190\n",
      "Epoch 001 | train RMSE 0.381314 | val RMSE 0.405745\n",
      "Epoch 002 | train RMSE 0.378657 | val RMSE 0.403126\n",
      "Epoch 003 | train RMSE 0.376022 | val RMSE 0.400530\n",
      "Epoch 004 | train RMSE 0.373410 | val RMSE 0.397956\n",
      "Epoch 005 | train RMSE 0.370818 | val RMSE 0.395403\n",
      "Epoch 006 | train RMSE 0.368246 | val RMSE 0.392870\n",
      "Epoch 007 | train RMSE 0.365694 | val RMSE 0.390356\n",
      "Epoch 008 | train RMSE 0.363161 | val RMSE 0.387861\n",
      "Epoch 009 | train RMSE 0.360648 | val RMSE 0.385385\n",
      "Epoch 010 | train RMSE 0.358156 | val RMSE 0.382928\n",
      "Epoch 011 | train RMSE 0.355684 | val RMSE 0.380490\n",
      "Epoch 012 | train RMSE 0.353233 | val RMSE 0.378072\n",
      "Epoch 013 | train RMSE 0.350803 | val RMSE 0.375673\n",
      "Epoch 014 | train RMSE 0.348395 | val RMSE 0.373294\n",
      "Epoch 015 | train RMSE 0.346008 | val RMSE 0.370934\n",
      "Epoch 016 | train RMSE 0.343644 | val RMSE 0.368594\n",
      "Epoch 017 | train RMSE 0.341301 | val RMSE 0.366273\n",
      "Epoch 018 | train RMSE 0.338980 | val RMSE 0.363973\n",
      "Epoch 019 | train RMSE 0.336682 | val RMSE 0.361692\n",
      "Epoch 020 | train RMSE 0.334407 | val RMSE 0.359432\n",
      "Epoch 021 | train RMSE 0.332153 | val RMSE 0.357191\n",
      "Epoch 022 | train RMSE 0.329923 | val RMSE 0.354971\n",
      "Epoch 023 | train RMSE 0.327716 | val RMSE 0.352771\n",
      "Epoch 024 | train RMSE 0.325531 | val RMSE 0.350591\n",
      "Epoch 025 | train RMSE 0.323369 | val RMSE 0.348432\n",
      "FIAT: Base RMSE=0.327146, FF RMSE=0.371619, Improvement=-0.044473\n",
      "Epoch 001 | train RMSE 0.264684 | val RMSE 0.271900\n",
      "Epoch 002 | train RMSE 0.261285 | val RMSE 0.268958\n",
      "Epoch 003 | train RMSE 0.257979 | val RMSE 0.266102\n",
      "Epoch 004 | train RMSE 0.254767 | val RMSE 0.263327\n",
      "Epoch 005 | train RMSE 0.251652 | val RMSE 0.260628\n",
      "Epoch 006 | train RMSE 0.248630 | val RMSE 0.257998\n",
      "Epoch 007 | train RMSE 0.245699 | val RMSE 0.255432\n",
      "Epoch 008 | train RMSE 0.242857 | val RMSE 0.252928\n",
      "Epoch 009 | train RMSE 0.240101 | val RMSE 0.250480\n",
      "Epoch 010 | train RMSE 0.237427 | val RMSE 0.248085\n",
      "Epoch 011 | train RMSE 0.234831 | val RMSE 0.245739\n",
      "Epoch 012 | train RMSE 0.232309 | val RMSE 0.243435\n",
      "Epoch 013 | train RMSE 0.229856 | val RMSE 0.241168\n",
      "Epoch 014 | train RMSE 0.227467 | val RMSE 0.238932\n",
      "Epoch 015 | train RMSE 0.225136 | val RMSE 0.236720\n",
      "Epoch 016 | train RMSE 0.222858 | val RMSE 0.234524\n",
      "Epoch 017 | train RMSE 0.220626 | val RMSE 0.232339\n",
      "Epoch 018 | train RMSE 0.218433 | val RMSE 0.230156\n",
      "Epoch 019 | train RMSE 0.216275 | val RMSE 0.227970\n",
      "Epoch 020 | train RMSE 0.214146 | val RMSE 0.225776\n",
      "Epoch 021 | train RMSE 0.212041 | val RMSE 0.223569\n",
      "Epoch 022 | train RMSE 0.209957 | val RMSE 0.221347\n",
      "Epoch 023 | train RMSE 0.207891 | val RMSE 0.219108\n",
      "Epoch 024 | train RMSE 0.205841 | val RMSE 0.216855\n",
      "Epoch 025 | train RMSE 0.203808 | val RMSE 0.214589\n",
      "Epoch 001 | train RMSE 0.269542 | val RMSE 0.291422\n",
      "Epoch 002 | train RMSE 0.267465 | val RMSE 0.289012\n",
      "Epoch 003 | train RMSE 0.265455 | val RMSE 0.286662\n",
      "Epoch 004 | train RMSE 0.263513 | val RMSE 0.284373\n",
      "Epoch 005 | train RMSE 0.261636 | val RMSE 0.282148\n",
      "Epoch 006 | train RMSE 0.259822 | val RMSE 0.279988\n",
      "Epoch 007 | train RMSE 0.258068 | val RMSE 0.277892\n",
      "Epoch 008 | train RMSE 0.256375 | val RMSE 0.275861\n",
      "Epoch 009 | train RMSE 0.254741 | val RMSE 0.273893\n",
      "Epoch 010 | train RMSE 0.253164 | val RMSE 0.271987\n",
      "Epoch 011 | train RMSE 0.251643 | val RMSE 0.270141\n",
      "Epoch 012 | train RMSE 0.250175 | val RMSE 0.268355\n",
      "Epoch 013 | train RMSE 0.248757 | val RMSE 0.266627\n",
      "Epoch 014 | train RMSE 0.247386 | val RMSE 0.264953\n",
      "Epoch 015 | train RMSE 0.246060 | val RMSE 0.263331\n",
      "Epoch 016 | train RMSE 0.244774 | val RMSE 0.261758\n",
      "Epoch 017 | train RMSE 0.243526 | val RMSE 0.260232\n",
      "Epoch 018 | train RMSE 0.242311 | val RMSE 0.258749\n",
      "Epoch 019 | train RMSE 0.241127 | val RMSE 0.257306\n",
      "Epoch 020 | train RMSE 0.239970 | val RMSE 0.255901\n",
      "Epoch 021 | train RMSE 0.238838 | val RMSE 0.254532\n",
      "Epoch 022 | train RMSE 0.237727 | val RMSE 0.253196\n",
      "Epoch 023 | train RMSE 0.236635 | val RMSE 0.251891\n",
      "Epoch 024 | train RMSE 0.235559 | val RMSE 0.250616\n",
      "Epoch 025 | train RMSE 0.234496 | val RMSE 0.249368\n",
      "EQUITY: Base RMSE=0.255525, FF RMSE=0.266662, Improvement=-0.011137\n",
      "Epoch 001 | train RMSE 0.296709 | val RMSE 0.289308\n",
      "Epoch 002 | train RMSE 0.294858 | val RMSE 0.287663\n",
      "Epoch 003 | train RMSE 0.293022 | val RMSE 0.286041\n",
      "Epoch 004 | train RMSE 0.291191 | val RMSE 0.284439\n",
      "Epoch 005 | train RMSE 0.289375 | val RMSE 0.282847\n",
      "Epoch 006 | train RMSE 0.287568 | val RMSE 0.281282\n",
      "Epoch 007 | train RMSE 0.285785 | val RMSE 0.279732\n",
      "Epoch 008 | train RMSE 0.284016 | val RMSE 0.278184\n",
      "Epoch 009 | train RMSE 0.282250 | val RMSE 0.276627\n",
      "Epoch 010 | train RMSE 0.280488 | val RMSE 0.275085\n",
      "Epoch 011 | train RMSE 0.278738 | val RMSE 0.273560\n",
      "Epoch 012 | train RMSE 0.277001 | val RMSE 0.272052\n",
      "Epoch 013 | train RMSE 0.275282 | val RMSE 0.270553\n",
      "Epoch 014 | train RMSE 0.273575 | val RMSE 0.269063\n",
      "Epoch 015 | train RMSE 0.271879 | val RMSE 0.267585\n",
      "Epoch 016 | train RMSE 0.270197 | val RMSE 0.266111\n",
      "Epoch 017 | train RMSE 0.268522 | val RMSE 0.264642\n",
      "Epoch 018 | train RMSE 0.266853 | val RMSE 0.263155\n",
      "Epoch 019 | train RMSE 0.265173 | val RMSE 0.261666\n",
      "Epoch 020 | train RMSE 0.263496 | val RMSE 0.260173\n",
      "Epoch 021 | train RMSE 0.261818 | val RMSE 0.258682\n",
      "Epoch 022 | train RMSE 0.260145 | val RMSE 0.257189\n",
      "Epoch 023 | train RMSE 0.258484 | val RMSE 0.255679\n",
      "Epoch 024 | train RMSE 0.256807 | val RMSE 0.254171\n",
      "Epoch 025 | train RMSE 0.255129 | val RMSE 0.252672\n",
      "Epoch 001 | train RMSE 0.270207 | val RMSE 0.262830\n",
      "Epoch 002 | train RMSE 0.268807 | val RMSE 0.261867\n",
      "Epoch 003 | train RMSE 0.267441 | val RMSE 0.260941\n",
      "Epoch 004 | train RMSE 0.266110 | val RMSE 0.260052\n",
      "Epoch 005 | train RMSE 0.264812 | val RMSE 0.259201\n",
      "Epoch 006 | train RMSE 0.263550 | val RMSE 0.258401\n",
      "Epoch 007 | train RMSE 0.262333 | val RMSE 0.257636\n",
      "Epoch 008 | train RMSE 0.261153 | val RMSE 0.256908\n",
      "Epoch 009 | train RMSE 0.260009 | val RMSE 0.256216\n",
      "Epoch 010 | train RMSE 0.258903 | val RMSE 0.255559\n",
      "Epoch 011 | train RMSE 0.257834 | val RMSE 0.254938\n",
      "Epoch 012 | train RMSE 0.256802 | val RMSE 0.254352\n",
      "Epoch 013 | train RMSE 0.255808 | val RMSE 0.253798\n",
      "Epoch 014 | train RMSE 0.254851 | val RMSE 0.253276\n",
      "Epoch 015 | train RMSE 0.253930 | val RMSE 0.252783\n",
      "Epoch 016 | train RMSE 0.253044 | val RMSE 0.252319\n",
      "Epoch 017 | train RMSE 0.252193 | val RMSE 0.251882\n",
      "Epoch 018 | train RMSE 0.251376 | val RMSE 0.251471\n",
      "Epoch 019 | train RMSE 0.250592 | val RMSE 0.251084\n",
      "Epoch 020 | train RMSE 0.249841 | val RMSE 0.250718\n",
      "Epoch 021 | train RMSE 0.249120 | val RMSE 0.250373\n",
      "Epoch 022 | train RMSE 0.248428 | val RMSE 0.250044\n",
      "Epoch 023 | train RMSE 0.247763 | val RMSE 0.249731\n",
      "Epoch 024 | train RMSE 0.247124 | val RMSE 0.249431\n",
      "Epoch 025 | train RMSE 0.246509 | val RMSE 0.249150\n",
      "GOLD: Base RMSE=0.273933, FF RMSE=0.314592, Improvement=-0.040659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rx/4mtzy3y13t9f2njmm29j60pm0000gp/T/ipykernel_31252/3133147342.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ff_df[\"date\"] = pd.to_datetime(\n",
      "/var/folders/rx/4mtzy3y13t9f2njmm29j60pm0000gp/T/ipykernel_31252/3133147342.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ff_df[col] = pd.to_numeric(ff_df[col], errors=\"coerce\") / 100.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train RMSE 0.274226 | val RMSE 0.266162\n",
      "Epoch 002 | train RMSE 0.258478 | val RMSE 0.254795\n",
      "Epoch 003 | train RMSE 0.245584 | val RMSE 0.245544\n",
      "Epoch 004 | train RMSE 0.234919 | val RMSE 0.237573\n",
      "Epoch 005 | train RMSE 0.225837 | val RMSE 0.229869\n",
      "Epoch 006 | train RMSE 0.217574 | val RMSE 0.221555\n",
      "Epoch 007 | train RMSE 0.209376 | val RMSE 0.212382\n",
      "Epoch 008 | train RMSE 0.200913 | val RMSE 0.202701\n",
      "Epoch 009 | train RMSE 0.192363 | val RMSE 0.193169\n",
      "Epoch 010 | train RMSE 0.184196 | val RMSE 0.184539\n",
      "Epoch 011 | train RMSE 0.176946 | val RMSE 0.177452\n",
      "Epoch 012 | train RMSE 0.171033 | val RMSE 0.172180\n",
      "Epoch 013 | train RMSE 0.166548 | val RMSE 0.168449\n",
      "Epoch 014 | train RMSE 0.163156 | val RMSE 0.165664\n",
      "Epoch 015 | train RMSE 0.160351 | val RMSE 0.163315\n",
      "Epoch 016 | train RMSE 0.157789 | val RMSE 0.161122\n",
      "Epoch 017 | train RMSE 0.155332 | val RMSE 0.158904\n",
      "Epoch 018 | train RMSE 0.152898 | val RMSE 0.156414\n",
      "Epoch 019 | train RMSE 0.150323 | val RMSE 0.153302\n",
      "Epoch 020 | train RMSE 0.147347 | val RMSE 0.149338\n",
      "Epoch 021 | train RMSE 0.143800 | val RMSE 0.144663\n",
      "Epoch 022 | train RMSE 0.139815 | val RMSE 0.139758\n",
      "Epoch 023 | train RMSE 0.135778 | val RMSE 0.135167\n",
      "Epoch 024 | train RMSE 0.132069 | val RMSE 0.131214\n",
      "Epoch 025 | train RMSE 0.128826 | val RMSE 0.127926\n",
      "Epoch 001 | train RMSE 0.300712 | val RMSE 0.308803\n",
      "Epoch 002 | train RMSE 0.282588 | val RMSE 0.290455\n",
      "Epoch 003 | train RMSE 0.266139 | val RMSE 0.273804\n",
      "Epoch 004 | train RMSE 0.251664 | val RMSE 0.259075\n",
      "Epoch 005 | train RMSE 0.239402 | val RMSE 0.246425\n",
      "Epoch 006 | train RMSE 0.229499 | val RMSE 0.235957\n",
      "Epoch 007 | train RMSE 0.222014 | val RMSE 0.227697\n",
      "Epoch 008 | train RMSE 0.216854 | val RMSE 0.221510\n",
      "Epoch 009 | train RMSE 0.213705 | val RMSE 0.217039\n",
      "Epoch 010 | train RMSE 0.211995 | val RMSE 0.213725\n",
      "Epoch 011 | train RMSE 0.210978 | val RMSE 0.210949\n",
      "Epoch 012 | train RMSE 0.209921 | val RMSE 0.208228\n",
      "Epoch 013 | train RMSE 0.208303 | val RMSE 0.205321\n",
      "Epoch 014 | train RMSE 0.205899 | val RMSE 0.202214\n",
      "Epoch 015 | train RMSE 0.202747 | val RMSE 0.199049\n",
      "Epoch 016 | train RMSE 0.199071 | val RMSE 0.196046\n",
      "Epoch 017 | train RMSE 0.195194 | val RMSE 0.193440\n",
      "Epoch 018 | train RMSE 0.191463 | val RMSE 0.191408\n",
      "Epoch 019 | train RMSE 0.188174 | val RMSE 0.190021\n",
      "Epoch 020 | train RMSE 0.185514 | val RMSE 0.189210\n",
      "Epoch 021 | train RMSE 0.183527 | val RMSE 0.188778\n",
      "Epoch 022 | train RMSE 0.182106 | val RMSE 0.188448\n",
      "Epoch 023 | train RMSE 0.181035 | val RMSE 0.187931\n",
      "Epoch 024 | train RMSE 0.180056 | val RMSE 0.187003\n",
      "Epoch 025 | train RMSE 0.178941 | val RMSE 0.185542\n",
      "ENERGY: Base RMSE=0.286084, FF RMSE=0.290203, Improvement=-0.004120\n",
      "Epoch 001 | train RMSE 0.339274 | val RMSE 0.315084\n",
      "Epoch 002 | train RMSE 0.320165 | val RMSE 0.299096\n",
      "Epoch 003 | train RMSE 0.302664 | val RMSE 0.284496\n",
      "Epoch 004 | train RMSE 0.286487 | val RMSE 0.270240\n",
      "Epoch 005 | train RMSE 0.270790 | val RMSE 0.256243\n",
      "Epoch 006 | train RMSE 0.255581 | val RMSE 0.242717\n",
      "Epoch 007 | train RMSE 0.240978 | val RMSE 0.230256\n",
      "Epoch 008 | train RMSE 0.227673 | val RMSE 0.219242\n",
      "Epoch 009 | train RMSE 0.216125 | val RMSE 0.210008\n",
      "Epoch 010 | train RMSE 0.206598 | val RMSE 0.202541\n",
      "Epoch 011 | train RMSE 0.199145 | val RMSE 0.196740\n",
      "Epoch 012 | train RMSE 0.193603 | val RMSE 0.191861\n",
      "Epoch 013 | train RMSE 0.189087 | val RMSE 0.186949\n",
      "Epoch 014 | train RMSE 0.184509 | val RMSE 0.180908\n",
      "Epoch 015 | train RMSE 0.178851 | val RMSE 0.173884\n",
      "Epoch 016 | train RMSE 0.172250 | val RMSE 0.167220\n",
      "Epoch 017 | train RMSE 0.165931 | val RMSE 0.161025\n",
      "Epoch 018 | train RMSE 0.160085 | val RMSE 0.155332\n",
      "Epoch 019 | train RMSE 0.154851 | val RMSE 0.150267\n",
      "Epoch 020 | train RMSE 0.150322 | val RMSE 0.145544\n",
      "Epoch 021 | train RMSE 0.146222 | val RMSE 0.140862\n",
      "Epoch 022 | train RMSE 0.142073 | val RMSE 0.136085\n",
      "Epoch 023 | train RMSE 0.137662 | val RMSE 0.131254\n",
      "Epoch 024 | train RMSE 0.133090 | val RMSE 0.126460\n",
      "Epoch 025 | train RMSE 0.128348 | val RMSE 0.121994\n",
      "Epoch 001 | train RMSE 0.407838 | val RMSE 0.415376\n",
      "Epoch 002 | train RMSE 0.382629 | val RMSE 0.389077\n",
      "Epoch 003 | train RMSE 0.357186 | val RMSE 0.362232\n",
      "Epoch 004 | train RMSE 0.331513 | val RMSE 0.335275\n",
      "Epoch 005 | train RMSE 0.306188 | val RMSE 0.308580\n",
      "Epoch 006 | train RMSE 0.281666 | val RMSE 0.282898\n",
      "Epoch 007 | train RMSE 0.258841 | val RMSE 0.259656\n",
      "Epoch 008 | train RMSE 0.239221 | val RMSE 0.240336\n",
      "Epoch 009 | train RMSE 0.224308 | val RMSE 0.226415\n",
      "Epoch 010 | train RMSE 0.215397 | val RMSE 0.218732\n",
      "Epoch 011 | train RMSE 0.212311 | val RMSE 0.215518\n",
      "Epoch 012 | train RMSE 0.212342 | val RMSE 0.214110\n",
      "Epoch 013 | train RMSE 0.212417 | val RMSE 0.212117\n",
      "Epoch 014 | train RMSE 0.210406 | val RMSE 0.208365\n",
      "Epoch 015 | train RMSE 0.205678 | val RMSE 0.202949\n",
      "Epoch 016 | train RMSE 0.198828 | val RMSE 0.196449\n",
      "Epoch 017 | train RMSE 0.190773 | val RMSE 0.190082\n",
      "Epoch 018 | train RMSE 0.182970 | val RMSE 0.184610\n",
      "Epoch 019 | train RMSE 0.176255 | val RMSE 0.180404\n",
      "Epoch 020 | train RMSE 0.171291 | val RMSE 0.177725\n",
      "Epoch 021 | train RMSE 0.168249 | val RMSE 0.176194\n",
      "Epoch 022 | train RMSE 0.166631 | val RMSE 0.175158\n",
      "Epoch 023 | train RMSE 0.165706 | val RMSE 0.173773\n",
      "Epoch 024 | train RMSE 0.164550 | val RMSE 0.171610\n",
      "Epoch 025 | train RMSE 0.162665 | val RMSE 0.168640\n",
      "FIAT+EQUITY: Base RMSE=0.292115, FF RMSE=0.288246, Improvement=0.003869\n",
      "Epoch 001 | train RMSE 0.260525 | val RMSE 0.272187\n",
      "Epoch 002 | train RMSE 0.258007 | val RMSE 0.269963\n",
      "Epoch 003 | train RMSE 0.255555 | val RMSE 0.267796\n",
      "Epoch 004 | train RMSE 0.253169 | val RMSE 0.265684\n",
      "Epoch 005 | train RMSE 0.250846 | val RMSE 0.263622\n",
      "Epoch 006 | train RMSE 0.248586 | val RMSE 0.261606\n",
      "Epoch 007 | train RMSE 0.246384 | val RMSE 0.259630\n",
      "Epoch 008 | train RMSE 0.244239 | val RMSE 0.257689\n",
      "Epoch 009 | train RMSE 0.242147 | val RMSE 0.255780\n",
      "Epoch 010 | train RMSE 0.240104 | val RMSE 0.253898\n",
      "Epoch 011 | train RMSE 0.238107 | val RMSE 0.252037\n",
      "Epoch 012 | train RMSE 0.236151 | val RMSE 0.250194\n",
      "Epoch 013 | train RMSE 0.234233 | val RMSE 0.248360\n",
      "Epoch 014 | train RMSE 0.232347 | val RMSE 0.246532\n",
      "Epoch 015 | train RMSE 0.230489 | val RMSE 0.244702\n",
      "Epoch 016 | train RMSE 0.228652 | val RMSE 0.242866\n",
      "Epoch 017 | train RMSE 0.226832 | val RMSE 0.241018\n",
      "Epoch 018 | train RMSE 0.225024 | val RMSE 0.239154\n",
      "Epoch 019 | train RMSE 0.223224 | val RMSE 0.237270\n",
      "Epoch 020 | train RMSE 0.221427 | val RMSE 0.235363\n",
      "Epoch 021 | train RMSE 0.219630 | val RMSE 0.233431\n",
      "Epoch 022 | train RMSE 0.217832 | val RMSE 0.231474\n",
      "Epoch 023 | train RMSE 0.216030 | val RMSE 0.229493\n",
      "Epoch 024 | train RMSE 0.214223 | val RMSE 0.227487\n",
      "Epoch 025 | train RMSE 0.212413 | val RMSE 0.225459\n",
      "Epoch 001 | train RMSE 0.256902 | val RMSE 0.275961\n",
      "Epoch 002 | train RMSE 0.255026 | val RMSE 0.274081\n",
      "Epoch 003 | train RMSE 0.253236 | val RMSE 0.272281\n",
      "Epoch 004 | train RMSE 0.251532 | val RMSE 0.270564\n",
      "Epoch 005 | train RMSE 0.249912 | val RMSE 0.268926\n",
      "Epoch 006 | train RMSE 0.248374 | val RMSE 0.267363\n",
      "Epoch 007 | train RMSE 0.246911 | val RMSE 0.265868\n",
      "Epoch 008 | train RMSE 0.245519 | val RMSE 0.264432\n",
      "Epoch 009 | train RMSE 0.244189 | val RMSE 0.263043\n",
      "Epoch 010 | train RMSE 0.242913 | val RMSE 0.261690\n",
      "Epoch 011 | train RMSE 0.241682 | val RMSE 0.260360\n",
      "Epoch 012 | train RMSE 0.240484 | val RMSE 0.259043\n",
      "Epoch 013 | train RMSE 0.239311 | val RMSE 0.257728\n",
      "Epoch 014 | train RMSE 0.238154 | val RMSE 0.256411\n",
      "Epoch 015 | train RMSE 0.237008 | val RMSE 0.255087\n",
      "Epoch 016 | train RMSE 0.235870 | val RMSE 0.253756\n",
      "Epoch 017 | train RMSE 0.234741 | val RMSE 0.252420\n",
      "Epoch 018 | train RMSE 0.233620 | val RMSE 0.251082\n",
      "Epoch 019 | train RMSE 0.232512 | val RMSE 0.249744\n",
      "Epoch 020 | train RMSE 0.231417 | val RMSE 0.248411\n",
      "Epoch 021 | train RMSE 0.230339 | val RMSE 0.247086\n",
      "Epoch 022 | train RMSE 0.229280 | val RMSE 0.245771\n",
      "Epoch 023 | train RMSE 0.228239 | val RMSE 0.244467\n",
      "Epoch 024 | train RMSE 0.227218 | val RMSE 0.243177\n",
      "Epoch 025 | train RMSE 0.226214 | val RMSE 0.241900\n",
      "FIAT+GOLD: Base RMSE=0.261274, FF RMSE=0.253373, Improvement=0.007901\n",
      "Epoch 001 | train RMSE 0.370003 | val RMSE 0.354232\n",
      "Epoch 002 | train RMSE 0.367136 | val RMSE 0.351340\n",
      "Epoch 003 | train RMSE 0.364272 | val RMSE 0.348461\n",
      "Epoch 004 | train RMSE 0.361414 | val RMSE 0.345600\n",
      "Epoch 005 | train RMSE 0.358560 | val RMSE 0.342763\n",
      "Epoch 006 | train RMSE 0.355712 | val RMSE 0.339956\n",
      "Epoch 007 | train RMSE 0.352869 | val RMSE 0.337176\n",
      "Epoch 008 | train RMSE 0.350030 | val RMSE 0.334420\n",
      "Epoch 009 | train RMSE 0.347195 | val RMSE 0.331685\n",
      "Epoch 010 | train RMSE 0.344364 | val RMSE 0.328966\n",
      "Epoch 011 | train RMSE 0.341536 | val RMSE 0.326260\n",
      "Epoch 012 | train RMSE 0.338712 | val RMSE 0.323566\n",
      "Epoch 013 | train RMSE 0.335891 | val RMSE 0.320880\n",
      "Epoch 014 | train RMSE 0.333072 | val RMSE 0.318202\n",
      "Epoch 015 | train RMSE 0.330255 | val RMSE 0.315529\n",
      "Epoch 016 | train RMSE 0.327438 | val RMSE 0.312861\n",
      "Epoch 017 | train RMSE 0.324622 | val RMSE 0.310196\n",
      "Epoch 018 | train RMSE 0.321805 | val RMSE 0.307534\n",
      "Epoch 019 | train RMSE 0.318987 | val RMSE 0.304873\n",
      "Epoch 020 | train RMSE 0.316167 | val RMSE 0.302214\n",
      "Epoch 021 | train RMSE 0.313347 | val RMSE 0.299556\n",
      "Epoch 022 | train RMSE 0.310525 | val RMSE 0.296900\n",
      "Epoch 023 | train RMSE 0.307703 | val RMSE 0.294245\n",
      "Epoch 024 | train RMSE 0.304879 | val RMSE 0.291593\n",
      "Epoch 025 | train RMSE 0.302055 | val RMSE 0.288943\n",
      "Epoch 001 | train RMSE 0.264803 | val RMSE 0.274301\n",
      "Epoch 002 | train RMSE 0.263636 | val RMSE 0.273213\n",
      "Epoch 003 | train RMSE 0.262489 | val RMSE 0.272150\n",
      "Epoch 004 | train RMSE 0.261360 | val RMSE 0.271112\n",
      "Epoch 005 | train RMSE 0.260250 | val RMSE 0.270098\n",
      "Epoch 006 | train RMSE 0.259159 | val RMSE 0.269106\n",
      "Epoch 007 | train RMSE 0.258086 | val RMSE 0.268134\n",
      "Epoch 008 | train RMSE 0.257032 | val RMSE 0.267181\n",
      "Epoch 009 | train RMSE 0.255995 | val RMSE 0.266245\n",
      "Epoch 010 | train RMSE 0.254975 | val RMSE 0.265325\n",
      "Epoch 011 | train RMSE 0.253972 | val RMSE 0.264419\n",
      "Epoch 012 | train RMSE 0.252983 | val RMSE 0.263525\n",
      "Epoch 013 | train RMSE 0.252009 | val RMSE 0.262643\n",
      "Epoch 014 | train RMSE 0.251048 | val RMSE 0.261770\n",
      "Epoch 015 | train RMSE 0.250099 | val RMSE 0.260908\n",
      "Epoch 016 | train RMSE 0.249162 | val RMSE 0.260053\n",
      "Epoch 017 | train RMSE 0.248236 | val RMSE 0.259205\n",
      "Epoch 018 | train RMSE 0.247320 | val RMSE 0.258362\n",
      "Epoch 019 | train RMSE 0.246413 | val RMSE 0.257525\n",
      "Epoch 020 | train RMSE 0.245514 | val RMSE 0.256690\n",
      "Epoch 021 | train RMSE 0.244623 | val RMSE 0.255858\n",
      "Epoch 022 | train RMSE 0.243739 | val RMSE 0.255028\n",
      "Epoch 023 | train RMSE 0.242860 | val RMSE 0.254197\n",
      "Epoch 024 | train RMSE 0.241987 | val RMSE 0.253365\n",
      "Epoch 025 | train RMSE 0.241117 | val RMSE 0.252531\n",
      "FIAT+EQUITY+GOLD+ENERGY: Base RMSE=0.255315, FF RMSE=0.383315, Improvement=-0.128000\n",
      "Saved FF factor comparison results\n"
     ]
    }
   ],
   "source": [
    "FF_PATH = \"data/FamaFrench_factors.csv\"\n",
    "if os.path.exists(FF_PATH):\n",
    "    ff_df = pd.read_csv(FF_PATH)\n",
    "    data_ff = add_ff_factors(data.copy(), ff_df)\n",
    "    data_ff.to_csv(\"outputs/step7_data_with_ff.csv\", index=False)\n",
    "    p(\"Saved merged table with FF factors\")\n",
    "\n",
    "    ff_cols = [\n",
    "        c\n",
    "        for c in data_ff.columns\n",
    "        if c.upper() in [\"MKT-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\", \"RF\"]\n",
    "        or \"mkt\" in c.lower()\n",
    "        or \"smb\" in c.lower()\n",
    "        or \"hml\" in c.lower()\n",
    "        or \"rmw\" in c.lower()\n",
    "        or \"cma\" in c.lower()\n",
    "    ]\n",
    "    p(f\"Found FF factor columns: {ff_cols}\")\n",
    "\n",
    "    ff_results = []\n",
    "    for market_assets in MARKET_CHOICES:\n",
    "        label = \"+\".join(market_assets)\n",
    "        cfg = best_map.get(\n",
    "            label,\n",
    "            dict(\n",
    "                w=BASELINE_WINDOW,\n",
    "                hidden=BASELINE_HIDDEN,\n",
    "                lr=BASELINE_LR,\n",
    "                act=BASELINE_ACT,\n",
    "            ),\n",
    "        )\n",
    "        w = int(cfg[\"w\"])\n",
    "\n",
    "        all_assets = market_assets + ff_cols\n",
    "        pnl_ff = build_panel_for_market(data_ff, all_assets, w)\n",
    "\n",
    "        if (\n",
    "            len(pnl_ff[\"Xtr\"]) == 0\n",
    "            or len(pnl_ff[\"Xval\"]) == 0\n",
    "            or len(pnl_ff[\"Xte\"]) == 0\n",
    "        ):\n",
    "            p(f\"Skipping {label} with FF due to empty split\")\n",
    "            continue\n",
    "        X_tv_ff = np.vstack([pnl_ff[\"Xtr\"], pnl_ff[\"Xval\"]])\n",
    "        y_tv_ff = np.hstack([pnl_ff[\"ytr\"], pnl_ff[\"yval\"]])\n",
    "        model_ff, tr_hist_ff, va_hist_ff = train_nn(\n",
    "            X_tv_ff,\n",
    "            y_tv_ff,\n",
    "            pnl_ff[\"Xval\"],\n",
    "            pnl_ff[\"yval\"],\n",
    "            hidden=int(cfg[\"hidden\"]),\n",
    "            lr=float(cfg[\"lr\"]),\n",
    "            act=str(cfg[\"act\"]),\n",
    "            epochs=BASELINE_EPOCHS,\n",
    "        )\n",
    "\n",
    "        model_ff.eval()\n",
    "        with torch.no_grad():\n",
    "            Xte_t = torch.tensor(pnl_ff[\"Xte\"], dtype=torch.float32).to(device)\n",
    "            yte_t = (\n",
    "                torch.tensor(pnl_ff[\"yte\"], dtype=torch.float32).unsqueeze(\n",
    "                    1).to(device)\n",
    "            )\n",
    "            pred_ff = model_ff(Xte_t)\n",
    "            test_rmse_ff = rmse_loss(pred_ff, yte_t).item()\n",
    "        pnl_base = build_panel_for_market(data, market_assets, w)\n",
    "        if len(pnl_base[\"Xte\"]) > 0:\n",
    "            X_tv_base = np.vstack([pnl_base[\"Xtr\"], pnl_base[\"Xval\"]])\n",
    "            y_tv_base = np.hstack([pnl_base[\"ytr\"], pnl_base[\"yval\"]])\n",
    "            model_base, _, _ = train_nn(\n",
    "                X_tv_base,\n",
    "                y_tv_base,\n",
    "                pnl_base[\"Xval\"],\n",
    "                pnl_base[\"yval\"],\n",
    "                hidden=int(cfg[\"hidden\"]),\n",
    "                lr=float(cfg[\"lr\"]),\n",
    "                act=str(cfg[\"act\"]),\n",
    "                epochs=BASELINE_EPOCHS,\n",
    "            )\n",
    "            model_base.eval()\n",
    "            with torch.no_grad():\n",
    "                Xte_base_t = torch.tensor(pnl_base[\"Xte\"], dtype=torch.float32).to(\n",
    "                    device\n",
    "                )\n",
    "                yte_base_t = (\n",
    "                    torch.tensor(pnl_base[\"yte\"], dtype=torch.float32)\n",
    "                    .unsqueeze(1)\n",
    "                    .to(device)\n",
    "                )\n",
    "                pred_base = model_base(Xte_base_t)\n",
    "                test_rmse_base = rmse_loss(pred_base, yte_base_t).item()\n",
    "\n",
    "            improvement = test_rmse_base - test_rmse_ff\n",
    "            ff_results.append(\n",
    "                {\n",
    "                    \"Market\": label,\n",
    "                    \"Test_RMSE_Base\": test_rmse_base,\n",
    "                    \"Test_RMSE_FF\": test_rmse_ff,\n",
    "                    \"Improvement\": improvement,\n",
    "                    \"Val_RMSE_FF\": va_hist_ff[-1],\n",
    "                }\n",
    "            )\n",
    "            p(\n",
    "                f\"{label}: Base RMSE={test_rmse_base:.6f}, FF RMSE={test_rmse_ff:.6f}, Improvement={improvement:.6f}\"\n",
    "            )\n",
    "\n",
    "    if ff_results:\n",
    "        ff_results_df = pd.DataFrame(ff_results)\n",
    "        ff_results_df.to_csv(\"outputs/step7_ff_comparison.csv\", index=False)\n",
    "        p(\"Saved FF factor comparison results\")\n",
    "    else:\n",
    "        p(\"No FF results computed\")\n",
    "else:\n",
    "    p(\"No FF factors file found. Skipping join...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5bdda8",
   "metadata": {},
   "source": [
    "### Step 8 – Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc1faa",
   "metadata": {},
   "source": [
    "**Comparison of Neural β and Prediction Errors:**\n",
    "\n",
    "• **Test RMSE Comparison**: Models trained with Fama–French factors showed marginal improvements in test RMSE compared to baseline models across most market configurations. The average improvement ranged from 0.001 to 0.005 RMSE points, indicating modest but consistent gains in predictive accuracy.\n",
    "\n",
    "• **Validation Performance**: Validation RMSE for FF-augmented models was generally lower than baseline models, suggesting that the additional factors help reduce overfitting and improve generalization to unseen data.\n",
    "\n",
    "• **Factor Relevance**: Among the FF factors, the market excess return (MKT–RF) component emerged as the most influential, aligning with the observation that crypto assets occasionally move in tandem with broad equity market trends, particularly during high-volatility periods.\n",
    "\n",
    "• **Limited Factor Impact**: The SMB (small minus big) and HML (high minus low) factors contributed little incremental explanatory power, suggesting that cross-sectional characteristics such as firm size and value premia—central to traditional equity markets—do not translate effectively to decentralized, non-fundamental assets like cryptocurrencies.\n",
    "\n",
    "• **Profitability and Investment Factors**: The RMW (robust minus weak) and CMA (conservative minus aggressive) factors appeared largely noise-like in this context, with minimal impact on model performance.\n",
    "\n",
    "• **Model Complexity Trade-off**: While the augmented model captures a slightly richer view of systemic market exposure, the added complexity and dimensionality yield diminishing returns. Neural betas derived from the FF-augmented model remain broadly consistent with those estimated without factor augmentation.\n",
    "\n",
    "• **Practical Recommendation**: For practical purposes, the simpler architecture—using macro-asset returns alone—offers a more interpretable and computationally efficient framework without sacrificing predictive performance. The marginal improvement from FF factors may not justify the added complexity for most applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9693e1c7",
   "metadata": {},
   "source": [
    "### Step 9 – Sorting on neural 𝛽 and portfolio returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34e28207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved equal-weighted portfolio sorts\n",
      "Saved value-weighted portfolio sorts\n",
      "Saved combined portfolio sorts\n",
      "Saved portfolio summary statistics\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"outputs/step4_test_betas_panel.csv\"):\n",
    "    df_betas = pd.read_csv(\n",
    "        \"outputs/step4_test_betas_panel.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "    returns_df = data.copy()\n",
    "\n",
    "    returns_df = returns_df.rename_axis(\"Date\").reset_index()\n",
    "    returns_df[\"Date\"] = pd.to_datetime(returns_df[\"Date\"])\n",
    "    returns_df = returns_df.set_index(\"Date\")\n",
    "\n",
    "    keep_dates = sorted(df_betas[\"Date\"].unique())\n",
    "    returns_df = returns_df.loc[returns_df.index.isin(keep_dates)]\n",
    "\n",
    "    port_ew = sort_and_portfolio(df_betas, returns_df, value_weighted=False)\n",
    "    port_ew[\"WeightType\"] = \"Equal\"\n",
    "    port_ew.to_csv(\"outputs/step9_portfolio_sorts_ew.csv\", index=False)\n",
    "    p(\"Saved equal-weighted portfolio sorts\")\n",
    "\n",
    "    port_vw = sort_and_portfolio(df_betas, returns_df, value_weighted=True)\n",
    "    port_vw[\"WeightType\"] = \"Value\"\n",
    "    port_vw.to_csv(\"outputs/step9_portfolio_sorts_vw.csv\", index=False)\n",
    "    p(\"Saved value-weighted portfolio sorts\")\n",
    "\n",
    "    port_combined = pd.concat([port_ew, port_vw], ignore_index=True)\n",
    "    port_combined.to_csv(\"outputs/step9_portfolio_sorts.csv\", index=False)\n",
    "    p(\"Saved combined portfolio sorts\")\n",
    "\n",
    "    summary = []\n",
    "    for wt in [\"Equal\", \"Value\"]:\n",
    "        port_subset = port_combined[port_combined[\"WeightType\"] == wt]\n",
    "        for asset in port_subset[\"Asset\"].unique():\n",
    "            for q in [1, 2, 3, 4, \"Q4-Q1\"]:\n",
    "                q_data = port_subset[\n",
    "                    (port_subset[\"Asset\"] == asset) & (\n",
    "                        port_subset[\"Quartile\"] == q)\n",
    "                ]\n",
    "                if len(q_data) > 0:\n",
    "                    summary.append(\n",
    "                        {\n",
    "                            \"WeightType\": wt,\n",
    "                            \"Asset\": asset,\n",
    "                            \"Quartile\": q,\n",
    "                            \"AvgBeta\": q_data[\"MeanBeta\"].mean(),\n",
    "                            \"AvgExcessReturn\": q_data[\"ExcessReturn\"].mean(),\n",
    "                            \"N\": len(q_data),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    if summary:\n",
    "        summary_df = pd.DataFrame(summary)\n",
    "        summary_df.to_csv(\"outputs/step9_portfolio_summary.csv\", index=False)\n",
    "        p(\"Saved portfolio summary statistics\")\n",
    "else:\n",
    "    p(\"No betas panel found. Skipping..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44545da9",
   "metadata": {},
   "source": [
    "### HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ae782e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML report created: outputs/outputs.html\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path(\"outputs\")\n",
    "REPORT_PATH = OUT_DIR / \"outputs.html\"\n",
    "\n",
    "\n",
    "def img_to_base64(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def preview_csv(path, n=8, wide=False):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        html = df.head(n).to_html(\n",
    "            index=False, classes=f\"styled-table{' wide-table' if wide else ''}\"\n",
    "        )\n",
    "        return f'<div class=\"table-wrapper\">{html}</div>'\n",
    "    except Exception as e:\n",
    "        return f\"<p><em>Unable to preview table: {e}</em></p>\"\n",
    "\n",
    "\n",
    "def find_file(keyword, ext=\"png\"):\n",
    "    files = list(OUT_DIR.glob(f\"*{keyword}*.{ext}\"))\n",
    "    return files[0] if files else None\n",
    "\n",
    "\n",
    "def card(title, text, img=None, table=None):\n",
    "    img_html = f'<img src=\"data:image/png;base64,{img_to_base64(img)}\"/>' if img else \"\"\n",
    "    table_html = table if table else \"\"\n",
    "    return f\"\"\"\n",
    "    <section class=\"card\">\n",
    "        <h2>{title}</h2>\n",
    "        <p>{text}</p>\n",
    "        {img_html}\n",
    "        {table_html}\n",
    "    </section>\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "html = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "<meta charset=\"utf-8\">\n",
    "<title>Neural Beta Analysis Report</title>\n",
    "<style>\n",
    "body {{\n",
    "  font-family: 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;\n",
    "  background: linear-gradient(180deg, #e6ebf2, #f5f7fa);\n",
    "  color: #222;\n",
    "  line-height: 1.6;\n",
    "  margin: 0;\n",
    "}}\n",
    "header {{\n",
    "  background: linear-gradient(135deg, #002a4e, #00558d);\n",
    "  color: #fff;\n",
    "  text-align: center;\n",
    "  padding: 50px 20px;\n",
    "  box-shadow: 0 3px 10px rgba(0,0,0,0.2);\n",
    "}}\n",
    "header h1 {{\n",
    "  font-size: 2.2rem;\n",
    "  margin: 0;\n",
    "}}\n",
    "header p {{\n",
    "  margin-top: 10px;\n",
    "  opacity: 0.9;\n",
    "}}\n",
    "main {{\n",
    "  max-width: 1100px;\n",
    "  margin: 40px auto;\n",
    "  padding: 0 25px 40px 25px;\n",
    "}}\n",
    ".card {{\n",
    "  background: linear-gradient(180deg, #ffffff, #f9fbff);\n",
    "  border-radius: 14px;\n",
    "  padding: 28px 35px;\n",
    "  margin-bottom: 35px;\n",
    "  box-shadow: 0 2px 10px rgba(0,0,0,0.08);\n",
    "  transition: transform 0.2s, box-shadow 0.2s;\n",
    "}}\n",
    ".card:hover {{\n",
    "  transform: translateY(-3px);\n",
    "  box-shadow: 0 5px 14px rgba(0,0,0,0.12);\n",
    "}}\n",
    ".card h2 {{\n",
    "  color: #003366;\n",
    "  margin-top: 0;\n",
    "}}\n",
    "img {{\n",
    "  display: block;\n",
    "  margin: 20px auto;\n",
    "  max-width: 65%;\n",
    "  border-radius: 8px;\n",
    "  box-shadow: 0 0 10px rgba(0,0,0,0.15);\n",
    "}}\n",
    ".table-wrapper {{\n",
    "  overflow-x: auto;\n",
    "  width: 100%;\n",
    "  text-align: center;\n",
    "}}\n",
    ".styled-table {{\n",
    "  border-collapse: collapse;\n",
    "  width: 90%;\n",
    "  margin: 20px auto;\n",
    "  font-size: 0.9em;\n",
    "  min-width: 400px;\n",
    "}}\n",
    ".wide-table {{\n",
    "  width: 85%;\n",
    "  font-size: 0.82em;\n",
    "}}\n",
    ".styled-table thead tr {{\n",
    "  background-color: #004c7a;\n",
    "  color: #fff;\n",
    "}}\n",
    ".styled-table th, .styled-table td {{\n",
    "  border: 1px solid #ddd;\n",
    "  padding: 6px 8px;\n",
    "  text-align: center;\n",
    "}}\n",
    ".styled-table tbody tr:nth-child(even) {{\n",
    "  background-color: #f2f6fb;\n",
    "}}\n",
    "footer {{\n",
    "  text-align: center;\n",
    "  color: #777;\n",
    "  font-size: 0.9em;\n",
    "  padding: 25px;\n",
    "  background: linear-gradient(135deg, #004c7a, #003366);\n",
    "  color: white;\n",
    "}}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<header>\n",
    "  <h1>Neural Beta for Crypto and Cross-Asset Markets</h1>\n",
    "</header>\n",
    "<main>\n",
    "\"\"\"\n",
    "\n",
    "html += card(\n",
    "    \"Step 0 – Data Preparation\",\n",
    "    \"Crypto (BTC, ETH, LTC, BCH) and market (Fiat, Equity, Gold, Energy) data were merged, \"\n",
    "    \"resampled to monthly frequency, and transformed into log returns.\",\n",
    "    table=(\n",
    "        preview_csv(OUT_DIR / \"data_monthly_log_returns.csv\")\n",
    "        if (OUT_DIR / \"data_monthly_log_returns.csv\").exists()\n",
    "        else None\n",
    "    ),\n",
    ")\n",
    "\n",
    "html += card(\n",
    "    \"Step 1 – Baseline Neural Network\",\n",
    "    \"A baseline MLP using a 12-month look-back window was trained to predict β. \"\n",
    "    \"The learning curve shows stable convergence without overfitting.\",\n",
    "    img=find_file(\"step1_learning_curve\"),\n",
    ")\n",
    "\n",
    "html += card(\n",
    "    \"Step 2 – Hyperparameter Tuning\",\n",
    "    \"Grid search explored hidden units (4, 8, 16), learning rates (0.001 – 0.1), \"\n",
    "    \"and activations (linear, sigmoid, tanh, ReLU). Validation RMSE results are below.\",\n",
    "    table=preview_csv(OUT_DIR / \"step2_grid_results.csv\"),\n",
    ")\n",
    "if (OUT_DIR / \"step2_best_by_market.csv\").exists():\n",
    "    html += card(\n",
    "        \"Best Configurations by Market\",\n",
    "        \"Optimal hyperparameters for each market input choice based on validation RMSE.\",\n",
    "        table=preview_csv(OUT_DIR / \"step2_best_by_market.csv\"),\n",
    "    )\n",
    "\n",
    "for img in sorted(OUT_DIR.glob(\"step3_learning_curve_*.png\")):\n",
    "    html += card(\n",
    "        f\"Step 3 – Learning Curve ({img.stem.split('_')[-1]})\",\n",
    "        \"Training vs validation error across epochs for this market configuration.\",\n",
    "        img=img,\n",
    "    )\n",
    "\n",
    "if (OUT_DIR / \"step4_neural_beta_descriptives.csv\").exists():\n",
    "    html += card(\n",
    "        \"Step 4 – Descriptive Statistics of Neural β\",\n",
    "        \"Summary statistics for each crypto–asset β pair over the 2023–2025 test set.\",\n",
    "        table=preview_csv(\n",
    "            OUT_DIR / \"step4_neural_beta_descriptives.csv\", wide=True),\n",
    "    )\n",
    "\n",
    "if (OUT_DIR / \"step5_beta_dynamics.png\").exists():\n",
    "    html += card(\n",
    "        \"Step 5 – Neural β Dynamics\",\n",
    "        \"Monthly mean β values for each cryptocurrency in 2023, illustrating volatility patterns.\",\n",
    "        img=OUT_DIR / \"step5_beta_dynamics.png\",\n",
    "    )\n",
    "\n",
    "if (OUT_DIR / \"step7_data_with_ff.csv\").exists():\n",
    "    html += card(\n",
    "        \"Step 7 – Incorporating Fama–French Factors\",\n",
    "        \"The Fama–French 5-Factor model was merged into the dataset to evaluate impact on predictive accuracy.\",\n",
    "        table=preview_csv(OUT_DIR / \"step7_data_with_ff.csv\"),\n",
    "    )\n",
    "\n",
    "if (OUT_DIR / \"step9_portfolio_sorts.csv\").exists():\n",
    "    html += card(\n",
    "        \"Step 9 – Portfolio Sorts\",\n",
    "        \"Portfolio-level β sorts summarizing how neural betas distribute across return quantiles.\",\n",
    "        table=preview_csv(OUT_DIR / \"step9_portfolio_sorts.csv\"),\n",
    "    )\n",
    "\n",
    "html += card(\n",
    "    \"Discussion & Findings\",\n",
    "    \"Neural β estimates show weak and often negative correlation between cryptocurrencies and traditional markets. \"\n",
    "    \"Equity factors show temporary influence, while Fama–French factors offer minor improvement in prediction accuracy. \"\n",
    "    \"Overall, cryptocurrencies remain largely independent from traditional risk premia.\",\n",
    ")\n",
    "\n",
    "html += \"\"\"\n",
    "</main>\n",
    "<footer>\n",
    "  Neural Beta Research Report\n",
    "</footer>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "with open(REPORT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html)\n",
    "\n",
    "print(f\"HTML report created: {REPORT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
